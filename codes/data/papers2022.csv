text
Using text and image machine learning to classify and prevent cyberbullying Final report submitted by BT Group Mukeshwaran Baskaran A Y Nanhai Zhong A E Ng Boon Khai A E Kuan Ju Lin A M Chen Hong A X Abstract Group github code link https github com typing Cyberbullying Detection Introduction Cyberbullying defined as the use of digital technology to inflict harm Englander et al is a pressing issue that needs to be addressed Compounded by the fact that the on going COVID pandemic has paved the way for decreased face to face interactions in favour of online ones it is now more urgent than ever to solve the cyberbullying problem For example on April th UNICEF issued a warning about the rise in cyberbullying cases during the pandemic The threat of cyberbullying is corroborated by the following statistics of schooling age children have been cyberbullied before which led to side effects such as decreased academic performance depression or even suicidal thoughts Furthermore aggressors are often anonymous making them difficult to be stopped by the relevant authorities Objective Therefore correct identification of cyberbullying over the internet is imperative as a first step towards introducing effective intervention strategies to stem cyberbullying prop agation To this end a machine learning approach is ideal for the task at hand For example sentiment analysis using natural language processing NLP Wang et al The aim of this project is to build machine learning models and train on the cyberbullying kaggle dataset Wang et al to classify whether or not a sample is categorized as cy berbullying or not Further a novelty of the dataset over previous ones within the literature is that the labelled data provides finer detailed information about the type of cyber Business Analytics Centre National University of Singapore Singapore Singapore Correspondence to Mukeshwaran Baskaran e u nus edu bullying actions For example samples could be labelled as age ethnicity gender religion or others category of cyber bullying This multi classification fine grain gives rise to opportunities for relevant targeted intervention strategies or counselling Wang et al assuming the perpetrators could be identified Beyond building traditional machine learning models such as logistic regression LR support vector machine SVM k nearest neighbors k NN and naive bayes NB to solve the classification problem this project aims to investigate the use of advanced machine learning models such as long short term memory LSTM and Bi directional Encoder Representations from Transformers BERT Furthermore to build on the work of Ref Wang et al it may be worthwhile to investigate data augmentation techniques such as lexical replacement back translation text surface transformation random noise injection instance crossover augmentation and generative methods to generate more text data in order to better understand the impact of hav ing more data on deep learning model performance Next given that cyberbullying takes the form of both text and image to round off the cyberbullying detection problem a convolutional neural network CNN was built to classify cyberbullying images Finally a short discussion on the methods of Ref Vishwamitra et al is given where using contextual factors list captured by software list the basic CNN classification model can be greatly improved with multimodality learning Note that the two classification tasks text and image are distinct and separate from each other Data In this project two datasets are considered The labelled kaggle dataset for cyberbullying text as well as a second dataset credited to Ref Vishwamitra et al which comprises a novel comprehensive labelled cyberbullying image dataset  religion age gender ethnicity not cyberbullying other cyberbullying of records Table Number of records in each class Text dataset The textual dataset was obtained from Kaggle which was originated from Ref Wang et al The Kaggle dataset used in this section has rows and features tweet text and cyberbullying type The initial exploratory data analysis shows that Tweet text and cyberbullying type are the columns Both columns are of the datatype object and are strings The dataset has no missing values The classes for cyberbullying types are relatively balanced as shown in Tab In addition as demonstrated in the wordcloud displayed in Fig offensive tweets based on ethnicity have the most characters followed by gender and religion related offen sive tweets Figure Word cloud visualization of all the tweets showing the most commonly occurring words The term bullying is the most common among the top words in the not cyberbullying category This could indicate that the purpose of these tweets is to convey an anti bullying message The most popular tweets for gender are rape gay and funny meaning that gays are more likely to be the target of rape jokes on Twitter The most popular word for religion is muslim implying that the bulk of hostile religious tweets are directed against Muslims The N word is one of the most commonly used words in the ethnicity category implying that ethnicity based slurs on Twitter are primarily directed at the black community Image dataset The classes of dataset are imbalanced but acceptable with images belonging to the non cyberbullying class and belonging to the cyberbullying class a b c d Figure Sample examples of cyberbullying images a b c d e Figure Sample examples of non cyberbullying images Figs and show sample examples of the cyberbullying image dataset Vishwamitra et al respectively for the class labels of cyberbullying and non cyberbullying  According to Ref Vishwamitra et al the difficulty of classifying image cyberbullying is that it is contextual For example in Fig objects such as guns Fig a or hanging rope Fig d are associated with cyberbullying as they suggest physical harm of some sort On the other hand in Fig c an image of a soldier firing a gun is shown but this image should not be a cyberbullying image as the soldier is just doing his job Other types of objects such as a football shoe as shown in Fig e should also not be associated with cyberbullying Likewise for the skull image in Fig a it should not be a cyberbullying image as there is no contextual information suggesting harm to the victims Furthermore the facial expression of a person are also im portant contextual cues in cyberbullying as shown in the left figure Figs b and c Aggressive facial expressions may be associated with cyberbullying On the other hand in Fig b although the facial expression of the two persons are quite intense they are not associated with cyberbullying This is likely due to the fact that their body posture are not faced towards the front All of the image cyberbullying classification are contextual For example in Fig d al though the woman s body posture is facing to the front her expression is calm so it is also not a cyberbullying image A discussion on the important contextual factors in image data and how to extract them will be discussed further in Sec Text data preprocessing Before feature engineering several preprocessing steps were conducted to clean the text data To convert raw strings of text to encodings which can be consumed by a machine learning model the raw text needs to be treated with a series of preprocessing steps to filter and clean the text data Next use NLTK Natural Language Toolkit WordNetLemma tizer demoji packages to achieve this The steps include tokenization lowercasing stop words filtering lemmatization stemming removing special characters punctuations and digits removing hashtag mention mark and URLs spell checking converting emoji into words It is important to be mindful that some preprocessing steps may cause loss in information especially in a social media context and therefore need to be chosen wisely For instance in a tweet symbol is usually used as hashtags and symbol is used to refer to a twitter handle Some authors may also deliberately misspell a word or use internet slang to express certain sentiments The next step is transforming the unstructured text data into structured data so that classification models could be built In this project word level encoding Word Vector and BERT were used Word level encoding includes Bag of Words and Term Frequency Inverse Document Frequency which are the most prevalent encoding methods for convert ing text sentences into numeric vectors for statistical models In essence BoW is a count of word occurrences whereas TF IDF encoding also takes into account the importance of the words Word Vector considers whether those words have similar semantics when encoding BERT is a word embedding model based on the self attention mechanism that is pre trained on top of the bidirectional transformer For statistical language models Probabilistic models that learn the probability distribution of words are known as statistical language models For statisti cal language models both BoW and TF IDF were used as encoding methods and establish classification models based on Na ve Bayes NB Logistic Regression LR Support Vector Machine SVM and Decision Tree DT To begin with the cleaned text data was split into training dataset and test dataset The ratio between the number of records in the training dataset and the test dataset is Next use the CountVectorizer function and TfidfTransformer function to calculate the BoW and TF IDF of each word To avoid data leakage both are fitted on the training dataset and then transform for both training dataset and test dataset For neural language models Classic statistical models perform and generalize less well than neural language models Word embeddings are used as inputs to a neural network which converts a sentence s words into vectorized representations In the vector space words with similar semantics have comparable representa tions Word Vector and BERT are some of the most often used word embedding technologies LSTM WITH WORD VECTOR To use Word Vector encoding a vocabulary of the top words from the training dataset was created Next tokenize the text using the vocabulary add padding to tokens to keep them in the same length and create a dimension word embedding matrix by Word Vector As an example of the top most common words in the vocabulary see Fig Fig shows that there may be a lot of text related to  school fuck dumb girl high u bully people nigger bullied muslim joke rape gay one dont get idiot black woman Words count Top most common words Figure Top Most Common Words in the Vocabulary school which is consistent with the fact that school is a place where bullying might occur In addition some words represent a group of people are quite common such as girl muslim and so on Some swear words also have a high frequency After creating the word embedding matrix further split the original training dataset into training dataset and validation dataset Besides random over sampling to the training dataset was performed so that each class has records BASIC BERT A pre trained BERT model from the Hugging Face library was used to perform classification BERT is a context based language model which is designed to be bidirectionally trained on transformer architecture After data encoding a Neural Network with hidden layer hidden nodes and ReLU activation function was built ROBERTA Next a variant of BERT RoBERTa which is a more effi ciently pre trained model was experimented The motivation of this variant is to improve upon basic BERT which is sig nificantly undertrained The central idea was to train the same BERT model for longer more epochs and on more data Text data results For statistical language models The out of sample performance of basic classification mod els are shown respectively in Tabs and for accuracy and recall as follows Tab shows that there is no big difference of accuracy between different encoding methods Among different basic NB LR SVM DT BoW TF IDF Table Accuracy of Different Model Combinations classification models the Logistic Regression and SVM out perform the Na ve Bayes and Decision Tree However the training process of the SVM model is very time consuming compared with the Logistic Regression model Table shows that the classes with clear themes are easier to be detected by those models compared with not cyberbullying and other cyberbullying Among those classes with a clear theme the recall of gender is only around while recalls of other classes are higher than In addition although accuracy of different encoding methods is similar the recalls with different encoding meth ods are different When it comes to recall the Na ve Bayes model with BoW has better performance than the Na ve Bayes model with TF IDF while the Decision Tree model with TF IDF performs better than the Decision Tree model with BoW For neural language models LSTM WITH WORD VECTOR LSTM represents Long Short Term Memory which is a recurrent neural network RNN extension that allows it to learn meaningful context over longer sequences The hyperparameters of the bidirectional LSTM is shown in Tab A linear layer and a softmax layer were added at the end of the Bi LSTIM model to transform the output into dimensions classes and range between to The in sample accuracy of the Bi LSTM model was found to be while the out of sample accuracy was which  Class NB BoW NB TFIDF diff LR BoW LR TFIDF diff not cb gender religion other cb age ethnicity Class SVM BoW SVM TFIDF diff DT BoW DT TFIDF diff not cb gender religion other cb age ethnicity Table Recall of Different Model Combinations where not cb stands for not cyberbullying and other cb means other cyberbullying Hyperparameters num classes hidden dim lstm layers learning rate E dropout epochs Table The Hyperparameters of the Bi LSTM model indicates the model is not overfitting BASIC BERT The out of sample accuracy is the highest among previ ous models reaching At the same time gender and not cyberbullying achieve and recall respectively surpassing the previous models while others still have high recall ROBERTA The out of sample accuracy of RoBERTa surpasses BERT with a score of accuracy owing to its improved model capability to capture contextual meanings of words PERFORMANCE COMPARISON BETWEEN BERT AND ROBERTA Comparing the classification accuracy performance of BERT and Roberta the results are summarized in Tab Class LSTM with Word Vector Basic BERT Roberta not cyberbullying gender religion other cyberbullying age ethnicity Accuracy Table Performance accuracy Comparison Between BERT and Roberta TEXT AUGMENTATION Data augmentation is used to generate additional synthetic training data by applying transformation to the existing train ing data For natural language data NLPAug library is used to implement various text augmentation techniques The combination of the following text augmentation techniques are implemented to about of train data Synonym replacement via word embeddings and Back translation RoBERTa is used as the benchmark to study the augmenta tion effects on model performance The results are shown in Tab a Synonym replacement Synonym replacement obtains different sentences with the same meaning by replacing certain words with their corre sponding synonyms based on word embeddings b Back translation Back translation generates additional data with different wordings and sentence structure by translating the existing data to a different language and subsequently translating it back to the original language From Tab it is evident that text augmentation has lit tle to no effect on the performance in this application It is suspected that this could be due to loss in information that captures cyberbullying content during translation or synonym replacement Terms and phrases that appear in cyberbullying text often contain profanity defamation and vulgar expressions that only hold contextual meanings in local language Hence any replacement of words or transla tion may not be effective to enrich the dataset as they may result in introducing text samples with entirely different meanings than the original ones  Class Roberta Roberta with T A diff not cyberbullying gender religion other cyberbullying age ethnicity Accuracy Table Comparing performance accuracy before and after text augmentation where Roberta with T A indicates Roberta with text augmentation Image data preprocessing After reading in the data reshaping and scaling images was conducted all images were resized to pixels so as to make sure the CNN model can process the data smoothly Thereafter the data was randomly split with percent to be for training and percent to be for testing Image data results CNN model Convolutional Neural Network is a type of neural network model which is well known for working with the images and videos CNN takes the image s raw pixel data trains the model then extracts the features automatically for better classification In the CNN model used a series of convo lution layer ConV D followed by a Max pooling and a Dropout layer were built in the first part of deep learning model Convolutional layers apply a convolution operation to the input passing the result to the next layer A convolu tion converts all the pixels in its receptive field into a single value Here the most common type D convolution layer was applied It is a filter that slides over the D input data performing element wise multiplication and sums up the results into a single pixel output Max pooling is a pool ing operation that selects the maximum value of elements from the region of the feature map where filter covers It helps to reduce the dimensionality and thus reduces the number of parameters to learn In addition it also removes noise from input data and retains only the significant values The Dropout layer randomly sets input units to with a frequency of rate at each step during training time which helps prevent overfitting After the first series of layers a flatten layer was connected between the following Dense layer and the previous ones to flatten the multi dimensional input tensors into a single dimension Several Dense and Dropout layers were constructed afterwards Summarily the CNN network architecture is displayed in Fig in Appendix Evaluation For evaluation the ROC AUC and accuracy are metrics used to evaluate the CNN classifier model for cyberbullying images prediction epochs loss train test Figure Training and testing loss plotted against number of epochs epochs accuracy train test Figure Training and testing accuracy plotted against number of epochs During training the Epochs was set to be with callback monitoring the validation loss for early stopping and storing the best parameters As shown in Fig the training loss decreases as the epochs increases However when it reaches to epoch equals around it appears to begin overfitting the data since testing loss start to increase at that moment In Fig the accuracy performance of the CNN model is illustrated The trained CNN model can reach accu racy on the testing data assuming class separation criteria Z which outperforms the naive baseline model if it is assumed that the naive model predicts every data point to be non cyberbullying class   fpr tpr CNN AUC Figure ROC curve with AUC score of Predicted label True label Figure Confusion matrix of CNN classifier assuming class sep aration crtieria Z The ROC AUC and confusion metrics are also important evaluation metrics to consider The ROC analysis provides a means of reviewing the performance of a model in terms of the trade off between False Positive Rate and True Positive Rate as class separation criteria Z is varied As shown in Fig the CNN model reaches in terms of AUC score Fig displays the confusion matrix for Z with precision equals and recall equals indicating a reasonable classifier performance Discussion Project application Youngsters tend to spend most of their free time on social media platforms like TikTok and Twitter These platforms have had tremendous difficulty in moderating the content shared by their users The classification models can be utilized by social media platforms like Twitter and TikTok to reduce the exposure of harmful cyberbullying content towards children The NLP models could be used by twitter to ensure users below a certain age do not get to view explicit content while TikTok could protect their underage users with the computer vision model Improvement to image cyberbullying classification multimodality model While the basic baseline CNN model performs adequately achieving validation accuracy of and recall of there is still room for improvement Note that for the CNN model the training image and class labels were fed to the CNN model without doing anything else In other words the CNN model may not have adequately learned the contextual information of cyberbullying images In the research paper of Ref Vishwamitra et al the novelty of their contribution lies in being the first of its kind to not only provide a comprehensive cyberbullying image dataset but to contextualise the factors involved in cyberbullying images This subsection discusses the approach taken by the authors to do image factor extraction to build a multi modality learning model together with CNN to achieve a high out of sample classification accuracy of Mainly there are contextual factors to consider Body pose facial emotion hand gesture objects and social fac tors such as anti LGBT or hate speech related factors For body pose the authors used OpenPose Cao et al to extract where the persons involved in images are facing By doing cosine similarity analysis of the body pose oc currences in cyberbullying and non cyberbullying images they found that body posture that directly faces the front are more likely to be associated with cyberbullying while non front facing postures are more likely be to associated with non cyberbullying In facial emotion factor extraction the authors used Open Face Baltru saitis et al to extract the emotions of the persons involved in images Surprisingly the cosine simi larity analysis revealed that cyberbullying images tend not to show strong emotions Counter intuitively cyberbullying images subjects tend to show happy emotions perhaps as a way to mock the victims In hand gesture factor extraction they used Google Cloud Vision API Hand gestures such as loser middle finger thumbs down or gun point are associated with cyberbully ing images For object factor extraction the authors used YOLO Red mon Farhadi you only look once object detection algorithm Although majority of cyberbullying images do not have any objects some portion of them contain threaten  ing objects such as gun or knife to intimidate victims The object factor is also an important feature Finally for social factors such as hate speech or anti LGBT factors for example black face or hanging rope it is difficult currently to do factor extraction For this factor the authors manually analysed the images and hand label these factors Finally with these factors obtained the authors built a multi modality model that combines the images of the image dataset with the features of the extracted feature With this ML model they achieved a mean accuracy of Conclusion In conclusion in this project the problem of machine learn ing classification of cyberbullying was addressed both for the text data and the image data separately For the text data traditional approaches such as statistical language models for example bag of words tfidf and using these features to train classifical classifiers such as svm lr dt and naive bayes was considered These classifiers performed ade quately achieving average accuracy of about Next for text data classification advanced models such as neural language models were also considered For neural language models the models built were LSTM with word vec BERT and Roberta As expected the deep learning models per formed better achieving average accuracy of about Since deep learning models perform best when there is a lot of data text augmentation techniques such as back transla tion and synonym replacement were experimented It was found that text augmentation when applied to Roberta had little to no effect in terms of performance The second part of the project involves image cyberbullying classification For this problem the authors of Ref Vish wamitra et al were contacted directly in order to gain access to their novel comprehensive labelled dataset Next a CNN model was built to perform the classification task and achieved validation accuracy of in agreement with the baseline CNN model performance stated in Ref Vishwamitra et al Further in Sec the vari ous contextual factor information such as body pose facial expression hand gesture objects and social factors which could be used as features to build an effective multimodality machine learning model were discussed Due to time limita tion and technical difficulty of the factor extraction it was not possible to experiment with this model Finally it has to be said that cyberbullying is an important problem to tackle Since more people are on social media these days and victims of cyberbullyings especially chil dren are reported to suffer from reduced academic perfor mance or even worse still struggle with suicidal thoughts preventing cyberbullying is an important problem to tackle It is hoped that the work of this project contributes to solv ing this problem and in turn brings society one small step closer towards a better world with less suffering Appendix Appendix contains only the CNN network architecture Fig References https www unicef org press releases children increased risk harm online during global covid pandemic https www kaggle com andrewmvd cyber bullying classification http www broadbandsearch net blog cy ber bullying statistics google cloud vision api https cloud google com vision Baltru saitis T Robinson P and Morency L P Openface an open source facial behavior analysis toolkit In IEEE Winter Conference on Applications of Computer Vision WACV pp IEEE Cao Z Simon T Wei S E and Sheikh Y Realtime multi person d pose estimation using part affinity fields In Proceedings of the IEEE conference on computer vi sion and pattern recognition pp Englander E Donnerstein E Kowalski R Lin C A and Parti K Defining cyberbullying Pediatrics Supplement S S Redmon J and Farhadi A Yolov An incremental im provement arXiv preprint arXiv Vishwamitra N Hu H Luo F and Cheng L Towards understanding and detecting cyberbullying in real world images In th IEEE International Conference on Machine Learning and Applications ICMLA Wang J Fu K and Lu C T Sosnet A graph convo lutional network approach to fine grained cyberbullying detection In IEEE International Conference on Big Data Big Data pp IEEE  conv d input InputLayer input output None None conv d Conv D input output None None max pooling d MaxPooling D input output None None dropout Dropout input output None None conv d Conv D input output None None max pooling d MaxPooling D input output None None dropout Dropout input output None None conv d Conv D input output None None max pooling d MaxPooling D input output None None dropout Dropout input output None None conv d Conv D input output None None max pooling d MaxPooling D input output None None dropout Dropout input output None None flatten Flatten input output None None dense Dense input output None None dropout Dropout input output None None dense Dense input output None None Figure The network architecture of CNN used for the image cyberbullying classfication problem
Subscription Fee Reference for Streaming Platforms Launching in New Markets Abstract Streaming platforms are gradually taking over the traditional TV industry those days Netflix as the st streaming video platform reaching millions paying customers globally and launching in most of countries The problem statement has been focused on other streaming service platforms who has only launched in a few countries what the subscription fee should be set when entering the new market By searching multiple features and pre processing datasets features could be better implemented into models Random Forest Light GBM XGBoost Neural Network Depends on overall MAE RMSE MAPE value the best model has been decided to predict the subscription reference fee Business insights and future improvement also been discussed Introduction With the evolution of communication and network technology there is an increasing number of people giving up traditional television channel subscription and turning to watching movies and drama series through online streaming platforms such as Netflix Amazon Prime Video Disney and so on These companies have developed the convenient tools for people to access the online entertainment at any time with any devices Users can navigate the resources in the organized manners and resources are plenty including movies documentaries and television series To enrich the content some of them even invest in the programs hence reserving the exclusive authority and copyright For a paid streaming platform it only charges a flat subscription fee For only a flat monthly fee you paid you could consume unlimited shows at any time on whatever you prefer Netflix is the very first one that started the streaming platform in and now already become a mature platform and owned a huge user community that has about million paying customers globally As one of the most successful online streaming companies the strategy to invest in resources and price for different services in different areas is critical to improve the service quality and attract more users The project team will utilize the subscription data from Netflix and macro economy data for various countries to explore factors that potentially impact Netflix s pricing strategy The pricing model developed will be able to predict the optimal price that should be set with different resource pools for different service levels in different regions This could act as subscription fee guideline for other streaming platforms who is planning to launch their service in countries which Netflix has already operated in so that the price could be competitive as Netflix Motivation There are some interesting facts that motivated the team to research further on this topic Nowadays streaming platforms are gradually taking over the traditional TV industry Within the project team all the team members have subscribed to at least one online streaming service and none of them subscribe to the traditional TV service Taking a glance at Netflix statistics at below figures it is obvious that Netflix has increased its revenues significantly especially in where Covid hit human society and people had to stay longer time at home However the average revenue earned per user has remained steady since which implies a huge increase of user base Figure Netflix Revenue by Year Group Li Ruirui A L Wu Jitong A U Xu Han A E Zhou Lingyi A X Zhou Yin A B  Figure Netflix Average Revenue per User The profitability of the online streaming market has attracted followers to participate in the race However it cannot be denied that Netflix has set a benchmark and industry standard for its competitors to follow Netflix has already grown to be a global platform which has been launched in countries in the world which the rest still have rooms to pick up in terms of user coverage Based on those problem statement of the project has been come up For other streaming service platforms who has only launched in a few countries what the subscription fee should be set when entering the new market where Netflix has launched given the resources they owned and how to ensure the competitiveness of the pricing strategy One of the practical examples is that if Disney would like to expand the service network which currently is only available in countries and launch in Turkey what the target subscription price should be Assumption It is complicated to define whether a subscription fee is set successfully as there are other factors may affect business performance including customer loyalty viewing experience movie drama series popularity etc These factors are unpredictable and difficult to measure based on the given dataset hence will not be considered in modelling Project team assumes all the streaming platforms services are at the same competency level and all the resources are identical Hence the subscription fee reference will be based on the features listed down below only Objectives Areas of focus in this project are Analyze Netflix subscription fee dataset for different countries Explore potential correlations between subscription fee and macro economic and statistical figures e g Gini Index population GDP over different countries and determine whether or how those measures can be utilized to improve business profit Discover Netflix pricing strategies for different users across different countries and its practicality for streaming platforms to replicate when entering a new market Provide the subscription fee reference guide for streaming platforms or services which planned to enter a new market in countries never been launched this platform before based on most successful on demand Video streaming platform Netflix s dataset Data Pre processing Feature Generation Below is the detail table of the data after pre processing which will be used in the following model training Feature Description Country Code Short alphabetic code to represent countries ISO alpha standard is used Country Country name Total Library Size Total Movie and TV shows available on Netflix in this country No of TV Shows No of TV shows available on Netflix No of Movies No of movies available on Netflix Cost per Month Standard Standard subscription fee per month USD Movie aveRati ng IMDb weighted average of all the individual user ratings for Movies in Netflix TV aveRating IMDb weighted average of all the individual user ratings for TV shows in Netflix GDP GDP at purchaser s prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products Population Total population is based on the de facto definition of population which counts all residents regardless of legal status or citizenship The values shown are midyear estimates Gini Index Also named as Gini coefficient is a measure of statistical dispersion intended to represent the income inequality or wealth inequality within a nation Happiness Sha re Data from World Happiness Report evaluation question Represent share of people in percentage who say they are very happy or rather happy from  categories they were asked for each country Life Satisfacti on Data from self reported life satisfaction Average numbers are calculated for different countries The best possible life for them being a and the worst possible life being a Broadband sub scriptions Broadband subscriptions refer to fixed subscriptions to high speed access to the public Internet a TCP IP connection at downstream speeds equal to or greater than kbit s Cellular subscr iptions Mobile phone subscriptions measured as the number per people averageRating IMDB rating average of top movies and TV Shows in each country numVotes IMDB votes number average of top movies and TV Shows in each country Table Features used for Model Training There are data sources of the dataset here and the pre processing detail for each data source will be introduced in each part later Netflix subscription fee and library size in different countries The data source is Comparitech and it consists of following features Table Netflix features used for Model Training Below is a sample look of this data Figure Sample of Netflix Datasets In this data the prices included in are the based prices advertised by Netflix as the following The prices here do not include the taxes and other charges Figure Plans Pricing And after comparing the ratio between Basic Standard and Premium Cost per Month as below graph   Figure Standard Basic Premium Standard Price Comparison The ratio between Standard and Basic is around and it is for the majority so only the Cost per Month Standard will be predicted in the following model training The outlier in Standard versus Basic Cost per Month is India whose Cost per Month Basic is however Standard is IMDb rating data for Movies and TV shows There subsets of IMDb data used title ratings tsv gz which contains the IMDb rating and votes information for titles consists of the following features Table Features used for Model Training title episode tsv gz which contains the tv episode information it is used to differentiate the rating data of Movie and TV Shows It consists of the following features Table Features used for Model Training However there is no Netflix library data in part Netflix subscription fee in different countries data so the Movie aveRating and TV aveRating are calculated in following method random selection applied here according to each country s Movie and TV Show library size then the average rating score was calculated Country level Metrics Table Features used for Model Training Data are extracted from World Bank and Our World in Data Year data is selected as the latest representation There are some countries that do not have data therefore we choose the latest available data before We found there are still missing values in the dataset By leveraging other similar information e g Country ranking by happiness index and broadband subscription information a portion of missing values are filled by other countries metrics that have a similar ranking For remaining missing values that can t be found with the understanding of feature values distribution via boxplot in Figure data especially GDP and population are quite skewed in the end the median value is chosen to fill in missing data Figure Feature Values Distribution via Boxplot Weekly top watches on Netflix The following two features were built to associate users preferences on content offered by Netflix  using top movies and TV Shows data from Netflix s Top list Table Netflix used for Model Training Figure Average Rating over Country Ranges Figure Number of Votes Lookup The weekly top dataset is available on Kaggle We matched the title of contents from top dataset with the title in IMDB dataset for rating and number of votes lookup The average rating over countries ranges from to with a standard deviation of in Figure There are also some areas that can be improved in terms of feature engineering for these two features Only the top movie data in week was used to compute average ratings and number of votes it could be more accurate and robust if all data over a long time span can be used for the feature extraction Some title of movie or TV shows is associated with multiple instances in the IMDB dataset To simplify the feature engineering process the latest released movie or tv show is eventually matched to the title from top dataset to obtain ratings and the number of votes Models Feature Importance Using following machine learning and deep learning techniques on Netflix subscription fee and external data at region level to build a prediction model to achieve relatively high accuracy Models Random Forest Random Forest is the commonly used model for resolving regression problems It takes average numbers based on the decision trees built on different samples Applying bootstrap aggregation bagging ensemble method constructs individual decision tree based on the sample selected and each will generate an output The final output is based on the average figures of all the decision trees Random forest model helps to avoid the curse of dimensionality as each tress does not consider all the features At the same time as each tree is created separately and run in parallel the computing efficiency could be optimized Hyperparameters like n estimators max features and mini sample leaf will help to increase the predictive accuracy while n jobs and oob score will increase the speed In our project prediction result of three service categories Basis Standard Premium are populated first where prediction for Premium subscription fee generated the lowest MAPE of Figure Random Forest GridSearch Results By applying the grid search to adjust the key hyperparameters below are the best set with variance among six hyperparameters The result has improved significantly as compared to the default setting which is shown in the table below Table RF Parameters Improvements Table RF Subscription Type and MAPE   LightGBM Light Gradiant Boosting Machine is also a decision tree based algorithm with distributed high performance framework The advantage of the LightGBM is the fast training data hence gains the popularity for the large dataset problems It also supports both parallel learning and GPU learning which makes the algorithm more attractive Though the dataset for this project is relatively small it is worthy to develop the prediction model based on LightGBM For the LGBM Regressor larger num leaves leads to better accuracy though it increases the probability for over fitting max depth and lambda will help to deal with over fitting Figure Light GBM GridSearch Results Similarly after obtaining the prediction on test data for three different categories The grid search is applied to find out the best hyperparameters for the LightGBM model Unlike Random Forest the best hyperparameters do not give much better results Table LightGBM Parameters Improvements Table RF Subscription Type and MAPE XGBoost XGBoost a popular decision tree based ensemble method which boosts the performance of weak learners to attain the performance of stronger learners is applied to our dataset We use Grid Search for hyperparameter tuning and mean absolute percentage error MAPE as the evaluation metric for best parameter selection XGBoost has a lot of hyperparameters that can be adjusted which may easily lead to overfitting Furthermore the dataset size is quite small so when a complex algorithm like XGBoost is applied the risk of overfitting increases drastically After tuning the model training MAPE is around and the test MAPE is around Figure visualizes the actual and prediction result and the prediction error is significant Figure XGBoost Actual vs Prediction Result Neural Networks A neural network is a computational system that can create predictions based on existing data It is used Supervised learning in this case and the chosen features that form the input for this neural network are followings Total Library Size No of TV Shows No of Movies Movie aveRating TV aveRating GDP Population Gini Index Happiness Share Life Satisfaction Broadband subscriptions Cellular subscriptions averageRating and numVotes Cost Per Month Standard has been set as the y variable Coming to the neural network configuration the most important considerations when training a neural network is choosing the number of neurons to include in the input and hidden layers Given that the output layer is the result layer which will be the standard cost this layer has neuron present by default Based on the dataset in this project the number of neurons in each layer is configured as below Input layer Number of features in the training set In this case as there were features in the training set to begin with input neurons are defined accordingly Hidden layer The number of neurons in the hidden layer Training Data Samples Factor Input Neurons Output Neurons A factor of is set in this case the purpose of the factor being to prevent overfitting With neurons in the input layer neuron in the output layer and observations in the training set the hidden layer is assigned neurons Output layer As this is the result layer the output layer takes a value of by default   Figure Neural Network Model Configuration Figure Model Loss with Increasing Epoch With the expectation that the loss will reduce while increasing each epoch to fit the model which means the model is predicting standard cost more accurately when the model has been continued trained The training loss has decreased as the number of epochs is increased meaning that model gains a high degree of accuracy as number of forward and backward passes is increased Eventually by passing the test data to the same model to predict the standard cost and the results in MAPE of testing data Feature importance Figure and Figure show Random Forest and lightGBM feature importance ranking Broadband subscription Happiness Share Life Satisfaction are top three features that affect the model prediction Netflix library features like No of Movies and Total Library Size seem to be less important Figure Random Forest Feature Importance Figure LightGBM Feature Importance A quite different feature importance ranking is obtained from XGBoost model as shown in Figure The top features influencing the model are averageRating Happiness Share Gini Index GDP and No of Movies for each country Figure XGBoost Feature Importance Conclusion and Recommendations Model Evaluation Table Model Performance Comparison RandomForest has the lowest RMSE MAE and MAPE compared to other models With the increase of model complexity all three metrics of error are getting larger For instance the error of neural network is higher than RandomForest in terms of RMSE For complex model such as neural network data size is crucial part of the training process due to substantial number of parameters needs to be tuned in hidden layer Therefore a complex model may not be an ideal option for a prediction problem if the data size is as small as the dataset used in this work Most importantly one of the best practices for machine learning modeling is to always start with a simple model followed by adding more complexities in machine learning algorithms we choose Business Insights Netflix subscription fee is highly correlated with broadband which reveals users in countries with higher Internet popularity must pay more for monthly subscription of streaming service Secondly  customers are not getting a larger size of movie library if they pay more as the correlation between subscription fee and movie library size is for standard subscription This interestingly defies the common sense of users getting what they paid Also subscription is cheaper in countries with a higher Gini index which can be interpreted as subscription fee is to be set at a lower range so that the content provider is able to get a larger number of paid subscribers to reduce the marginal cost of upfront investment of entering this market where income and wealth inequality is significant In another word this is to ensure that the subscription service has a minimum number of users to cover the upfront investment cost to ensure the service is profitable Business Applications With best performance model trained by Random Forest quantitative evaluations could be analysed which will help on management decision to expand the service network Two potential business scenarios could be applied using the established model and pricing strategy is only advised for standard subscription which aligned with assumptions made in the model evaluation part Firstly when entering a new market what is the recommended price to be set given fixed macro data for a specific country and pre defined library size For the same country how will the price change in response to the library size It is critical to understand if increasing resource available in the platform will help on the pricing as each resource on boarding to the platform occurs authorization expense In the elementary exploration we selected China Egypt and Vietnam as examples to test out considering the economic scale and representativeness For each country we set five different library sizes for prediction based on the distribution of the train dataset It is interesting to find out that the higher library size does not always mean the higher pricing based on the model trained For example both Egypt and Vietnam return the highest pricing under while highest price for China market is under library size of Table Library Size Information Table Library Size v s Standard Cost Secondly for the existing markets is there any opportunity to improve the revenue by adjusting the library size The test countries chosen are North Europe countries namely Denmark Finland Norway and Sweden We noticed that for these four countries the average subscription fees are higher among others as they are well developed countries with high income level but library size for counties are relatively small Hence it is worthy to examine if increasing library size will help on subscription fee Applying the same sets of library sizes for these four countries it is aspiring to find out that for Denmark Finland and Norway slightly increasing the library size to will be able to generate the highest pricing However for Sweden the original library size is still the best Table Library Size v s Standard Cost From above explorations we could conclude that it is not necessary to put all the resources on expanding the library resource as the subscription fee will peak  at certain range The conclusion seems to be a good reflection of current situation Netflix is facing While investing plenty of money and resource on the original series the new users did not grow as per expectation which resulted in a bad financial performance for Q and the stock market has responded dramatically In order to sustain the business growth Netflix could spend more capital to acquire new users and enhance the market penetration However here the only variable considered is the library size and each movie or TV show is treated identically The size of the subscribers is not included as well due to confidentiality hence the exact profit analysis is not able to be calculated Limitations and future improvements Netflix is available in over countries as of now However the subscription dataset only reveals price insights of approximately one third of countries A larger dataset with more countries would be ideal for machine learning problems in terms of robustness of model and accuracy of predicted results The title of top movies and TV shows was fuzzily matched with IMDB rating in feature engineering This could be calibrated manually to reduce the mismatched ratings so that the average rating and number of votes would be more accurate Reference Investopedia How Netflix Is Changing the TV Industry online Available at https www investopedia com articles investing how netflix changing tv industry asp Backlinko Netflix Subscriber and Growth Statistics How Many People Watch Netflix in online Available at https backlinko com netflix users whattowatch com Disney Plus price what it costs in all the countries where it s available online Available at https www whattowatch com watching guides disney plus price what it costs in all the countries in which its available Comparitech Which countries pay the most and least for Netflix Comparitech online Available at https www comparitech com blog vpn privacy countries netflix cost IMDb IMDb online Available at https www imdb com interfaces Data worldbank org World Bank Open Data Data online Available at https data worldbank org Our World in Data Our World in Data online Available at https ourworldindata org Goshwami S World Happiness Index Country wise Rank Report online DMER Haryana Recruitment News Admit card result Available at https dmerharyana org world happiness index En wikipedia org List of countries by number of broadband Internet subscriptions Wikipedia online Available at https en wikipedia org wiki List of countries b y number of broadband Internet subscriptions Netflix Top Global Top Weekly Top lists of the most watched TV and films Available at https top netflix com Netflix Top Weekly Dataset Available at https www kaggle com datasets mikitkanakia ne tflix top weekly dataset Brownlee J How to Develop a Light Gradient Boosted Machine LightGBM Ensemble online Machine Learning Mastery Available at https machinelearningmastery com light gradient boosted machine lightgbm ensemble Lightgbm readthedocs io Parameters Tuning LightGBM documentation online Available at https lightgbm readthedocs io en latest Paramete rs Tuning html Code Link https github com yzhou bt project Appendix A Correlation Matrix
Group Final Report Beware of Job Scams Fake Job Prediction Group Lam Fu Yuan Kevin A B Lee Hui Gek Judy A Y Lim Wei Ren A N Mansi Agarwal A J Zhang Jie A B Introduction Background As Singapore advances into a Smart Digital Nation digitization of services has enhanced our convenience but also act as a double edged sword as they expose users to the risk of scams During the Covid pandemic adoption of contactless digital payments increased tremendously and created a large pool of targets for scammers While many people were displaced from their jobs due to the pandemic the incidences of reported job scams skyrocketed due to the lure of jobs advertised to be convenient and high paying An instance of such scams misled victims to websites that enticed them to subscribe packages and to transfer money to unknown bank accounts to be paid the commission Lim Sun Alternatively scammers use fake job listings to trick victims into disclosing personal information allowing them access to victims online credit cards or bank accounts Rafter Straits Times reported that in the first six months of there were cases of job scams a fold increase from in the same period of During these six months victims lost S million which is a considerable increase from during the same period in Sun Lim Large international companies are not spared from recruitment fraud Since early video game giant Riot Games has been trying to deal with scammers who lured eager professionals into handling sensitive data by dangling fraudulent job postings Some recruiting websites even allowed potentially false jobs to be posted on an official company page appearing next to legitimate listings The US Federal Bureau of Investigation estimate these scams to cost victims an average of US and often cause long term harm by negatively impacting victim s credit scores Janofsky The scale and impact of this problem has given recruitment giants such as Linkedin impetus to act Between Jan to Jun LinkedIn removed million spam or scam content of which of these were stopped by automated defenses Linkedin Problem Statement Back in Singapore Government Agencies have developed ScamShield that helps to block scam calls and filter scam messages Scamshield n d However there is no tool widely available to the job seeking population for discerning fake job listings We believe machine learning can be effectively applied to tackle this problem by training a model for fake job classification For the purpose of this project we will be focusing on job scams from job portal sites where it is more difficult to determine authenticity compared to questionable job offers received via messaging platforms such as Whatsapp Business Value Coming from different perspectives we have summarized the benefits which the different audiences can reaped in Table With a proven model design we can extend the usage to detect job portals sites that have a high percentage of potential fake jobs This would be an attractive tool for search engines Table Business Value for Different Audiences For Job Seekers For Job Portals For Job Posters Benefits Reduce time wasted Minimize risk of phishing Increased confidence Improved accuracy in detecting fraudulent postings Increased platform trust Increased user growth stickiness Attractiveness to partners Reduced false positives from existing flagging mechanisms Willingness to Pay Low High Medium Research Questions We aim to answer the following three research questions RQ Which features are most associated with fraudulence RQ What are the topics present in the company profile job description job requirements and job benefits RQ Which machine learning model is most suitable to predict fraudulence   Beware of Job Scams Fake Job Prediction Data Data Source The Employment Scam Aegean Dataset EMSCAD was downloaded from the Laboratory of Information and Communication Systems Security Department of Information and Communication Systems Engineering at the University of Aegean The EMSCAD contains job advertisements published on a recruitment software between and each job advertisement was classified as either fraudulent or not fraudulent by specialized employees of the recruitment software Table describes both the features and the target variable Fraudulent in the dataset http emscad samos aegean gr https www mdpi com htm Data Pre Processing The data was pre processed in three steps In the first step some nominal variables such as employment type required experience and required education were bucketed to reduce the granularity In the second step all variables were processed to indicate whether a sample is missing a value on each of them Missing Not Missing This create a new missingness feature In last step we concatenated strings from department company profile description requirements benefits industry and function sections into a single feature After having concatenated the strings we applied the following steps in order to pre process them Convert all alphabets to lowercase Remove all punctuations Remove all numbers Remove all stopwords Remove all non English words Remove all rare words Lemmatise words No Name Type Description Job ID Nominal Serial number e g Job Title String Title e g Infrastructure Engineer Job Location Nominal Geographical location e g US CA California Job Department Nominal Corporate department e g Accounting Job Salary Ordinal Indicative salary range e g Company Profile String Company description e g Our mission to clients is Job Description String Job description e g Drive the sales effort Job Requirements String Job requirements e g Proven leadership experience Job Benefits String Job benefits e g Fun supportive team Telecommuting Nominal Telecommuting position Yes No Company Logo Nominal Presence of company logo Yes No Questions Nominal Presence of screening questions Yes No Employment Type Nominal Employment type e g Full Time Part Time Required Experience Ordinal Required experience e g Entry Level Mid Senior Level Required Education Ordinal Required education e g Bachelor s Degree Master s Degree Job Industry Nominal Industry e g Banking Design Job Function Nominal Function e g Administrative Advertising Fraudulent Nominal Fraudulence Fraudulent Not Fraudulent Notes A nominal variable is a variable whose values are treated as categories without a hierarchical ordering an ordinal variable is a variable whose values are treated as categories with a hierarchical ordering and a string is a variable whose values are treated as text Table Description of Variables in the Employment Scam Aegean Dataset N   Beware of Job Scams Fake Job Prediction Figure Fraudulent Word Cloud Figure Fraudulent Figure Fraudulent Figure Fraudulent Figure Non Fraudulent Word Cloud Figure Not Fraudulent Figure Not Fraudulent Figure Not Fraudulent These steps improved the performance of the LDA model Only one out of samples were removed as a result of the application of these steps The sample was removed because it contained too few words with Office Manager as its title there was no other information in the department company profile description requirements benefits industry and function sections Weightage of each topic generated from topic modelling were added into the dataset as new features for machine learning except for CNN and BERT For Convolutional Neural Network CNN modeling all text features were combined and tokenized to words which were converted to numbers via text to word sequence Word sequencing forms the input features for CNN Exploratory Data Analysis Initial exploratory data analysis was conducted to examine the distribution of the target variable the associations between values on the feature variables and those on target variable and the associations between missingness on the feature variables and values on the target variable Some interesting insights found are First the distribution of the target variable is skewed Fraudulent Non Fraudulent This suggests that we should consider techniques including but not limited to random oversampling or synthetic minority oversampling technique SMOTE Second the presence of company profiles company logos and screening questions whether the job was a telecommuting position and the employment type were associated with fraudulence Fraudulent job posts mostly lacked company profiles and company logos and these posts also eradicated the need for screening questions in the survey portraying an easy to apply hiring process to attract more applications Hence we notice fraudulent job posts were more likely to be telecommuting positions Fraudulent Non Fraudulent and were less likely to be full time positions Fraudulent Non Fraudulent Third a considerable number of features suffer from a high percentage of missing values e g Job Salary Job Department Required Education Job Benefits Required Experiment It is interesting that fraudulent job posts were more likely to provide salary information Fraudulent Non Fraudulent This is consistent with our domain understanding that most legitimate companies have policies that discourage the presentation of such information in public domains For example job salary is confidential and not disclosed to avoid competitors setting a higher benchmark Last given that the dataset contains features of string types Job Title Company Profile Job Description Job Requirements and Job Benefits we attempted to tokenize these text strings into words and created word clouds as shown in Figure Non Fraudulent and Figure Fraudulent Based on preliminary visual analysis word such as full time appears more frequently in non fraudulent job listings than fraudulent job listings Another interesting insight is the word experience and project have a high count while customer service seems to be appear as frequent in both non fraudulent and fraudulent job listings Topic Modelling Before employing supervised models we used Topic Modelling an unsupervised ML technique which is a Figure Comparison of proportions as a of total fraudulent non fraudulent jobs respectively   Beware of Job Scams Fake Job Prediction Figure Determining the optimal number of topics quick way to gather the cluster words and does not require training Topic models are algorithms that enable the discovery of hidden topical patterns or thematical structures in a large collection of documents Latent Dirichlet Allocation LDA is one such algorithm LDA seeks to maximise the separation between the means of projected topics and minimise the variance within each projected topic The topic model allows us to deduce what each set of texts are talking about A limitation is that topic modelling does not guarantee accurate results Model Selection The optimal number of topics was determined using the elbow method Because the elbow method involves a grid search over multiple numbers of topics which has high computational cost it was conducted on a random subset of job advertisements After having identified the optimal number of topics the LDA model was then fit on the entire dataset The results of the grid search suggests that a model with topics would fit the data sufficiently well Figure Model Interpretation For each topic we looked at the top most relevant terms to deduce what each set of texts represents In general the topics were associated with the sector the company or the skills of the corresponding job advertisement Topics and are associated with the sectors data web development healthcare manufacturing and education respectively Topics and are associated with the companies Upstream Spectrum Learning Tidewater Finance Co Ixonos Applied Memetics ABC Supply Co Vend Network Closing Services and Novitex Enterprise Solutions Topics and are associated with the skills communication stakeholder co ordination and teamwork respectively Table summarises each of the topics based on their most representative observations and words Topic Feature Impt Top Job IDs Top Words Associated Topic Description system data support inform technic Jobs related to system and data will work new learn within Jobs related to Upstream company develop web test use end Jobs related to web development recruit candid will career avail Jobs related to Spectrum Learning company benefit employ employee posit paid Jobs related to Tidewater Finance Co company design product project work team Jobs related to Ixonos company OR jobs related to UX design market brand media digit social Jobs related to Applied Memetics company sales custom product will return Jobs related to ABC Supply Co company work great make look can Jobs related to Vend Company OR a Company profile that has lot of office benefits care home train health assist Jobs related to Healthcare skill excel strong work must Jobs that require strong communication skills client account profession close success Jobs related to Network Closing Services company that deals with settlement of property transactions plan organ develop maintain report Jobs that require planning and coordination with stakeholders custom process perform document product Customer Service Role with Novitex Enterprise Solutions company team grow build fast lead Team based roles related to growth in partnerships clients job will high time posit Full time positions in Manufacturing amp help job get month Jobs related to overseas educator Table Summary of the Topics   Beware of Job Scams Fake Job Prediction Classification Model Selection and Evaluation Models We conducted fraud detection using the following six machine learning models as shown in Table Logistic Regression Random Forest Gradient Boost CNN Na ve Baysian LightGBM and BERT Model Description Logistic Regression Logistic regression is an appropriate regression model for binary classification problems Logistic regression predicts the probability of outcome and classifies the data into different classes base on the pre set probability threshold Features generated from Topic modeling were input features for model training Random Forest Random Forest is a bagging ensemble method that builds multiple uncorrelated trees Each tree gives an independent prediction of the outcome The outcome with the highest number of combined counts from the trees determines the model s prediction Random forest has good predictive performance with reduced variance and bias Features generated from Topic modeling were input features for model training Optimal model is identified with the following hyperparameters maximum features min samples leaf maximum depth min sample split n estimators Gradient Boost Gradient Boost is a boosting ensemble classifier that builds simple decision trees sequentially with each tree built to predict the residual error from the previous tree Gradient Boost is useful for weak learners classification and the algorithm aims to minimise the loss functions Features generated from Topic modeling were input features for model training Hyperparameters of Gradient Boost were tuned to identify models that maximise the recall Optimal models are identified with the following hyperparameters learning rate n estimators minimum sample splits maximum depth maximum features sqrt Convolutional Neural Network In Convolutional Neural Network CNN filters of different matrix size were applied on the word vectors and output to convolved vectors Max pooling of convolved vectors reduces the feature dimensions and prevent overfitting Classification is performed at the fully connected layer which receives inputs from convolutional and pooling layers CNN is a black box model with lowest interpretability but can achieve high prediction accuracy The text to word sequencing prepares input vectors for CNN model Na ve Bayes Na ve Bayes Classifier is based on Bayes Theorem for calculating probabilities and conditional probabilities to predict class of unknown data set Classifier assumes that presence of particular feature in a class is independent to the presence of any other features Features generated from Topic modeling were input features for model training LightGBM LGBM is an implementation of Gradient Boosting Decision Tree algorithms with combination for gradient based on side sampling GOSS and exclusive feature binding EFB techniques GOSS excludes data with smaller gradients preferring instances with larger gradients for calculating information gain EFB puts mutually exclusive features to reduce number of features without compromising accuracy of split point LGBM has faster training speed parallel learning support and low memory utilization GridSearchCV provided following tuned hyperparameters learning rate max depth metric binary logloss num leaves objective binary BERT Bidirectional Encoder Representations from Transformers BERT is designed to pre train deep bidirectional representations from unlabeled text BERT s key innovation is applying bidirectional training of Transformer to language modelling Transformer attention mechanism learns contextual relations between words based on all its surroundings text Bidirectional training uses Masked Language Modelling MLM technique Pretrained classification model on large corpus of unlabeled text including entire Wikipedia is fine tuned with additional Binary Classification output layer Table Description of Machine Learning Models for Job Posting Fraud Detection   Beware of Job Scams Fake Job Prediction Metrics To evaluate the performance of our approach we used accuracy as one of the metrics as it is commonly used to express rates of correct predictions and the second metric to be used is the F measure which is calculated from precision and recall In context to our use case accuracy will measure the percentage of correct labels detected for fraudulent fraudulent and non fraudulent jobs fraudulent accuracy measure based on this imbalanced data set will mislead our interpretation of result hence techniques such as smote was applied The Area Under ROC Curve AUC quantifies the ability of classifiers to distinguish between fraudulent and non fraudulent labels The higher the AUC the better our model is performing in distinguishing the positive and negative class labels We focused on improving recall wherein it is acceptable for our system to flag non fraudulent jobs mistakenly as fraudulent Models based on balanced train dataset were then evaluated using Recall implying that the cost for wrongly classifying a fraudulent job as non fraudulent is high and we should aim at increasing classification of True Positives F measure is a combined measurement of precision and recall which helps in striking a balance between classifying the True Negatives and restricting the system from over sensitivity for fraudulent class label Models Performance After applying SMOTE technique to overcome the class imbalance Non Fraudulent Fraudulent the TRP scores Key Metric were higher for than the models performance on the untreated imbalanced data Based on the plot of our ROC curves for the various models Figure Random Forest is suggested to be the best choice for our problem We also observe the slight improvement in ROC values from SMOTE application as compared to the untreated data Figure We also observe Model Performances from CNN and BERT are generally high despite not using the topic modelling features Our suspicion is that there is information loss during data processing for topic modelling However we did not choose CNN and BERT as final best model as both models predict on text features Other features which give significance to model prediction were not used by these models a refers to the number of true positives i e job listings that are correctly predicted b refers to the number of true negatives i e job listings that are correctly predicted c is the number of false positives and d is the number of false negatives Figure Combined ROC curves after SMOTE treatment Balanced Training Data Figure Combined ROC curves on Imbalanced Data   Beware of Job Scams Fake Job Prediction Results Interpretation Here we would like to share our findings on the research questions set out at the start of our project RQ Which features are most associated with fraudulence After generating a feature importance ranking using the Random Forest Model see Figure we can see that a majority of the top features are generated from topic modelling An interesting observation is that the top two features in fraud prediction are Having company logo and Missing a company profile RQ What are the topics present in the company profile job description job requirements and job benefits In general the topics were associated with the sector the company or the skills of the corresponding job advertisement RQ Which ML model is most suitable to predict fraudulence The SMOTE treatment helped to improve the TPR scores across all models The best performing models by AUC score based on the SMOTE treated Balanced training data are Random Forest and Gradient Boost We further decide Random Forest to be the most suitable model as it has a better TPR than Gradient Boost Metric Na ve Bayes Logistics Random Forest Gradient Boost Light GBM CNN BERT Imbalanced Training Data Accuracy Precision TPR TNR AUC SMOTE treated Balanced Training Data Accuracy NA NA Precision NA NA TPR NA NA TNR NA NA AUC NA NA Figure Feature Importance Ranking using Random Forest Model Table Summary of Model Performance Evaluated on Testing Data   Beware of Job Scams Fake Job Prediction Conclusion Business Application By deploying our selected model job portals will be able to detect potential fraudulent job postings and prevent any applications to be received till the job posters provide additional supporting information Business value to the Job Portal To convince Job Portals to utilize these models we employ the Expected Value Framework Sukup to quantify the business impact they can expect to achieve The formula used is as follows and in essence a multiplication of values between Cost Benefit Matrix and Probability Matrix The Probability Matrix is derived from the confusion matrix see Figure of our selected model Random Forest E X P p P TP p V TP p P FN p V FN p P n P FP n V FP n P TN n V TN n P refers to probability of observing the class V refers to the associated value of the observed class TP refers to the Class True Positive FN refers to the Class True Negative p refers to Actual Positive observations n refers to Actual Negative observations Table Cost Benefit Matrix ACTUAL FRAUD ACTUAL NON FRAUD PREDICTED FRAUD PREDICTED NON FRAUD Requires days of work from a recruiter with a monthly salary of S Based off latest news report Lim Sun Table Probability Matrix based on RF Confusion Matrix ACTUAL FRAUD ACTUAL NON FRAUD PREDICTED FRAUD TP FP PREDICTED NON FRAUD FN TN TOTAL P N Expected Value from our Anti Job Fraud Modelling E X Positive benefit of per potential fraudulent job posting In summary by implementing our model the job portal can expect to receive an overall positive benefit equivalent to S for every potential fraudulent posting identified Limitation of Current Study Our existing dataset is based on the US market and may not accurately reflect the characteristics of the local i e Singapore market due to the lack of data from incumbent job portals Our models and packages are currently run based off text data from job postings listed in English We may not achieve the same performance while applying to jobs postings of other languages Possible Future Work We can improve the performance of our models by aggregating datasets across multiple job portals of the target local market by attuning to the characteristics of each region Figure Confusion Matrix of Random Forest   Beware of Job Scams Fake Job Prediction References Janofsky A February Scammers continue to spoof job listings to steal money and data FBI warns The Record by Recorded Future https therecord media scammers continue to spoof job listings to steal money and data fbi warns Lim J Sun D January Five types of job scams in S pore and how to avoid them The Straits Times https www straitstimes com singapore courts crime five job scam variants Linkedin Community Report Transparency Community Report https about linkedin com transparency community report Rafter D February Job posting scams and how to avoid them NortonLifeLock https us norton com internetsecurity online scams job posting scams html Scamshield n d ScamShield https www scamshield org sg Sun D Lim J January m lost in months More falling prey to job scams The Straits Times https www straitstimes com singapore courts crime job scams the next big worry says head of anti scam centre Sukup J November Explain the So What Behind Machine Learning Models with the Expected Value Framework Part of Oracle AI Data Science Blog https blogs oracle com ai and datascience post explain the quotso whatquot behind machine learning models with the expected value framework part of Codes and Data GitHub lttlrain Group Project Group project Job Scam
Question Answering QA Tool for COVID BT Group Project Xhoni Shollaj A N Philippine Gallot A B Remy Masbatin A N Sahil Sharma Isaac Sadikin A U A E SECTION Problem Statement Coronavirus disease COVID is an infectious disease caused by the SARS CoV virus Although infected people will have mild flu like symptoms COVID can expose older and more vulnerable populations to greater risks As of April globally over million people were infected and over million people died of the disease Given the new nature of this coronavirus people have been asking many questions surrounding COVID Searching for answers online is the most natural next step yet sources may not be reliable and contradict each other On the one hand for governments making sure people quickly get access to the latest most reliable and most relevant information to answer their interrogations is crucial to the management of the crisis For example in Singapore people infected with COVID used to have to be quarantined in dedicated facilities even though they were asymptomatic Now most infected people can stay at home to recover for a few days For the Singaporean government making sure people know this is crucial to prevent hospitals from being overburdened On the other hand for the general public searching accurate COVID related information can be time consuming and cumbersome with various online platforms that offer multiple information There may not always be a research tool and on some websites it may be necessary to go through a lot of questions before finding an appropriate answer The risk is that people would directly type the question in their search engine as the first answers may not be up to date or accurate Therefore there is a need to create a Question and Answer QA system whereby the accurate COVID information can be made swiftly and easily available to many people thereby reducing the amount of time in searching in open source Delivering accurate responses could in turn help fight misinformation and allow the general public to act responsibly and knowingly   SECTION Data Source Data Preprocessing and EDA The COVID Open Research Dataset CORD is a growing resource of scientific papers on COVID and related historical coronavirus research We used the historical releases of CORD from Allen Institute which is prepared by a coalition of leading research groups using over scholarly articles While the dataset is updated on weekly basis we used the version that was released on November file cord Every version release of the corpus is tagged with a date stamp and has the following files changelog A text file summarizing changes between this and the previous version cord embeddings tar gz A collection of precomputed SPECTER document embeddings for each CORD paper document parses tar gz A collection of JSON files that contain full text parses of a subset of CORD papers metadata csv Metadata for all CORD papers We used the metadata of the articles which include features such as sha hash value of encryption algorithm paper title Paper Title publish time Date of Publishing of Paper authors Name of Paper Authors Abstract Paper Abstract url the site from which paper is fetched etc and used pickle library to serialize the structure in a dataframe A brief overview of the metadata features can be found in Appendix We used the pickle library to serialize the PDF documents and the JSON files from the cord embeddings tar gz document which contained a collection of precomputed SPECTER Document level Representation Learning using Citation informed Transformers document embeddings for each CORD paper for which the features included question question asked by user answer answer for the query score assigned score to the question answer probability confidence level for the answer context text containing the context around the question offset start starting index of the answer offset end end index of the answer and document id ID od document containing the answer High level of how the data set is created and how the query would be processed against the research papers document store we would create   Further the serialized JSON of PDF and PMC files were merged to include body text entire text of the abovementioned files with the main metadata dataframe Basic sanity of the data was carried out by removing stopwords lowercasing text null values and we then analyzed the abstract and body text features and retained the more accurate and complete versions in the main dataframes We included samples from the data The knowledge and relevant information from CORD dataset were retrieved to illustrate extracting valuable information from scientific papers The results suggest that papers include a wide variety of the many entity types that are engineered and that assertion status detection is a useful filter on these entities First let s explore the date of publication of documents in our dataset On the chart below it appears that the papers were published between and with most papers being published in and It is surprising that some papers were published in but these papers must be centered around precedent coronaviruses Interestingly the chart on the right shows that most papers were published around the spring and summer with a peak in May and June A smaller number of papers were published during the end of the year September to December As for the day of publication the distribution of days is almost uniform with a peak on the first day of the month It can either be a practice to publish a paper on the first day of the month or it is also possible that when the day of the month is unknown it is set to the first day of the month by default What s more the most frequent phrases from the selected entity types can be found in the wordcloud below Unsurprisingly the word covid appears twice and other words among the most important words include pandemic sarscov and patient   Finally the two word clouds below show the content of the two main topics extracted from the data The method used to extract these two topics is Latent Dirichlet Allocation LDA LDA is a popular topic modeling technique to extract topics from a text corpus Latent means hidden or concealed conveying the idea that the topics extracted from the data are hidden topics LDA is built on the idea that each document can be described with a distribution of topics and each topic can be described with a distribution of words These topics are precisely the latent or hidden layer since they are not observed Notably the Topic cites words in the lexical fields of epidemiology and medical studies such as study method and model On the other hand the Topic gathers words around the topic of illnesses such as virus infection disease and biological words including viral protein cell  SECTION Methodology To build our Extractive QA engine we used Docker container images to host the application used Haystack library for developing an end to end question answering systems along with elasticsearch retriever model to fetch the data received set up the backend of the tool through FastAPI and built the UI for user to see question answers in Streamlit We started by pulling elasticsearch image from the docker hub ran elasticsearch docker and mapped local server port with the host server We ran a docker container in single node discovery type to setup our application Further to run the retriever model we populated documents in the document store by converting the data to dictionary key value pairs to make it compatible with the elastic search format and used elastic search to access the document store for data by running it on Docker Image After setting the retriever model we initiated a query reader pretrained model deepset roberta base squad covid from huggingface co a model trained specifically with large number of QA pairs from articles written by scholars on COVID and then setup Extractive QA Pipeline We setup the core component of the tool the QA engine which was built on Docker It receives user request through its API endpoint reads the data from knowledge base runs the QA pipeline and sends the answer back to the user In the frontend we let the user select top best results from up to number of documents   Sample Overview of the Streamlit Web App  SECTION Conclusion In a nutshell this project aimed to create an Extractive QA engine whereby information on COVID could be made easily available to the general public Not only is this QA system beneficial to the general public but also to governments and healthcare institutions as it makes it possible to diffuse centralized complete and accurate information about COVID The main steps involved in this project started with the gathering of research papers on COVID and their cleaning and integration to a database Then thanks to state of the art Docker Haystack Elasticsearch and Streamlit the outcome of this project is a user friendly interface where the user can type a question and choose both the maximum number of answers and the maximum number of documents from the retriever The current prototype of the QA system is confined till the general knowledge of COVID Different countries have different sets of measures in dealing with COVID There are details of approaches how to handle COVID by each country Further development can be done by including several countries approaches and training even more data to test out the accuracy of the model Gathering more data and adding more processing data as well as hyperparameter optimization will assist in improving the accuracy of QA system Dockerize the whole webapp and deploy to the cloud for faster outcome of answer to the queries as well as bigger datasets As the topic of the matter is delicate as it relates to public health matter a robust evaluation of the system must be carried out to prevent further misinformation being trickled down to the public which cause unnecessary additional burden to the healthcare system High lebel next steps for the Web App Deployment And Continuous Integration Flow   SECTION Appendix SECTION Appendix Appendix Metadata overview of CORD dataset Feature Data type Description Comments cord uid str Unique identifier to each CORD paper Not necessarily unique per row As same paper can come from different sources sha List str Secured Hash Algorithm SHA Id s of all the pdf s associated with CORD dataset Mostly papers will have either zero or single value here Some papers will have multiple indicating more than one pdf associated with single paper source x List str Sources of paper Single paper can have multiple sources title str Paper title doi str Digital object identifier DOI of the paper pmcid str Paper s Id on PubMed Central Begins with pmc followed by an integer pubmed id Int Paper s Id on PubMed license str License associated with the paper abstract str Paper s abstract publish time Str Published date of the paper In yyyy mm dd format  authors List str Authors of the paper Each author name is in Last First Middle format and semicolon separated Journal Str Paper journal Strings are not normalized mag id Int Value field for the paper as represented in Microsoft Academic graph Deprecated who covidence id Str ID assigned by the WHO for this paper arxiv id Str arXiv ID of the paper pdf json files List str Paths from the root of the current data dump version to the parses of the paper PDF s in Json format Multiple paths are semicolon separated pmc json files List str Same as above but corresponds to the full text XML files taken from PMC parsed in to the same Json files as above Multiple paths are semicolon separated url List str All URL s associated with the paper Semicolon separated s id Semantic Scholar ID for this paper Can be used with the Semantic Scholar API e g s id corresponds to http api semanticscholar org corpusid Appendix Basic EDA on the text corpus Some basic statistics on word counts character counts word density and punctuation count confirm that titles and abstracts contain less words and therefore characters and punctuation and they have a  smaller word density On the other hand the length of papers measured in body text varies a lot and the word density is slightly higher Let s have a look at polarity and subjectivity Polarity analysis takes into account the amount of positive or negative terms that appear in a given sentence Subjectivity quantifies the amount of personal opinion and factual information contained in the text We observe that the average polarity and subjectivity is slightly above zero for both abstracts and body texts   SECTION References CORD The COVID Open Research Dataset Allen Institute Datasets Roberta base squad covid Model https github com deepset ai COVID QA blob master README md https github com allenai cord https arxiv org abs https www udemy com course deep learning nlp build and deploy bert covid qa system https huggingface co deepset roberta base squad covid
Spotify Music Recommendation BT Applied Machine Learning in Business Analytics Group Project Report Li Rui A J Tang Ning A U Fu Hanqi A Y Gan Bingzheng A A Pei Ziang A H Abstract Online streaming music industry has grown rapidly in recent years How to provide mean ingful and engaging song recommendations to users becomes increasingly important Recom mendation system has been widely adopted by industries like e commerce to improve user expe rience and grow business impact and can benefit the music industry in a similar way Thus in this project we explored different ma chine learning based recommendation models us ing user preference and song feature datasets from Spotify These models include content based filtering user based collaborative filtering NMF model SVD model and auto encoder model We also explored combining these models into an ensemble model After evaluating with Hit Ratio and NDCG user based collaborative fil tering is identified as the best performing model Introduction As we know in recent years recommendation systems that use machine learning algorithms to improve user experience and business performance have been widely used especially in the music streaming media industry which greatly im proves the user experience and viscosity of users Back in the s Songza kicked off online music curation using manual curation to create playlists for users But it was manual and simple and therefore couldn t take into account the nuance of each listener s individual music taste Later Pandora improved this in a creative way by employing a slightly more advanced approach to manually tag attributes of songs They asked a group of people to listen to music and chose a bunch of descriptive words as tags for each track In this way Pandora s algorithm could simply filter for certain tags to curate playlists of similar sounding music Nowadays global audio streaming subscription service gi ant Spotify which reportedly has million subscribers and million tracks has invested heavily in building recommendation features Discover Weekly is Spotify s fully personalized playlist management at scale with more than billion hours of total playtime years after its launch Bandits for Recommendations as Treatments or BaRT for short is one of the key recommendation algo rithms running on Spotify that generates weekly lists of the most relevant and engaging music for users every Monday Spotify uses three main functions in BaRT Natural Lan guage Processing NLP which work by analyzing text Audio models which work by analyzing the raw audio tracks themselves and Collaborative Filtering CF which work by analyzing your behavior and others behavior They then further improved the model in April releasing a dynamic model called the Preference Transition Model that not only addresses the filter bubble problem but also enhances user interaction and engagement These recom mendation models are all connected to Spotify s much larger ecosystem which includes giant amounts of data storage and uses lots of Hadoop clusters to scale recommendations and make these engines work on giant matrices endless internet music articles and huge numbers of audio files Problem Statement With above background in mind we set out to propose the problem statement how to develop a music recommen dation system that combines machine learning knowledge learning our research and understanding and some con cepts from Spotify s BaRT algorithm to improve online streaming music users experience This problem statement contains main aspects Music recommendations for existing users learn their preferences through past behavior and recommend songs accordingly Music recommendation for new users and songs i e cold start problem For New Users By clustering songs by their features  Spotify Music Recommendation we plan to select songs closest to cluster centroid for each cluster and recommend them to new users For New Songs By predicting the cluster that new songs belong to we plan to recommend this new song to the users who like this corresponding cluster Data Dataset Description In this project we not only selected a dataset called spotify from Kaggle but also further explored and scraped a dataset called spotify audio feature from the online API provided by Spotify KAGGLE DATASET SPOTIFY The first dataset spotify can be downloaded directly from Kaggle It contains records of tracks in users Spotify playlists based on their sharing with nowplaying tag via twitter The dataset contains around million rows of data and the detailed information on variables is shown below Variable Type Descriptions user id string Unique hash of the user s Spo tify username artist string The name of the artist of the track track string The title of the track which user listened to playlist string The name of the playlist that contains this track Table Kaggle Dataset spotify SCRAPED DATASET SPOTIFY AUDTIO FEATURE Besides tracks in different users playlists we also explored and found other song features such as danceability energy and so on from the scraped dataset spotify audio feature using Spotipy a lightweight Python library for the Spo tify Web API There are two main steps to find information related to songs Firstly we obtain specific track id by searching the combination of artist and track name Then we call the API again to get access to audio features for different tracks with their unique track ids This dataset contains additional features for songs obtained from the first dataset and is composed of songs The description of the selected song features is shown below Variable Type Descriptions track string The title of the track which user listened to danceability float How suitable a track is for dancing energy float A measure of intensity and activity energetic tracks feel fast loud and noisy key integer The estimated overall key of the track loudness float The overall loudness of a track in decibels db mode integer The modality major or mi nor of a track speechiness float The presence of spoken words in a track acousticness float Whether the track is acoustic instrumentalness float Whether a track contains no vocals rap or spoken word tracks are vocal liveness float The presence of an audience in the recording valence float The musical positiveness con veyed by a track tempo float The overall estimated tempo of a track in beats per minute BPM type string The type of the song id string Unique id of the song uri string A uniform resource indicator code for the song track href string The URL for the song analysis url string URL to a low level audio anal ysis duration ms integer The duration of the track in milliseconds time signature integer An estimated overall time sig nature of a track Table Scraped Dataset spotify audio feature  Spotify Music Recommendation EDA To better understand the data we performed exploratory data analysis on both datasets Firstly we would like to understand the relationship be tween users and tracks So we group the tracks and user ID respectively and see the number of the user ID and in another case number of the tracks with the visualization of two histograms Figure The Number of Times A Track is Listened to by Users Figure The Number of the Tracks Each User Listened In Figure we could see the distribution of the favoured tracks is not so skewed which probably means each kind of music is welcomed by a group of audience and there are a few tracks that are really on trend and be in favour by a huge group of users From Figure We can also know that except for some extreme users most of the users music collection numbers are on average Furthermore to understand the distribution more compre hensively we analyzed and drew Figure and Figure to show the track and user distribution by listening count it helps us understand more about the general skewness and the extremeness Besides to understand more about audio features statistics characteristics according to the data type of the features we performed a few descriptive analyses and obtained the visualizations below for some of the audio features from Firgure to Figure Figure Track Distribution by How Many Time it s Listened Figure User Distribution by How Many Track they ve Listened The distribution of the track length fall in a certain range of lengths There is significant skewness for duration and loudness histogram distribution The keys of the tracks are almost evenly distributed Most of the time signature of the anthems are in mode Figure Distribution of Track Loudness  Spotify Music Recommendation Figure Distribution of Track Tempo Figure Distribution of Duration Data Pre processing The original Spotify playlist dataset from Kaggle is more unstructured To make it more convenient and reduce overall costs for further audio feature crawling as well as model study the following preprocessing steps have been applied to the dataset Drop rows which contain null values of any of user id artist track or playlist Drop rows with unrecognized value under each column Drop rows in which their tracks are no longer available in Spotify with the help of Spotify API Based on our findings in EDA only rows are kept which meet two criteria I each user has listened to different songs within the range between and II each song has been listened to by to users In the original dataset there are a large number of users who only listened to a few songs which are not listened by any other users It is impossible to find similar users to do the recommendation So only users and songs with high frequency are selected After cleaning there are rows with unique users and distinct tracks Then we crawled the audio Figure Distribution of Key Figure Distribution of Mode features of each track using Spotify API Since the data is precleaned by Spotify for the use of developers only irrelevant columns are dropped for cleaning Train Test Split To evaluate and compare the performances between different recommendation models train test split is applied to the Spotify playlist dataset For all the users of the songs they listened to are kept as a train set while the other are used as a test set Hence we assume that these songs in the test set are not listened to by those users when training our model By comparing the recommendation to every user and the test set of them we could get the performance of the model Details of evaluation could be found in Section Modelling Evaluation Modelling There are five individual models including Content Based Model User Based Collaborative Filter Model NMF Model SVD Model and Auto encoder Model Except for Content Based Model the other four models work together as a com bined model to get the final recommendation The details of these models will be introduced in the following Sections to All five models could make recommenda  Spotify Music Recommendation tions separately and their performance is evaluated by Hit Ratio and NDCG Except for Content Based Model the other models only used the user item matrix where every row is a user vector and every column is an item vector For easy comparison every model recommends songs to every user Since the recommendations of the combined model are based on scores defined in four different models these scores are normalized with the following equation Scorein Scorei Scoremin Scoremax Scoremin Scorein is the normalized score of the ith song recom mended while Scorei is the original score of the ith song Scoremax and Scoremin are the max score and min score of the songs among the songs recommended to this user After this normalization the scores of songs recom mended to every user by every model will be distributed from to Then we multiply the Hit Ratio of every model to their scores and combine the recommendation list from these four models together to get the final result If a song appears in multiple recommendation lists or in the list of the model having higher Hit Ratio or the song itself has a high score in a model it tends to get a better rank in the final recommendations CONTENT BASED FILTERING Content based filtering is used as a start up model This model works under the assumption that users will like songs that have similar characteristics To start with K means is performed on the dataset spotify audio feature to identify distinct clusters of songs based on various features like danceability tempo duration etc Then for each user in the train set we identified the top clusters that contain the most number of songs they have liked before and selected songs closest to the cluster center as the recommendation candidates These songs are then evaluated against the actual list of songs this user likes from the test set Apart from making recommendations this model could be used to solve cold start which is the most significant prob lem for the following models because they all rely on the interaction between users and songs but this Content Based Model mainly focuses on the features of songs themselves Content Based Model will select the song which is closest to the centroid of every cluster songs in total and recom mend them to all the new users Besides when a new song is released feeding it to Content Based Model to predict which cluster it belongs to Then find out the users liked the songs in this cluster and recommend this new song to these users USER BASED COLLABORATIVE FILTERING User Based Collaborative Filtering is utilized to look for a certain number of users similar to the target user take the songs listened to by these similar users and recommend songs which have never appeared in the targets playlists The initial step is to transpose the user item matrix and it takes user id as columns and song id as rows to better calculate the Pearson Correlation coefficient The value contains only and which represents whether or not the user has listened to the song Based on the Pearson Correlation of users representing similarity between users we find out the top three users who have similar tastes with the target user Then we iterate over each similar user s playlist for each of the songs which does not appear in the target s playlist we assign the Pearson Correlation between this user and target user as score to this song After all iterations we combine the playlist and take the sum of the score if the song appears in playlists from multiple similar users For example if the song appears in the playlist of both similar users A and B then we sum up the Pearson Correlation of both users as the song s score For each target user we list the final similar song with their scores The first ten songs obtained from the descending order of the list are regarded as the recommended songs of User Based Collaborative Filtering AUTO ENCODER MODEL The code of Auto encoder Model is taken from the sample code of this course with minor changes The basic con cept of Auto encoder Model is similar to NMF Model and SVD model All of them are used to reconstruct the sparse user item matrix This matrix is decomposed to latent space with patterns hidden behind users tastes In other words in the user item matrix a user vector is the interaction between this user with all the songs if listened if not listened the dimension of this vector is in the latent space this user vector will be some number representing some hidden tastes with lower dimension In our NMF and SVD Model we set this dimension to be which means we represent the taste of a user with only hidden features These hidden features are extracted automatically from his her interaction with songs For the Auto encoder Model the dimension of latent space is set to be For NMF and SVD Models multiplying the user matrix and item matrix divided by the length of corresponding vector lengths we could get a matrix with cosine similarity between every user and ev ery song because the cosine similarity is calculated by the following equation Cos Simij ui vj ui vj  Spotify Music Recommendation Cos Simij is the cosine similarity between user i and song j ui is the ith user vector while vj is the jth item vector ui and vj are the length of them For the Auto encoder Model the encode and decode process is done in corresponding layers and the scores representing similarities between users and songs are given in the output layer Then the songs with highest scores in every row of the matrix every user are taken as the recommendation of the Auto encoder Model to those users Simple hyperparameter tuning is done for Auto encoder Model the dimension of encode layer decode layer and latent space is tuned and the performance is shown below It can be seen that the performance is the best when the sizes of encode layer latent space and decode layer are and respectively NMF MODEL Non Negative Matrix Factorization also known as NMF the basic concept is introduced in Section Af ter decomposing the user item matrix with NMF from sklearn decomposition library and getting user matrix and item matrix in latent space multiplying user matrix and item matrix and divided by the length of corresponding vector including user vector and item vector the cosine similarity matrix is found Similar to the Auto encoder Model the songs with highest similarity in every row are recommended to the corresponding user SVD MODEL Singular Value Decomposition also known as SVD the basic concept is introduced in Section After de composing the user item matrix with TruncatedSVD from sklearn decomposition library and getting user matrix and item matrix in latent space multiplying user matrix and item matrix and divided by the length of corresponding vector including user vector and item vector the cosine similarity matrix is found Similar to the Auto encoder Model the songs with highest similarity in every row are recommended to the corresponding user Evaluation EVALUATION METRICS In this report we choose Hit Ratio and normalized dis counted cumulative gain NDCG which are commonly used to evaluate the recommendation systems to be our evaluation metrics Hit Ratio measures the percentage of the users for which at least one of the actual songs are included in the recom mendation list with length k among all the users When the Hit Ratio is higher more users would be the hit users and could be recommended properly therefore the model performance would be better Below is the formula for the Hit Ratio Hit Ratio Number of hit users with k recommendations Number of total users Normalized discounted cumulative gain NDCG For Hit Ratio as long as we can predict one of the actual songs for a user the user will be a hit user However it couldn t measure the ranking performances of the recommendations If the right recommendations are located at the top of the recommendation list the performances should be better Therefore we also use NDCG to evaluate our recommenda tion system It can measure not only how relevant the results are but also how good the ordering is The discounted cumu lative gain DCG is the sum of relevance up to a position k in the recommendation list discounted by the rank However the discounted cumulative gain couldn t be used to compare the systems recommending different numbers of items This can be solved by dividing the ideal discounted cumulative gain IDCG which is the discounted cumulative gain for the most ideal ranking to obtain the normalized discounted cumulative gain To obtain the overall NDCG we take the average of NDCG of the users in the test set The formulas are shown as below DCGi k k j Gj log j IDCGi k I k j Gj log j NDCGi k DCGi k IDCGi k overallNDCGi k N N NDCGi k In these formulas i represents the individual user k is the number of the recommendation for each user and j is the position of the recommendation G is the relevance of the recommended song which is binary with or in our case  Spotify Music Recommendation I k means the ideal list of the recommendation and N is the total number of the users For both Hit Ratio and NDCG we choose to recommend songs for each user therefore k is MODEL COMPARISON EXPLANATION The performance of models is demonstrated in the table below Model Hit Ratio NDCG Score Content Based Filtering User Based Filtering NMF SVD Auto encoder Combined Table Model Performance Comparison It can be seen that the performance of the User based CF Model is the best in both Hit Ratio and NDCG The performance of the Auto encoder Model is not good although the latent space dimension is much larger than the other two similar models NMF and SVD to The possible reason is the limited sample volume because the number of parameters in the Neural Network is huge We can also see that the performance of content based filter ing is the lowest among all models This is expected as the model takes in explicit features like danceability and tempo of songs and makes predictions accordingly Latent features that determine whether a user likes a song or not cannot be properly understood and leveraged like in collaborative filtering models The recommendation considering all the models except Content based Model is not as good as the best model User based CF Model but is still better than most of the indi vidual models This is not surprising because the weight of the CF Model is diluted by other models However we just tested them on this one dataset The other models may outperform on another dataset Hence this combined model is still meaningful because it could get a more stable perfor mance than individual models Strategy After implementing and evaluating the recommendation models above we proceed to propose how a music company in the real world like Spotify can adopt and implement an effective music recommendation system We introduce an iterative workflow below Step Collect user behaviour data towards songs and song features Step Perform various machine learning based mod els illustrated above using data collected from step Step Split user traffic into smaller segments ran domly and test the models from step online Note that in this step evaluation metrics should also include click through rate and user stay time etc to accurately measure model s impact Step From online A B testing in step the best performing model s can be selected for a full traffic deployment Last but not least this is an iterative workflow meaning that after the first deployment continuous data collec tion modeling and A B testing should be performed On top of the steps it is also important to enable both historical and incremental data pipeline so that models can always learn from a comprehensive and latest dataset for optimal performance Limitations and Further Studies In this section we will discuss a few limitations in this project and can be explored for further studies Data limitations There are several limitations from the perspective of the dataset itself The initial playlist dataset contains more than million pieces of data Due to the capability as well as the model performance we only take a subset of the original data to make sure that the subset is not sparse However we also add some bias to the subset when we take only a certain range of tracks or users Moreover during our exploration we only take the name of the song into consideration while there exists a situation in which each  Spotify Music Recommendation song may have different versions For instance there is a remastered version of a previous song but the audio features between each other might be the same With this saying it is also possible to take artists into account and regard the same song with different artists as two distinct songs for further exploration Auto encoder hyperparameter tuning Only the dimensions of encode decode layer and latent space are tuned Many other hyperparameters such as the optimizer and activation are not tuned The Auto encoder Model may have better performance when it is tuned with more combinations Conclusion To conclude we started off by introducing music recom mendation s background and importance then set out to propose the problem statement of how to build an effective music recommendation system to improve user experience By using the Spotify datasets on user preference and song features machine learning based recommendation models and combined model are implemented and evaluated using Hit Ratio and NDCG score And used based collaborative filtering is identified as the best performing model Limitations around data and the auto encoder model are also discussed to shed some light onto the next steps We finally conclude the project with a proposed workflow for online music providers like Spotify to adopt and iteratively improve their recommendation The code implementation of this report can be found on this link https github com ningtang bt group References How Does Spotify Know You So Well How does Spotify know you so well n d Retrieved April from https scribe rip p spotifys discover weekly how machine learning finds your new music a ab efe About Spotify Spotify Febru ary Retrieved February from https newsroom spotify com company info Spotify users have spent over billion hours streaming discover weekly playlists since Spo tify July Retrieved February from https newsroom spotify com spotify users have spent over billion hours streaming discover weekly playlists since Larxel November Spotify playlists Kaggle Retrieved February from https www kaggle com andrewmvd spotify playlists Home Spotify for developers Home Spotify for Developers n d Retrieved February from https developer spotify com Ranking Evaluation Metrics for Recom mender Systems Retrieved April from https towardsdatascience com ranking evaluation metrics for recommender systems d a ef BT Applied Machine Learning for Busi ness Analytics Lecture AutoEncoder from https bt msba github io note blogs html
Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Github link https github com saejin BT Final Project Abstract Classification of hematopoietic stem cells HSCs are based on bone marrow biopsy and flow cytometry Both methods have limitations which range from labor intensive approach to high investment and operating cost However given that classification of HSCs is crucial for the diagnosis of the hematological disorders where the number of cases are increasing globally this paper will evaluate the impact of adopting convolutional neural networks transfer learning approach and visual transformers in classifying five classes of HSCs The analysis is conducted on two data sets where one data set is based on microscopic cytological images taken from bone marrow smears from patients with expert annotation of the type of hematological diseases whereas the other is based on the same images but cropped Our CNN and pre trained CNN models achieve a MCC of for non cropped data set This study is a set towards an automated evaluation of HSCs using state of the art image classification algorithms Introduction Background Hematopoiesis refers to the formation of blood cellular component in the bone marrow which occurs during embryonic development and throughout adulthood to produce and replenish the blood system Studying hematopoiesis can help scientists and clinicians to understand better the processes behind blood disorder like leukemia MDS RA etc and also hematopoietic stem cells HSCs can be used as model system for understanding tissue steam cells and their role in ageing and oncogenesis Currently to analyze the HSCs flow cytometry is used for comprehensive single cell analysis method over the traditional method of bone marrow biopsy where the analysis of HSCs is done almost relatively manually by cytotechnologist The flow cytometry will require cells obtained from blood and placed into suspension before staining with dyes This is followed by three processes of using fluidics system to guide cell sample past the laser for separate measurement of every single cell the optics system which will emit light to collect signal and lastly detection of signals to digital parameters to analyze the cells using a separate software Despite the advantages of flow cytometry of enabling scientists and clinicians to evaluate blood disorders there are several limitations Some limitations include i fluidics system usually causes blockades which affect the analysis of HSCs ii manual check on the laser alignment iii damage of cells which affects the analysis of HSCs and iv the exorbitant cost of purchasing a flow cytometry and the high operating cost of the device Furthermore effective examinations are highly dependent on the availability and experience of cytotechnologists This problem is exacerbated by the increase in bone marrow biopsies required due to the increase in the number of hematological disorders globally e g the number of newly diagnosed leukemia cases increased from in to in Dong Project objective One major development in recent years is the application of machine learning techniques to identify abnormal bone marrow cells However these models often rely on cell level features in order to make a prediction These cell level features often require specialized techniques to extract i e flow cytometry While this is an improvement over the traditional approach of bone marrow biopsy it would still require some lab work which is often also the bottlenecks and limitations in order to obtain the features Therefore in this study we aim to improve the analysis of HSCs and overcome the challenges faced by both flow cytometry and bone marrow biopsy by using images of single cell samples to train a neural network that can be used to identify the five classes of HSCs The reduced samples could be flagged to a trained medical professional for further examination We believe that adopting this approach will reduce the diagnosis time for certain bone marrow related diseases drastically which can lead to earlier treatment and overall better prognosis for the patient Furthermore it will also reduce the cost of analyzing HSCs which helps to allow support earlier blood abnormalities detection  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Business applications Although cytopathologists and cytotechnologists play an important role in precision medicine there is a global shortage of these professionals S J Robboy This imposes challenges on the early diagnosis of abnormalities in the bone marrow This study can strengthen the basis for the automatic analysis of the cell morphologies to quantify the initial degree of malignancy on top of the numerous literature papers that have similar findings Mori Teekaraman Furthermore it also improves the diagnostic accuracy as it reduces the reliance on the experience of cytopathologists and shorten the diagnosis time since it reduces the images that the cytotechnologists are required to review In addition the deep neural networks can be further studied to use images taken directly from the microscope from the cell culture flasks which eliminates the need for a dedicated smearing of single cell and dedicated imaging platform Yao Furthermore the deep neural networks developed for the study can be extended to other medical diagnosis like breast cancer pathology images Wang H lung cancer detection Cheng J Z skin cancer classification Rao P etc via deep learning and transfer learning In addition the study may propel more hospitals to invest resources to create bigger and highly accurate classification of biomedical images which will improve the automatic analysis done via deep neural networks in the long run Data Summary of raw data set The cleaned raw data set contains identified expert annotated mainly single celled images from bone marrow smears taken from patients stained using the May Gr nwald Giemsa Pappenheim stain between and Matek The data source where data set is downloaded is from a Kaggle website Bone Marrow Cell Classification The images were annotated into classes including classes that were artificially added i e smudge cell artefact other cell and not identifiable to avoid biasing the annotation for easily classifiable images Each image is by pixels There is no overlap between images implying no correlation between different images in the data set Classes in the raw data set With reference to the Figure the cell types for classes in the raw dataset have highly imbalanced distributions Such class imbalance is common in medical data because of the unequal prevalence of the hematological diseases and the collection of the annotated single cell images from patients Refer to Appendix I for detailed explanation of each class Figure Distribution of classes in the raw dataset Data pre processing AGGREGATION OF SOME CLASSES Given that some of the morphological classes are challenging to distinguish even for trained cytotechnologists there are there is a reduction of the number of classes from to since these classes matches the basic categories of HSCs that define the differentiation pathway in hematopoiesis based on our consultation with a medical doctor Refer to Figure Despite the challenges to distinguish some morphological classes this paper will attempt to use sets of CNN models to do one vs rest classification refer to Section for more details Erythropoiesis refers to the process which generates fully mature erythrocytes and requires the synthesis of vast amounts of hemoglobin along with the ultimate loss of the cell s nucleus and intracellular organelles Erythroblast and Proerythroblast are classified under Erythropoiesis Granulopoiesis refers to the process by which mature granulocytes differentiate within the bone marrow Segmented neutrophil Promyelocyte Blast Band neutrophil Myelocyte Eosinophil Metamyelocyte  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Basophil Faggot cell and Abnormal eosinophil are classified under Granulopoiesis Lymphopoiesis refers to the process by which mature lymphocytes differentiate within the bone marrow Lymphocyte Plasma cell Hairy cell and Immature lymphocyte are classified under Lymphopoiesis Monopoiesis refers to the process by which mature monocytes are generated within the bone marrow Only Monocyte is classified under Monopoiesis Others refer to the additional classes i e smudge cell artefact other cell and not identifiable were artificially added to avoid biasing the images Figure Distribution of classes after aggregation TRAIN TEST SPLIT FOR DATA SET The raw data set will be randomly divided into the training validation data set and testing data set in the ratio of Within the training validation data set the samples would further be divided into training and validation data validation by the ratio of for the purposes of hyperparameter tuning CLASS IMBALANCE With reference to the Figure the cell types for the classes would still have highly imbalanced distributions To address the class imbalance in the training data set down sampling and up sampling will be adopted to achieve images per class The up sampling will be based on image augmentation transformation i e rotation by random continuous angle vertical and horizontal flips shifts up to of the image weight and height and shears by of the image size Whereas the down sampling will select the first nth images up to the sample size required For example since Lymphopoiesis class has types of cells each cell type will have images Lymphocyte and plasma cells have training images more than they will be down sampled whereas the hairy cell and immature lymphocyte will be up sampled Refer to Table for the summary of the training validation and testing datasets Table Summary of training validation and test dataset IMAGE CROPPING Even though the raw data set attempted to focus mainly on single celled images it is observed that there are substantial number of images with peripheral cells Based on our consultation with a medical doctor the classification of the blood cell type focuses on the appearance of a single cell eg the shape of the nucleus the surface appearance of the cytoplasm the colour of the nucleus etc As such there will be an evaluation of the models based on sets of training validation and test dataset one without image cropping and one with image cropping This will enable future research to evaluate if image cropping is useful to improve model performance In order to get a good crop of the cells we first perform edge detection to identify contours of all cells in the image Next all contours less than pixels in length are dropped as these would mainly relate to smudges in the cell staining process or incomplete cells Next we identify the cell of interest in the image by identifying the center most contour and cropping a square around that contour A sample of the cropped images can be found in Appendix II Methodology Models To classify the classes we have adopted models convolutional neural network with the Xception model architecture CNN transfer learning combined with different number of layers for the CNN Pre CNN and Pre CNN and vision transformer All were trained on a batch size of for the different sets C K vR etc  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells CNN MULTICLASS CLASSIFICATION CNNs are at the core of most state of the art computer vision solutions for a wide variety of tasks Szegedy et al For our CNN architecture we chose an Xception model with depth wise separable convolutions in order to limit the number of training parameters as compared with traditional inception architectures In this architecture residual blocks from certain convolutional layers are passed through as separable convolutional layers in the neural network to optimize learning Empirically there is evidence to suggest that such networks are easier to optimize as compared to an inception network of similar depth He Zhang Ren Sun Refer to the Figure for the summary of the model and Appendix III for detailed model plot Figure Summary of model architecture for CNN CNN ONE VS REST CLASSIFICATION During our exploratory model training phase we discovered that the CNN appears to be very good at binary classification problems achieving test accuracy on sample binary classification datasets taken from our main dataset Therefore we also explore if we can get better classification results if we train a CNN to identify each class in total Samples can then be classified by each of the models and classified as the class whose model outputs the highest probability For this model the CNN architecture is the same as that described in except that the output layer contains only neuron with the sigmoid activation function A detailed flowchart of the vR classification process can be found in Appendix IV INCEPTTIONV AND CNN Transfer learning approach is adopted to leverage on existing established pre trained image recognition model for feature extraction from images For this study the InceptionV is adopted as it is a CNN trained on ImageNet dataset which consists of more than million images and is known to attain accuracy around As such we use this pre trained model to extract the features vectors from our training data set of images before using them as inputs to two different CNN models The CNN model architecture is based on a subset of the Xception model architecture The difference between Pre CNN and Pre CNN models are the number of layers and hence level of complexity Refer to Figure and for the summaries of the models Due to computational resource constraints each of the models is only trained on only samples each in the positive and negative class We assemble the positive class by randomly selecting samples from the class that the model is supposed to identify The negative class is assembled by randomly selecting samples from each of the remaining classes for a total of samples in the negative class  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Figure Summary of model architecture for Pre CNN Figure Summary of model architecture for Pre CNN VISION TRANSFORMER Transformers were already well known and utilized in NLP tasks with some examples being BERT DeBERTa However they had limited applications in images until Vision Transformers ViT came along While ViTs are not an entirely new concept they became popular through a revolutionary new way in processing images Earlier ViT methods tried looking at each pixel but the revolutionary approach by Dosovitsky A et al divided each image into patches Although the authors showed the remarkable performance of ViTs surpassing even the long held SOTA CNNs one downside of ViTs were the large amount of data needing to be fed before they would outperform This could range in the scale of at least million images Given that training large number of images was computationally intensive we opted for a pre trained ViT B Morales F shown in Figure Figure Summary of model architecture for pre trained ViT The pre trained model was just fine tuned minimally following Momin s method by adding a BatchNormalization followed by a small dense layer with GELU activation between BatchNormalizations before the final output of Classes GELU which features in transformers such as BERT can be seen as an improvement over ReLU as they characterize input by value instead of the sign in ReLU In addition they have remarkedly better performance likely due to the function being curved at all points allowing for improved approximation Hendrycks Gimpel Results We have adopted i accuracy ii weighted average recall and iii Matthews Correlation Coefficient MCC to evaluate the performance of the models Accuracy is an overall measure of how much the model is correctly predicting the classification of a single image above the entire set of data Refer to Equation Even though accuracy is the most famous classification performance indicator it will be the least important performance metric among the three K Blagex since it is less appropriate when the dataset is imbalanced Equation Accuracy Formula Whereas weighted average recall is the weighed mean of recall with weights equal to the class probability There is a focus on recall refer to Equation because the detrimental consequence of classifying the HSCs which  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells results in delays in receiving necessary medical treatments Equation Recall Formula MCC is adopted as the most important evaluation metric among the three because i it is widely used in biomedical as a performance metric ii it is a more reliable statistical measure which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories true positives false negative true negative and false positive proportional to both the size of the positive element and the size of negative elements in the dataset and iii it can be adopted for imbalanced classification and can even be used for classes that are very different in sizes Chicco D Ietswaart R Refer to Equation Equation MCC Formula Performance evaluation metrics for classes and datasets not cropped vs cropped COMPARISON OF COMPUTATION EFFICIENCY One important fact that cannot be overlooked too would be the computational time which for a larger dataset would scale immensely although it provides improved scores estimates Table shows in seconds the time taken to run the various models on the training validation dataset Unless specified otherwise they were run using Google Colab Pro s Tesla P GPU with GB Ram The testing evaluation took around the same time averaging at approximately mins each Table Comparison Computational Efficiency We can see that each model even at a smaller dataset for training validation takes hours on average Initial trials run on the full dataset were attempted but ultimately abandoned as each epoch would take around hours COMPARISON OF PERFORMANCE Overall all models attained a higher accuracy weighted average recall and MCC for the test data set that is not cropped as compared to cropped images This implies that cropping images result in the loss of critical features for the classification Refer to Table for the summary of results by evaluation metrics Table Summary of results by evaluation metrics Furthermore the CNN and Pre CNN models attained almost similar performance for the non cropped test data prediction Both models outperform the Pre CNN and Pre trained ViT performed the worst Based on our analysis even though both Pre CNN Pre CNN and Pre trained ViT models were all pre trained on ImageNet dataset the Pre trained ViT model lacks the inductive bias typically present in CNNs such as translational symmetry that gave rise to CNNs outperforming FCNN This may prevent the model from fully capturing the critical features necessary to differentiate the individual HSC class as it would require vastly more data as compared to the CNN model all were pre trained on ImageNet dataset In additional even though the Pre CNN has additional layers which results in an additional of trainable parameters as compared to Pre CNN it is observed that the MCC differ by only This implies that InceptionV is relatively effective in extracting the critical features for HSCs classification which corroborates with the use of InceptionV for similar cell classification Mzurikwao D In addition it is observed that the recall for the Monopoiesis class is the lowest for all models based on non cropped images This may be due to the fact that the training dataset for Monopoiesis class has the highest percentage of augmented images This implies that the image augmentation may distorted the necessary key features required to differentiate the HSCs Refer to Table and for results by classes Refer to Appendix V for the confusion matrix Table Summary of results for non cropped images  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Table Summary of results for cropped images Performance evaluation metrics for classes We evaluated the performance of the one versus rest CNN classifier on a test dataset containing samples across all classes The classifier only managed to achieve an accuracy score of In order to understand why this strategy performed poorly we analyze the confusion matrix of the test set classification as shown in Figure below Figure Confusion matrix for the classes From the confusion matrix we observe that some binary classifiers performed extremely poorly with extremely high false positive rates or false negative rates For example classifier obtained a false positive rate of while classifier obtained a false negative rate of Discussions Neural networks have shown to be successful in various image classification problems In this study the results are encouraging with a relatively high MCC of given the small training data set that the models are performed due to the limitation on computational resources We believe that limitations in the ViT model attempted resulted in a low performance as compared to other models Given the relatively nascent stage of ViT although it is becoming more widely adopted the availability of pre trained models was primarily offered in PyTorch and or JAX An adapted keras version was found however it was limited in the implementation to a ViT B pre trained on imagenet k and fine tuned on imagenet k Further improvements could be had by further fine tuning patch size transformer layers projection dimensions and training on larger amounts of data As mentioned in although ViTs are extremely powerful we can see that when applying it on a domain that is remarkedly different from the domain on which it is trained the performance of ViTs for a smaller data size is unable to match up with a pre trained CNN For the one versus rest classifier we believe that part of the problem was the extremely imbalanced dataset Even though each binary classifier models were trained on synthetically balanced datasets the number of original samples for certain cell types such as ABE is so few that most of the augmented samples are very similar This hindered the model learning which could explain why certain binary classifiers had extremely high false negative and false positive rates References n d Retrieved from American Society of Hematology Image Bank https imagebank hematology org about Ali Darakhshandeh M D Faggot cell leukemic promyelocyte Retrieved from https imagebank hematology org image faggot cell leukemic promyelocyte Biron n d Metamyelocyte Glossary Laboratory Radiology sleep and genetic Retrieved from https www biron com en glossary metamyelocyte Blagec Kathrin Dorffner Georg Moradi Milad Samwald Matthias A critical analysis of metrics used for measuring progress in artificial intelligence Bone Marrow Cell Classification Retrieved from https www kaggle com andrewmvd bone marrow cell classification select abbreviations csv Cheng J Z N D H M C Computer aided diagnosis with deep learning architecture applications to breast lesions in US images and pulmonary nodules in CT scans Chicco D J G The advantages of the Matthews correlation coefficient MCC over F score and accuracy in binary classification evaluation BMC Genomics Choi JW K Y White blood cell differential count of maturation stages in bone marrow smear using dual stage convolutional neural networks Retrieved from https doi org journal pone Christian Matek S K Retrieved from https ashpublications org blood article Highly accurate differentiation of bone marrow  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells CORPath n d Blasts Retrieved from https www corpath net blasts Dong Y S Leukemia incidence trends at the global regional and national level between and Retrieved from https doi org s Dosovitskiy A Beyer L Kolesnikov A Weissenborn D Zhai X Unterthiner T Dehghani M Minderer M Heigold G Gelly S Uszkoreit J Houlsby N June An image is worth x words Transformers for image recognition at scale arXiv org Retrieved April from https arxiv org abs eClinpath n d Normal leukocytes Retrieved from https eclinpath com hematology morphologic features white blood cells normal leukocytes Encyclop dia Britannica Inc n d Erythroblast Retrieved from https www britannica com science erythroblast Encyclop dia Britannica inc n d Erythroblast Encyclop dia Britannica Retrieved from https www britannica com science erythroblast Hendrycks D Gimpel K July Gaussian error linear units GELUs arXiv Retrieved April from https arxiv org abs Ietswaart R A S Machine learning guided association of adverse drug reactions with in vitro target based pharmacology Retrieved from EBioMedicine https pubmed ncbi nlm nih gov KG K Book Review Deep Learning Krizhevsky A S I Imagenet classification with deep convolutional neural network Advances in neural information processing systems Matek C K An Expert Annotated Dataset of Bone Marrow Cytology in Hematologic Malignancies Retrieved from The Cancer Imaging Archive https doi org s Momin R February Vision Transformer VIT fine tuning Kaggle Retrieved April from https www kaggle com code raufmomin vision transformer vit fine tuning Morales F July VIT Keras Keras implementation of VIT Vision Transformer GitHub Retrieved April from https github com faustomorales vit keras Mzurikwao D Khan M U Samuel O W et al Towards image based cancer cell lines authentication using deep neural networks Sci Rep https doi org s Salama K January Image classification with Vision Transformer Implementing the Vision Transformer ViT model for image classification Retrieved April from https keras io examples vision image classification w ith vision transformer Szegedy C Vanhoucke V Ioffe S Shlens J Wojna Z Rethinking the inception architecture for computer vision In Proceedings of the IEEE conference on computer vision and pattern recognition pp  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix I Explanation of each type  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix II Example of non cropped against cropped images for classes  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix III Convolutional Neural Network Architecture  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix IV One Versus Rest Classification Strategy binary classifiers are trained to identify one specific cell type out of the cell types The basic model architecture is the same as the multiclass convolutional neural network with Xception architecture Each training dataset contains samples from one particular cell type as the positive class and samples from each of the remaining cell types as the negative class Each validation dataset contains samples from one particular cell type as the positive class and samples from each of the remaining cell types as the negative class The training and validation result of each classifier is shown in the table below The testing dataset contains samples across all classes To perform a sample classification the image is fed to each of the classifiers which would output a probability that the sample belongs to the class that the classifier is trained to identify The final classification would be the class of the classifier that outputs the highest probability  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix IV One Versus Rest Classification Strategy cont d Classifier Cell Type to Identify Val loss Val Acc ABE ART BAS BLA EBO EOS FGC HAC KSC LYI LYT MMZ MON MYB NGB NGS NIF OTH PEB PLM PMO  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix V Confusion matrix a Without image cropping CNN Pre CNN Pre CNN Pre trained ViT  Application of Deep Learning Neural Networks in the Identification of Abnormal Bone Marrow Cells Appendix V Confusion matrix cont d b With image cropping CNN Pre CNN Pre CNN
Exploring and Deciphering The Perfect Tweet Abstract For our project we analyzed the tweets from the top accounts to explore and decipher what makes a good tweet that can generate a high engagement rate We first research the top accounts to extract the tweets from following which several different levels of features are extracted which is vital to explain the tweets finally modelling the data with the respective engagement rates using different machine learning techniques to obtain the best accurate model Using the most accurate model the feature importance is derived to determine which feature is more important in predicting the engagement rate thereby deciphering which feature is key when developing the content for a good tweet Finally we test the resulting model on tweets generated from an online tweet generator to predict the estimated engagement rate and see how the online tweet generator performs This online tweet generator is intended to generate tweets based on historical data of past tweets of a specific account Therefore we will select a specific account and generate tweets based on learning from past tweets created by the said account owner Our comparison will then show the effectiveness of the online tweet generator as well as our model s efficiencies in identifying the areas the tweets can improve on to have a better engagement result Introduction Background As consumers increasingly spend their attention on mobile devices social media is a powerful tool for businesses especially those with a global reach Motivation In recent weeks Twitter has gained a lot of public attention since Elon Musk became the largest shareholder in the company with a stake and is on a quest to acquire the entire company with a bid of US billion as of the time of writing Given Elon Musk s entrepreneurial track record Twitter may become an even more powerful platform if his bid is successful As such businesses who understand how to engage Twitter users will be well positioned to benefit from the growth Twitter started off in as a social networking platform where users can post character posts called Tweets that can be seen by other users The character limits forced users to be more creative in their Tweets which arguably made Twitter more interesting In Twitter increased the character limit to The key engagement metrics for Tweets are likes retweets and replies Objectives In this paper we train our machine learning models on Tweets posted by the top Twitter accounts to find out what are the key factors that affect the engagement rates of a Tweet and how do we generate the best possible Tweet Data Extraction We first extract the top most followed Twitter accounts as listed by Viral Pitch https viralpitch co topinfluencers twitter top twitter influencers an influencer marketing research platform For each of these accounts we extract all Tweets posted between December to March along with the number of likes retweets and replies of each Tweet With this we have a total of Tweets Data Feature Extraction The feature extraction from the contents of the tweets can be split into sections Each section involves extracting a specific type of feature that will be use later and tested during the modelling phase The sections are Account Level The features represent the account in which the tweets are generated from which mainly represent the properties of the account owner Raw Features these are the raw features that are typically extracted from messages and content explaining certain properties of a text Tweet Type Category Since each tweet is from a specific account that handles a specific type of genre these genre categories are processed to create features Topics Category Even with the tweet genres the tweets themselves may contain certain common topics amongst all the tweets TF ID An additional feature extractor to vectorize the frequency of words across the text  BT Team Project Group AY Semester Account Level The account level features are aimed to account for impact on the engagement rate of tweets due to the account itself not the content tweet itself The accuracy of the estimated feature importance of the other explainable features of the tweet content might be more accurate with the inclusion of these features In addition the modelling can confirm if the account level features itself has a high impact on the various engagement rate The account level features are number of followers and average number of tweets created per day Raw Features The raw features extracted from the tweets are typical properties of text based data that help illustrate the text Table The raw features and their descriptions RAW FEATURES DESCRIPTION NUMBER OF WORDS COUNT OF WORDS IN TWEET VERBS COUNT USING SPACY PACKAGE NOUNS PUNCTUATIONS COUNT USING NLTK PACKAGE NUMBERS COUNT OF EXCLUSIVE NUMBERS MISSPELLED WORDS COUNT VALIDATED BY DJANGO SENTIMENT SCORE SCORING USING NLTK SENTIMENTINTENSITYANALYZER Since the objective is to quantify what makes a good tweet these properties can serve as indicators of what tweeters should look out for when crafting their tweets depending on the results from the modelling phase and the feature importance of these properties Tweet Type Category Tweets posted by top users are usually content from a certain category as defined by the account owner These categories can be defining factors in which certain categories of content are more favored than others Therefore these categories are converted into variables using one hot encoding Table List of categories from generated tweets NEWS POLITICS FAMILY FINANCE EDUCATION ENTERTAINMENT FOOD SPORTS FITNESS HEALTH TECHNOLOGY LIFESTYLE FASHION GAMING Topics Category Aside from the categories in which the account owner usually tweets about there could be other topics currently during the period of posting that could be trending such that tweeting content about the trending topics might help to boost the tweets engagement Thus identifying these topics can quantify the trending topic variables To identify the topics Latent Dirichlet Allocation topic modelling was used This topic modelling is used to classify text in a post article to fit a certain topic where it builds a topic per document such that each word s presence in the document is attributable to a small number of topics in the document Therefore using the genism package to model an estimate the top three topics the topic modelling was done through iterations st Iteration Figure Word Importance from the top topics from the st iteration run of LDA modelling From the first iteration excluding the topics that contain main stop words it is observed that these topic words carry the most weights Table The top topics from the st iteration ONEFAMILY IND LIVE SAVIND nd Iteration Figure Word Importance from the top topics from the nd iteration run of LDA modelling For the second iteration the tweets that contain the topic words from the st iteration is removed before extracting the topics Table The top topics from the nd iteration SOLD INVASION TATIPLAUCTION UNSOLD NCT Therefore from the two iterations of topic modelling the final topics that has the highest weights are Table Overall top topics generated from LDA modelling ONEFAMILY LIVE SAVIND SOLD INVASION TATIPLAUCTION UNSOLD NCT  BT Team Project Group AY Semester One hot encoding is done to specify if a tweet contains a topic or not creating feature variables describing the topics found in the data TF ID Term frequency inverse document frequency is a very common text vectorizer technique that transforms the text data into vector that is usable in modelling Using this technique additional features in the form of vectors of each tweet according to the term frequency and document frequency are created and will be later using in the modelling process as additional features to see if it improves the accuracy of the model to obtain a better estimate of the feature improvement coefficient Data Exploration Multi collinearity check Figure Correlation plot of all features With features aside from the TF ID features it is crucial to check for multi collinearity There are some effects between word count and different types of count which is expected and ignored since each explains specifics of a tweet There is a high collinearity between AverageTweets and category tweets News Politics Health and Sports Fitness This could suggest that users from such groups usually have high average tweets per day Average Tweets vs Category News Politics Health and Sports Fitness News Politics Health Sports Fitness Figure Boxplot of AverageTweets with categories as specified on the left From the box plot of average tweets against various categories it is observed and confirmed that most of the tweets are from News Politics and are mainly not about Health and Sports Fitness This supports the earlier collinearity effect as it stems from the number of tweets and the category the tweets are from However this variable may still have certain effect in determining a tweets engagement rate Hence these variables are kept and will be used in the modelling phase Figure Boxplot of AverageTweets with categories From the scatter plot of likes against AverageTweets colored by News Politics tweets it is observed that most of the tweets about News Politics have mostly lower likes and the average number of tweets per day by these accounts are quite spread out This shows that keeping the category features is appropriate since there is not clear trend when compared with the number likes The modelling phase may uncover other information  BT Team Project Group AY Semester Category Topics Exploration Category Total Tweets Average Likes Per Tweet Figure Total number of tweets and average likes per tweet from each category From the graphs above it is observed that althrough most of the tweets are mostly from a few categories this is not proporational to the average engagement rate of these tweets as shown by the nd graph depicting that tweets about family typically have higher average likes One reason could due that due to the higher number of tweets for certain categories the spread of engagement rate is large hence resulting in an overall lower average as compared to other categories with lower number of tweet but with a higher proportion of those tweets having higher number of likes Figure Boxplot of likes for all tweets in each category As evident from the boxplot of likes for each category it supports the earlier reasoning that due to the higher proportion of low likes for tweets in categories like News Politics the overall average likes is lower as compared to other categories with much lesser number of tweets Topics Total Tweets Average Likes Per Tweet Figure Boxplot of likes for all tweets in each category Even through most of the tweets are about LIVE topics which essentially could about News the topic NCT had the overall highest average likes amongst all the topics This is largely attributed to the fact that the topic NCT is related to a KPOP boy band which has a very high fanbase hence contributing to the higher average likes per tweet as compared to other topics Removing NCT the averages likes is comparable across topics except for invasion which due to the nature of the topic could explain the lower average likes Figure Boxplot of likes for all tweets for each topic The box plot clearly shows that the topic NCT has an overall higher average likes as compared to other topics such LIVE which is clearly observed to have a spread of much lower likes in each tweet Hence posting more tweets per day does not necessarily mean a better engagement rate rather posting tweets about the trendy topics and category would be more effective which will be tested in the modelling phase  BT Team Project Group AY Semester Modelling Key Concepts Polynomial Regression Polynomial regression is a form of regression analysis to analyze the relationship which is modelled as an nth degree polynomial Random Forest Random Forest is an ensemble learning method by constructing a multitude of decision trees with different samples for the output variable It takes the majority vote for classification tasks and average value for regression tasks XGBoost XGBoost is an implementation of gradient boosted decision trees designed for speed and performance LightGBM Light gradient boosting machine is a gradient boosting framework that is based on decision tree algorithms Mean Absolute Error Mean absolute error MAE is a measure of errors between true observation and predicted observation Hyper Parameter Tuning Modelling All the four models introduced are used for modelling for the prediction of replies retweets and likes with hyper parameter tuning performed Mean absolute error MAE is the model performance indicator being used for the hyper parameter tuning and model selections in this project Polynomial Regression Due to the limited computing power resources the modelling with polynomial regression is performed up to rd degree only However the performance of the st nd and rd degree polynomial regression are not differentiable based on MAE evaluated Hence the st degree of Polynomial Regression linear regression is considered as the modelling baseline in this project given the same performance level with higher degree regression but less resources needed Table Training MAE for Polynomial Regression Models DEG REPLIES MAE RETWEETS MAE LIKES MAE SELECT ST ND x RD x Random Forest Grid search is implemented for hyper parameter finetuning which formed sets of hyper parameters for random forest modelling In addition cross validation is used to refine the modelling outcome and there are fits in total for the modelling N estimator Max depth of decision tree Cross validation folds Table The best model s hyper parameters for random forest TARGET VARIABLE N ESTIMATOR MAX DEPTH REPLIES RETWEETS LIKES XGBoost Both grid search and cross validation are used for XGBoost modelling to fine out the optimal hyper parameters for the optimal performance There are fits in total for the modelling N estimator Max depth of decision tree Cross validation folds Table The best model s hyper parameters for XGBoost TARGET VARIABLE N ESTIMATOR MAX DEPTH REPLIES RETWEETS LIKES LightGBM In total there are fits for LightGBM modelling with grid search and cross validation implemented N estimator range Max depth of decision tree range Column sample by tree Cross validation folds Table The best performer s hyper parameters for LightGBM TARGET VARIABLE N ESTIMATOR MAX DEPTH COLUMN SAMPLE BY TREE REPLIES RETWEETS LIKES Modelling Result for Replies Retweets Likes With the hyper parameter tuning for each model the best performer for each model is obtained for replies retweets and likes respectively Hence the models can be compared with each other with its own best MAE performance Based on the train MAE and validation MAE XGBoost is selected to be the final model to be used for tweets replies given the best performance among all the models  BT Team Project Group AY Semester Table Modelling Result for Replies MODELS TRAIN MAE VALIDATION MAE SELECT POLY REGRESSION BASELINE x RANDOM FOREST x XGBOOST LIGHTGBM x For tweets retweets it is also XGBoost having the best performance compared with the rest models Table Modelling Result for Retweets MODELS TRAIN MAE VALIDATION MAE SELECT POLY REGRESSION BASELINE x RANDOM FOREST x XGBOOST LIGHTGBM x Like tweets replies and retweets the comparison for tweets likes is conducted as well and XGBoost outperforms other models Table Modelling Result for Likes MODELS TRAIN MAE VALIDATION MAE SELECT POLY REGRESSION BASELINE x RANDOM FOREST x XGBOOST LIGHTGBM x Modelling Result for Replies Retweets Likes With the final model selected for replies retweets and likes the prediction on the test dataset is generated Table Test MAE with the best model selected TARGET VARIABLE TEST MAE REPLIES RETWEETS LIKES Explanation Global Explanation We use different methods to show the global explanation of our models The first one is built in feature importance plot In XGBoost the default method to calculate feature importance is using the average gain across all splits where feature was used The second method is Permutation Feature Importance PFI In this method we will shuffle one of the feature values at a time to get some noise from the original dataset and use trained model to make predictions on this shuffle dataset The performance deterioration measures the importance of the variable we have just shuffled The third method is SHAP Shapley Additive exPlanations values Lundberg Lee This method has a solid math theory and can be applied to both global and local interpretation It can ensure fairness and accuracy in explanation Under normal conditions compared with the latter methods the results of built in feature importance are less convincing which can be reflected in the later results Figure Feature importance plots of Replies  BT Team Project Group AY Semester According to the built in feature importance plot the most important features are mainly TF ID features which seems to be over fitting However in PFI and SHAP results most important features are some features such as number of followers and daily average number of tweets which seems to be more convincing A total of features appears in the results at the same time https category News Politics and Ukraine We have successfully found the reasons why these features are very important On one hand if we add hyperlinks hashtags or pictures when tweeting we will get a URL that starts with https Therefore enriching our tweets rather than just a few sentences will make our tweets more attractive On the other hand because we collected the data up to March and the war between Russia and Ukraine broke out on February which directly affected the global political and economic patterns the category of news and politics and Ukraine became the focus of global Internet users Consequently we need to grasp the current hot news if we want to send a popular tweet Figure Feature importance plots of Retweets As for retweets the important features identified by methods simultaneously are number of followers the daily average number of tweets Ukraine category Health and category entertainment as it s easy to understand that the first features are closely related to the number of retweets but it s hard to say how and to what extend they will influence the prediction thus we will use local interpretability to provide targeted interpretation concerning the remaining categories or topics we think that when users want to collect or share some content with their friends such as some methods to keep healthy or some interesting games they will retweet therefore when one wants to increase the number of retweets it may be a good idea to share some practical content Figure Feature importance plots of Likes For likes the common important features are category Health daily average number of tweets and number of followers We think the reason why these variables are important have been introduced in the retweets section Local Explanation  BT Team Project Group AY Semester We will use different local explanation methods SHAP and LIME Local Interpretable Model agnostic Explanations In LIME we will create some new data points around an instance we are interested and use the trained model to make predictions Then we fit a simple explainable model to the perturbed data and use the feature weights to explain the local behavior Figure Screenshot of a tweet from Manchester United We randomly sampled from ManUtd to show some local interpretability Figure SHAP force plot SHAP decision plot and LIME plot of Replies https twitter com ManUtd status According to SHAP result features like daily average number of tweets category Health number of followers have a positive impact on the prediction and this tweet is not a news category which reduces the prediction value From the result of LIME many TF IDF features have large negative effects on the prediction Therefore if user wants to increase the number of replies he may need to use some words like make say and police Another interesting thing shown by LIME is that there always be some important TF IDF features like We finally find that they are emoji However unfortunately we don t have enough time to figure out which specific emoji each symbol represents since they are so similar Anyway in our sample tweet LIME suggests that we use one of the emoji Figure SHAP force plot SHAP decision plot and LIME plot of Retweets Combining the results we believe that users should not tweet too many every day LIME advises that it s better not to exceed a day They also both agree that users should tweet some health related content if possible to increase the number of retweets A major contradiction between methods about the retweets arises from followers SHAP believes that it reduces the prediction while LIME thinks it increase the final prediction However we don t think this is what we can change subjectively in the short term so we can pay more attention on suggestions on other features  BT Team Project Group AY Semester Figure SHAP force decision plot and LIME plot of Likes SHAP suggests tweeting something related to technology while LIME recommends some words to use It s also interesting that LIME believes that content related to fashion will receive less likes GPT Simple Evaluation We trained a GPT Simple model on all of Elon Musk s original Tweets excluding retweets since he registered his account in June with a total of Tweets We then generated Tweets based on the model with the temperature parameter set to Finally we run our models on these generated Tweets to get the best and worst Tweets based on the predicted engagement levels which are shown in the tables below Table Best tweets with the best predicted results TWEETS REPLIES RETWEETS LIKES These guys want us to die so bad they can taste it mins of static fire Important news in a few hours Good Starship news Stop gendering memes I mean mimes Table Worst tweets with the worst predicted results TWEETS REPLIES RETWEETS LIKES After Formula cars will have pressure deflection glare pucker bend and bulge lights traction ampl traction GPS connected lane changing via touchscreen Tesla LA storybook will be on par with a Tesla Model S No I I I don t dream I would break this Sat Sun Moon Earth Hy perloop completed We took this to heart by Polytopia Stop gendering memes Based on what we see above there are a few interesting observations Firstly the top Tweets seemed easier to read with very simple words that were well spaced apart The top Tweets appear to sound confident positive and cheeky Very interestingly Stop gendering memes I mean mimes is among the top Tweets while Stop gendering memes is among the worst These two Tweets have very similar contents but had very different results The former is cheeky while the latter seems more aggressive We noted that this is just a small sample but it does seem to provide us with some insights on what kind of tone and content would generate more engagement We can extend this analysis across the other Twitter accounts to gain deeper insights Limitations The raw features such as word count do not exactly help to quantify exactly how much words to stick to although from previous studies the guideline would be from words Another key limitation from an account perspective is that the number of followers is a key feature hence an account with little to no followers will not generate high engagement rate from its tweets In addition due to the limitation of computing power resources we were only able to test a few parameters for certain modelling technique Firstly we did not try GridsearchCV with more hyperparameters and wider range of each hyperparameter on models such as Random  BT Team Project Group AY Semester Forest and Xgboost which runs relatively slow Hence our final model s parameters may not be the best Secondly we could try some neural network algorithms if we had better computing power In the local explanation section we find many strange symbols that should be emojis which can be difficult to identify what exactly each emoji represents Users will also need to repeat the whole process of training the models on the latest Tweets periodically to ensure that the model is always up to date with the latest trends in Twitter Language patterns hot topics and other factors can change from time to time and hence we would recommend repeating the process every to weeks for better results Future Works Although the category is considered popular now it might not be couple of months of years later Hence another future work may involve quantifying how long a topic or category usually stays trendy what are the factors that determine it In addition instead of using word count directly perhaps a deeper study into the length words together with other features such as the topics category or the number of misspelled words as interaction variables would help determine the best length of words to use in different cases In addition a different word model can be created to convert emoji to probable phrases and words to better quantify such symbols A more advanced NLP model can be built to generate high performing Tweets directly from our model results For example we might be able to feed parameters such as topic word count range tonality etc along with the fixed parameters such as the category of the account or whether the account is a person or a brand etc Ideally the NLP model will be able to automatically generate a list of possible good Tweets for the user to select or perhaps inspire the user to write even better Tweets Conclusion In this paper we explored a systematic approach to help user generate the best possible Tweet The key steps to our approach can be found in Table Table Key steps to generate the best possible tweet STEP DESCRIPTION Extract recent Tweets from the top Twitter accounts Perform feature engineering on the extracted Tweets Train machine learning models to predict engagement levels likes retweets and replies Analyze model results to see what affects engagement levels Generate Tweet based on model predictions Choose the best Tweet based on model predictions Post Tweet Repeat Periodically Users will have to repeat the process periodically to ensure that the model is always up to date with the latest trends in Twitter Language patterns hot topics and other factors can change from time to time and hence we would recommend repeating the process every to weeks for better results Based on the current trends it is best for Tweets to mention hot topics such as Ukrainian war or to be about health or technology to contain picture to contain emojis to contain hashtags to be easily readable to be positive and non aggressive Users should also not Tweet more than times a day GitHub Code Link https github com BHLooi BT Group  BT Team Project Group AY Semester References Bird S Loper E NLTK Proceedings of the ACL on Interactive Poster and Demonstration Sessions https doi org Setiabudi R Iswari N M Rusli A Enhancing text classification performance by preprocessing misspelled words in Indonesian language TELKOMNIKA Telecommunication Computing Electronics and Control https doi org telkomnika v i Elbagir S Yang J Sentiment analysis on Twitter with Python s Natural Language Toolkit and vader sentiment analyzer IAENG Transactions on Engineering Sciences https doi org Canini K Shi L Griffiths T April Online inference of topics with Latent Dirichlet allocation PMLR Retrieved April from http proceedings mlr press v canini a html Mansfield E R Helms B P Detecting multicollinearity The American Statistician https doi org Fuchikami J November K popping An introduction to Korean popular music albums eVols at University of Hawaii at Manoa Home Retrieved April from https evols library manoa hawaii edu handle Lundberg S M and Lee S I A unified approach to interpreting model predictions Advances in neural information processing systems Wikimedia Foundation March Random Forest Wikipedia Retrieved April from https en wikipedia org wiki Random forest Wikimedia Foundation April XGBoost Wikipedia Retrieved April from https en wikipedia org wiki XGBoost Wikimedia Foundation February Lightgbm Wikipedia Retrieved April from https en wikipedia org wiki LightGBM Wikimedia Foundation November Mean absolute error Wikipedia Retrieved April from https en wikipedia org wiki Mean absolute error Wikimedia Foundation July Polynomial regression Wikipedia Retrieved April from https en wikipedia org wiki Polynomial regression Woolf M January How to build a Twitter text generating AI bot with GPT Max Woolf s Blog Retrieved April from https minimaxir com twitter gpt bot
Music Recommendation Filter Based on Emotions Extracted from Facial Expression and Speech Audio Group HUANG HAI HSIN TSAO KAI TING YINGGE XU KANG YIFAN MA KE GitHub Link github com BT Group final project Introduction Music is universally recognized as an effective way for humans to express emotion and regulate emotional states Studies show that people can evoke distinct emotions from listening to music Therefore besides daily refreshment music is also widely used in professional areas such as music therapy and music education Most of the existing music recommendation engines are based on collaborative preference or music content without considering human emotions However we think that what people need is not only music but music fitted for the current emotion We aim to create an emotion based music recommendation model layer to better empower the existing system which could bring lots of value Use Case The use cases could be summarized into two types the passive use case and the active use case Passive Use Case These are cases where we collect the users emotional data for analysis mostly in public places where privacy is not a big concern Some of the passive cases digest the information which in turn protects users bringing needed help in their work For example this layer can be embedded in the safe driving system to detect changes in the driver s expressions and voices in real time to avoid fatigue driving or conflicts Another example is that some cinemas automatically monitor and collect user expressions to get feedback on user experience Active Use Case In most causal use cases there is a huge difficulty in collecting emotional data since most of the users do not want to share the data Therefore asking for privacy permission is a must Users may be willing to share this data when looking for companions For example considering the old or the sick who need more delicate emotional care and people who are eager to have more interactions when quarantined during the Covid pandemic Business Value From the users perspective it provides a sense of compassion and navigates listeners to a more positive emotional state improving the user experience and gaining user satisfaction From the business side high satisfaction means higher customer stickiness as well as easier acquisition This emotion based layer could be embedded in various products no matter software or hardware For example we already see many AI assistants like Amazon Alexa Google assistant Xiaodu Xiaoai etc One can interact with it in apps also smart speakers and home automation During this implementation process with the model trained and the data gathered the business side could learn more about the user s behavior and psychology Finally the techniques the products both software and hardware and various use cases together help the business to form its smart industrial ecology Different from the previous ecology this integration is more emotion based generating a more user friendly environment Facial Expression Recognition Data Description Our facial expression dataset is Facial Expression Recognition Challenge Dataset from Kaggle This dataset has a uniform size for all images x pixels Pixel values of an image are extracted and stored in a csv file Due to resources and time constraints we will use the pre processed csv file to conduct a descriptive analysis Each emotion is imbalanced in this dataset as we show in Figure There is not enough disgust emotion data so we decide only to use the other emotions Facial Expression Recognition Challenge Dataset from Kaggle https www kaggle com debanga facial expression recognition chall enge  including anger fear happiness sadness surprise and neutral in our analysis Figure Number of Images by Emotion Table Facial Data Description Feature Type Description Emotion Integer Six categories anger fear happiness sadness surprise neutral Usage String for training for validation for test Pixels String Each image has x pixels Data Augmentation To enhance the accuracy of image classification we use the ImageDataGenerator method to augment data including rotation shifting width shifting height shearing zooming flipping etc Modeling This is a classification problem and the most promising machine learning tool for image recognition is Convolutional Neural Networks CNNs Figure is the sketch of image recognition problems using a CNN consisting of image inputs convolutional layers fully connected layers and output predictions Figure Sketch of Convolutional Neural Networks CNNs Normal CNN generally has two or three layers but deep CNN will have multiple hidden layers usually more than which are used to extract more features Using Convolutional Neural Networks for Image Recognition By Samer Hijazi Rishi Kumar and Chris Rowen IP Group Cadence https ip cadence com uploads cnn wp pdf and increase the accuracy of the prediction There are two kinds of deep CNN one is increasing the number of hidden layers or increasing the number of nodes in the hidden layer In this case we use the CNN model Table as a baseline model and further explore the deep CNN structure Table Tabel Hyperparameter Selection of CNN model Hyperparameter Structure Description Structure convolutional layers batch normalization layers max pooling layers dropout layers flatten layer dense layers Kernel Size Pool Size Activation Function relu Dropout Rate for first dropout layer for second dropout layer Batch Size Table Hyperparameter Selection of DCNN model Hyperparameter Structure Description Structure convolutional layers batch normalization layers max pooling layers dropout layers flatten layer dense layers Kernel Size for first two convolutional layers for other convolution layers Pool Size Activation Function elu Dropout Rate Batch Size To avoid the model overfit the training dataset and have poor performance on the test dataset we set   early stopping When the performance on a validation dataset starts to degrade the model stops training at that point Also we set the ReduceLROnPlateau When the performance on the validation dataset has stopped improving the model would reduce the learning rate We summarize the performance of the CNN and deep CNN DCNN model in Table DCNN model gives us a higher classification accuracy both in the training set and the validation set Table CNN DCNN Performance Model Training Accuracy Validation Accuracy CNN DCNN Therefore we select the DCNN model as the final one for the application The training process is shown in Figure and Figure After epochs of training the performance becomes stable The model stops at epochs of training and restores model weights from the end of the best epoch Figure DCNN Loss of Each Epoch Figure DCNN Accuracy of Each Epoch Model Result Table DCNN model result Emotions Precision Recall F score anger fear happiness sadness surprise neutral The final accuracy score of the facial expression recognition model is and the final loss of this model is We used this model to further predict the probability for each emotion giving the final emotion category along with the probability level The probability level was later plugged into the recommendation system as input Audio Emotion Recognition Data Description For the audio emotion dataset we use the RAVDESS Emotional speech audio dataset from Kaggle This dataset contains speech audio files collected from actors males and females Each actor will record audio trials with two lexically matched statements in a neutral North American accent The audio files have already been labeled with some features as below Modality full AV video only audio only All files in this dataset are audio only files Vocal channel speech song All files in this dataset are speeches Emotion neutral calm happy sad angry fear disgust surprised We will take the universal six emotions including angry fear happy sad surprise and neutral Emotional intensity normal strong Note that there is no strong intensity for the neutral emotion RAVDESS Emotional Speech Audio Dataset from Kaggle https www kaggle com uwrfkaggler ravdess emotional speech audi o   Repetition st repetition nd repetition Actor to Gender male and female There are files in each of the emotions in the dataset except the neutral emotion containing only files as we show in Figure Figure Audio Dataset Description Data Augmentation After the previous steps we keep files which is not enough for us to perform a deep learning analysis Therefore we create new synthetic data samples by adding small perturbations to our initial training set In this case we try to add random noise slow down the speech file by a rate of change the pitch and shift the time After these steps we get files in total Besides the augmentation also makes our model invariant to those perturbations and enhances its ability to generalize Audio Features Extraction Audio features such as pitch intensity spectral energy distribution average zero crossing density ZCD jitter and MFCC have been discovered to be useful in emotion recognition However using only one audio feature is inefficient to train emotion recognition models The trend of audio features extraction is to combine several complementary audio features Despite feature selection and engineering the model accuracy rate is also found to be correlated to the number of emotions to be classified Many models with higher accuracy rates classify only emotion classes indicating a tradeoff between model granularity and accuracy rate A new approach of audio emotion recognition By Chien Shing Ooi Kah Phooi Seng Li Minn Ang Li Wern Chew A new approach of audio emotion recognition ScienceDirect Table Audio Feature Description Audio Feature Description Formulation Zero crossing rate ZCR The weighted average of the number of times the speech signal changes sign within a particular time window Chroma Stft Performs short time fourier transform of an audio input and maps each STFT bin to chroma Mel Frequency Cepstral Coefficients MFCC Cepstral coefficients derived from a mel scale frequency filter ban Root mean square value Measures the average loudness of an audio track within a time window MelSpectogram A spectrogram where the frequencies are converted to the mel scale Mean values for pitch features are shown in Figure and averaged pitch signals for each emotion on the Hamming window are shown in Figure The below graphs display great discrimination between the six emotions Figure Mean Values for Pitch Figure Hamming Window   Modeling We first take some simple baseline models like the regression models SVM etc However these models give a bad performance whose accuracy is slightly higher than Considering the complexity of the audio dataset we finally decided to train a Convolutional Neural Network model We used the GridSearch method to choose the best hyperparameter and the network structure could be summarized below Table Hyperparameter Selection Hyperparameter Structure Description Structure Four convolutional layers four max pooling layers two dropout layers one flatten layer two dense layers Kernel Size Pool Size Strides for convolutional layer for pooling layer Activation Function tanh Dropout Rate Batch Size After epochs of training the loss and the accuracy both become stable The training process is summarized in Figure and Figure Figure Loss of Each Epoch Figure Accuracy of Each Epoch Model Result Table Model Result Emotions Precision Recall F score angry fear happy neutral sad surprise The final accuracy score of the audio emotion recognition model is and we used this model to further predict the probability for each emotion The probability level was later plugged into the recommendation system as input Recommendation System Building Matching Strategy Valence Arousal Space The goal of emotion based music recommendation is to guide the user to a more positive emotional state Figure Valence Arousal of Emotions  However Mou s research suggests recommending a piece of music that has a similar Valence Arousal value to the user s self reported emotional state as the first piece of music that the user listens to This expression of similar emotion can give the user a feeling of compassion Especially when a user is experiencing a depressed mood the attempt to positively influence the user by a piece of music showing compassion may be more effective than directly playing an exciting or joyful one In the common approach to simplify the recommender for demonstration we directly match emotions by similarity of emotions from music and user s current status Besides there may be other scenes with a more specific demand For example music therapy follows progressive emotion arousal and vehicle mounted application scenarios focus more on driving safety where fatigue driving can be detected and responded to with alarm or some uplifting music Output Emotion Integration To integrate emotion results generated from facial and audio data we execute the following steps For one moment we capture the user s current facial expression images and audio to train two models separately and output probabilities for six emotion categories Mapping the probability vector of six categories into Valence Arousal space by Table Combined the output emotion label as weighted average by f score for a certain emotion from two models x y The weighted score of valence arousal combining two models x y The valence arousal matrix of the six emotions p probability vectors of the emotion categories F The model f score for a specific emotion The suffix f represents results from the facial emotion model and the suffix a represents results from the audio emotion model We are doing this weighted average by assuming that in terms of one label the higher the F score the more confident a model is The prediction probabilities represent the intensity of different emotions Table Mapping Emotions into Valence arousal Emotions Valence Arousal angry fear happy neutral sad surprise This D matrix is obtained from Soundtracks dataset where intensity ratings for more than emotions are collected from professional musicians after they listen to over soundtracks from famous movies The ratings dimensions include valence and arousal range from as well And the targets of each piece of music cover our six emotion categories So by calculating the overall average valence arousal for each emotion we built this matrix Music Recommender Design Chill Commute Energy Boosters Feel Good Party Romance Sleep Workout YouTube Music recommend by mood moments Through facial expression recognition and audio emotion recognition systems information about the user s current emotion will be fed to any collaborative or content based music recommender as supplementary features We expect it can boost conventional recommender by improving recommended precision Valence arousal scores for emotions are gathered from the dataset Soundtracks Eerola T Vuoskoski J K A comparison of the discrete and dimensional models of emotion in music Psychology of Music https doi org Detailed page of the dataset https osf io wzc  We assume two ways for music recommendation engines to collaborate with user emotion data Algorithm Boost Feature matrix integration The proposed system is an interactive mood based songs recommendation system that considers the current emotion of the user along with vital factors related to songs and user preferences for these factors while recommending songs When the user gives feedback thumb score labels are generated Recommender then retrains the model to learn the user preference pattern with moods The recommender performs better as users interact more Emotion Time Directly mapping music by the current mood The second way is directly mapping music with the user s current mood through a matching algorithm As mentioned above emotions can be deconstructed into dimensions Arousal and Valence All moods will be mapped with labeled music in the dataset The mapping process required the involvement of musicians and volunteers To simplify this complex process our recommender maps music in a similar mood to resonate with users Music Emotion Recognition Dataset Algorithm Boost is built on the base of a comprehensive music recommender which is not the focus of our research In order to deploy and validate our design a demonstration of the Emotion Time music recommendation system was established We pick SoundTracks as a sample music dataset as it contains fruitful soundtracks from famous movies and all are labeled by valence arousal scores This large study established a set of film music excerpts half were moderately and highly representative examples of five discrete emotions anger fear sadness happiness and tenderness and the other half were moderate and high examples of the six extremes of three bipolar dimensions valence energy arousal and tension arousal These excerpts were rated in a listening experiment by non musicians and take average ratings for Valence Energy Tension Anger Fear Happy Sad Tender Finally the emotion with the highest ratings is assigned to this piece of music as the target A Demo for Emotion Time Recommender Because of dataset constraints we are not able to get streaming facial expression images and audio for one user Thus we simplify the process and build a demo program to demonstrate how the Emotion Time recommender works In the demo we pick one result probability vector from the facial recognition model After mapping it into valence arousal space we calculate its cosine similarity with every piece of music in the dataset and filter the top pieces of music with the largest similarity to recommend From the testing results of the demo program most users are recommended by music in a similar mood Figure Top Recommended music for No user in facial dataset Limitation and Future Improvements The general analysis framework has been constructed up to now However if we want to put this framework into industrial realization there are still some limitations calling for future improvements Datasets We have concluded three biggest problems in our framework as below first and foremost our facial dataset and audio dataset are collected from different sources and hence they are also from different people For this reason we are not able to merge these two datasets together to apply the formula presented in section However this is not a big issue for real business use cases because it is possible for customers to agree on providing facial and audio data authorization at the same time Secondly our datasets are not comprehensive enough which may contain biases In the current datasets the face images are basically taken from the front side and the audio dataset simply includes audio records from speeches However it is not possible nor is it appropriate to ask users to take photos at this exact angle or speak in an exact tone Thus in the training stage the data should include more possibilities for wider use cases For example facial images from all angles of a face can be trained to recognize facial emotion and the model should also be able to tell See the full demo program here  emotions from a person with and without wearing glasses Audio data used for model training purposes might include daily talks which provide more noise than speech scenarios The more input cases are included in the training and model construction part the better product experience customers will have in the future application The last problem regarding data is the limitation of the target variables Although we have types of emotions it will be more helpful if levels of emotional intensity such as happy high and happy moderate instead of simply happy are included in the labels If we can provide more accurately predicted emotion classes there will likely be more benefits for real applications and usage Models Secondly our models have some limitations Although the overall accuracies of the two models seem good we wonder if they are high enough for business purposes Moreover as presented in sections and we can see that our facial and audio emotion recognition models have different predictive power for different emotions This might provide some difficulties for customers under certain circumstances in usage To tackle this limitation other possible data augmentation methods and more advanced predictive models from papers could be applied These new methods might be useful even if we use more data later to exclude possible training data selection biases Music Recommendation Thirdly some improvements could be made to the recommendation strategy With the limited data resource we can only generate the formula in section Besides we use cosine similarity to decide which pieces of music to recommend It is reasonable since if the models predict that the user is angry now this recommender will not play a happy song However if available in industries some other mechanism could be designed based on insights from psychological research and user feedback from industry experiences Last but not least we need to consider the variety of real world use cases and make adjustments accordingly If our recommendation layer is implemented into a car it should not only play music to avoid fatigue but also make some alerts if the driver loses attention If this system is applied in an AI assistant designed for family usage more features than simply emotion can be inputted to improve the enjoyment in a home circumstance What we have provided in this project considering these real cases is a baseline framework with huge potential in real industries This implies that future works are needed for more specific cases and considerations Conclusion In this project we have successfully built a framework for music recommendation with facial and audio emotion recognition For facial emotion detection we explored the CNN and DCNN models finally reaching an average accuracy of For audio emotions we put much effort into data augmentation and feature extraction Though the audio data is more complex we reached an average accuracy of with the CNN model Recognizing emotions from facial expressions and audio speech is very useful in real world cases Our project mainly focuses on music recommendations based on the emotion we have detected We deep dived into the professional area of psychology especially the Valence Arousal space Depending on our research on various music databases and the matching strategies we are able to build a demo program recommending music of similar mood to the current emotion Though we simplify some procedures and there are improvements to be done the emotion based music recommendation layer can work smoothly acting as a powerful framework Furthermore it is of good potential for modification for industrial specific needs Especially for the emotion detection techniques which could be widely applied to other products or other industries to improve product functions and customer satisfaction efficiently Reference L Mou J Li J Li F Gao R Jain and B Yin MemoMusic A Personalized Music Recommendation Framework Based on Emotion and Memory IEEE th International Conference on Multimedia Information Processing and Retrieval MIPR pp doi MIPR F Kuo M Chiang M Shan and S Lee Emotion based music recommendation by association discovery from film music Proceedings of the th annual ACM international conference on Multimedia Association for Computing Machinery pp D Ayata Y Yaslan and M E Kamasak Emotion Based Music Recommendation System Using Wearable Physiological Sensors in IEEE Transactions  on Consumer Electronics vol no pp May doi TCE Song Yading and Simon Dixon PREDICT THE EMOTIONAL RESPONSES OF PARTICIPANTS F H Rachman R Samo and C Fatichah Song Emotion Detection Based on Arousal Valence from Audio and Lyrics Using Rule Based Method rd International Conference on Informatics and Computational Sciences ICICoS pp doi ICICoS J Kim S Lee S Kim and W Y Yoo Music mood classification model based on arousal valence values th International Conference on Advanced Communication Technology ICACT pp J Bai et al Dimensional music emotion recognition by valence arousal regression IEEE th International Conference on Cognitive Informatics Cognitive Computing ICCI CC pp doi ICCI CC A Kolli A Fasih F A Machot and K Kyamakya Non intrusive car driver s emotion recognition using thermal camera Proceedings of the Joint INDS ISTET pp doi INDS Morrison D Wang R De Silva L C Ensemble Methods for Spoken Emotion Recognition in Call Centers J Speech Communication Build Your First Mood Based Music Recommendation System in Python https towardsdatascience com build your first mood based music recommendation system in python a d Tidke S Bhutkar G Shelke D Takale S Sadke S Mood Based Song Recommendation System In et al Human Work Interaction Design Artificial Intelligence and Designing for a Positive Work Experience in a Low Desire Society HWID IFIP Advances in Information and Communication Technology vol Springer Cham https doi org A Aljanaki F Wiering R C Veltkamp Studying emotion induced by music through a crowdsourcing game Information Processing Management Deep Convolutional Neural Networks https www sciencedirect com topics computer scienc e deep convolutional neural networks Speech Emotion Recognition with CNN Kaggle
De ne people s MBTI personality through Machine learning Abstract Personality tests can help us better understand ourselves and provide guidance on career paths The motivation for building such a classifier is that companies and organizations would like to provide more customized products or services It is valuable for them to know their customers MBTI types and provide suitable services However nowadays the MBTI test takes quite a long time to complete Normally people will lose patience in the end and randomly select the answer just to complete the test leading to potential inaccurate test results For companies it is difficult and ineffective to collect customers personality types by doing the test Moreover some customers may not be willing to share their MBTI type with the company due to privacy concerns To save time and effort spent on the tests we propose a machine learning solution identifying people s MBTI personality through their posts in the forum We explored two ways of model building multiclass classifiers and combinations of binary classifiers compared their performance and discussed the potential limitations and business values The GitHub link for this project is https github com lychen BT MBTI Gro up Introduction Background Personality is the characteristic sets of behaviors cognitions and emotional patterns that evolve from biological and environmental factors For years people have been trying to link individual behavior with their personality The Myers Briggs Type Indicator MBTI is one of most popular personality tests in the world which is used in businesses online for fun for research and lots more It attempts to assign people to four categories introversion I or extraversion E sensing S or intuition N thinking T or feeling F judging J or perceiving P Problem Statement A typical MBTI questionnaire contains about multiple choice questions which usually takes about to minutes to complete Nowadays people are getting more and more impatient with filling out questionnaires People probably won t be willing to take time to test for MBTI results or they are likely to randomly choose an answer in order to quickly finish the test In the meantime companies and organizations want to provide highly personalized products and services for their customers to achieve better experience while the application of personality in customized services is not extensive due to difficulty to collect this data The objective of this project is to solve these problems by designing a machine learning system that evaluates individuals MBTI results by analyzing their recent posts on the Internet rather than through questionnaires Objective We aim to apply text analytics techniques to understand individual Myers Briggs Personality Type MBTI from the post which user posted in the social media so that we can Identify their MBTI personalities type Precisely push relevant products and services for people with different personalities for social media marketing purposes Increase pairing success rate by recommend people with similar personalities for dating applications and websites Increase the collaboration between different personalities in hope to increase workplace diversity Data and Preprocessing Data Description Our dataset is obtained from Myers Briggs Personality Type Dataset from Kaggle Mitchell According to Mitchell the data was collected from the Personality Cafe Forum Personality Cafe Forum is a popular forum community where people with all ranges of personality  Define people s MBTI personality through Machine learning types post and discuss interests health behavior care personality and more The dataset contains over rows Each row is data for a person and it records the type of the individual and this individual s posts Table Dataset Description Variable Data Type Example Type String INTJ Posts String Dear INTP I enjoyed our conversation the other day The data is highly imbalanced with more posts from top personality types consisting of of total posts The rest personality types have less number of posts of total posts Fig Distribution of personalities We want to dive into each of the different scales to see which scales have more imbalanced data distribution It is observed that Introvert Extrovert Sensing Intuition scales have more imbalanced distribution specifically we have less data from Extravert and Sensing types Personal attributes distribution in Thinking Feeling Judging Perceiving scales are relatively balanced This is probably because Introvert and Intuition types of individuals like to post online Fig Distribution of each of the scales E I S N T F J P Data Preprocessing Since both traditional machine learning models logistic regression random forest etc and deep learning models CNN and BERT will be examined in the following part different kinds of data preprocessing methods are required For the TF IDF algorithm heavy cleaning should be done to our dataset while the BERT model may give a better performance along with light cleaning Alexander In this regard we conducted a two step data preprocessing In the first step we only converted the text to lowercase and removed url digits punctuations diacritics and whitespace In the second step we further cleaned our dataset by removing the stop words and performing lemmatization Figure shows the detailed data preprocessing jobs Fig Data preprocessing Exploratory Data Analysis Before deep diving into models we want to have an overall understanding of our data and text content hence several EDA methods are performed Length of Posts We calculated the length of each post and found that the average word count is per post The distribution across different personality types is similar ESFP type has slightly shorter average post length but it could be due to smaller data samples for this type  Define people s MBTI personality through Machine learning Fig The length of posts for all personalities Word Cloud A word cloud is a visual representation of text data The more important or frequently occurring words are shown with larger font size We generated word clouds for each type of personality with max words in a graph To avoid noise from common words across different types the original top common words such as people think know are removed We selected some types to show in the figure below The insights are The posts normally include the personality type of the individual Word feel occurs frequently for types with F feeler This can be understood as people with the Feeling F trait follow their hearts and emotions Word friend is more common for Extrovert it is reasonable as extraverts are interested in engaging with their environment including the people around them Fig Word cloud analysis Sentiment Analysis Sentiment analysis is the practice of using algorithms to classify text into overall positive or negative categories We want to leverage sentiment analysis to understand whether different personality types tend to show more positive or negative emotions in their posts We performed sentiment analysis using the NLTK library The output is a dictionary of different scores negative neutral and positive scores which are then used to calculate the compound score The compound scores can range from to and are visualized using Altair in the figure below It clearly shows that the majority of posts are positive Certain types have slightly higher percent of negative posts such as ESTJ INTP ISTP One explanation for ESTJ having more negative posts is that they are generally more judgemental stubborn and easily agitated thus may have more negative or critical statements in their posts Fig Sentiment scores for all personalities Method We constructed two types of models to predict one s MBTI type one is a multi class classifier while the other one is a binary classifier In this part we will mainly show the result of multi class classifiers including TF IDF Doc Vec and neural network techniques Details about the binary classifier will be covered in Method part TF IDF We implemented TF IDF to transform the textual data to a vector and then we run several classifiers based on this TF IDF vector Since our dataset is highly imbalanced both accuracy and f score are regarded as evaluation metrics The result is listed in Table and it turned out that XGBoost outperformed all other models in terms of accuracy and f score  Define people s MBTI personality through Machine learning Table Model Performance with TF IDF Model Accuracy F score Logistic Regression Random Forest XGBoost LightGBM Multinomial Naive Bayes LinearSVC Doc Vec Though TF IDF is one of the most popular techniques utilized in text classification it doesn t take the word order into account To further explore the semantics and syntactic order of words in a complex text we decided to apply Doc Vec to our dataset which is an unsupervised algorithm based on word vec method that can generate a numeric representation of a document Shperber p After we constructed the feature vector we trained several classifiers including Random Forest XGBoost and etc to examine whether the document embedding model could improve the model performance The result is listed in Table Table Model performance with Doc Vec Model Accuracy F score Logistic Regression Random Forest XGBoost LightGBM LinearSVC The result shows that logistic regression outperforms its counterparts in terms of accuracy and f score and its confusion matrix is shown in Figure The confusion matrix indicated that even if the model failed to correctly predict the MBTI type of a specific post it can still identify the post as another MBTI type which generally shares or more categories in common with the real type suggesting that people with similar personalities may have similar texting habits Fig Confusion matrix of logistic regression model  Define people s MBTI personality through Machine learning Neural Network We tried two types of neural network models to build the multi class classifier We split the dataset into a training set a validation set and a test set Training set is used for model training and the validation set and test set are used for model performance evaluation CNN CONVOLUTIONAL NEURAL NETWORK For CNN we set the size of vocabulary to embedding dimensions to dropout rate to epoch to and batch size to The optimizer we leverage on is Adam We use a combination of grams grams and grams filters and max pooling method We use dropout learning rate decay and early stopping to prevent overfitting The initial learning rate is e Decay steps are Decay rate is The performance on the test set of CNN is presented in Table Table CNN Prediction Performance on Test Set Precision Recall F score Accuracy Macro avg Weighted avg BERT BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS For the BERT classifier we used a pre trained BERT layer combined with a dropout layer The optimizer we choose is Adam The batch size is The dropout rate is and we use a ReduceLROnPlateau learning rate scheduler The initial learning rate is e The performance on the test set of BERT is presented in Table Table BERT Prediction Performance on Test Set Precision Recall F score Accuracy Macro avg Weighted avg Method Binary Classifier For method our idea is to based on MBTI measurement dimensions we train classifiers individually to classify their personalities The Myers Briggs Type Indicator MBTI divides everyone into distinct personality types across axis Introversion I Extroversion E Intuition N Sensing S Thinking T Feeling F Judging J Perceiving P Algorithm used Logistic regression KNN SVM and XGBoost are applied to classify each axis MBTI type indicators were trained individually and the data was split into training and testing dataset using the train test split function from sklearn library Totally of data was used as the training set and of the data was used as the test set The model was fitted onto the training data and the predictions were made for the testing data Results The results of all the algorithm were shown in the below table Table Results of different algorithms in axis Extraver sion E Introver sion I Sensing S Intuition N Thinkin g T Feeling F Judging J Perceivi ng P Logistic Regressi on KNN SVM XGboost After calculating the average accuracy of four dimensions we select SVM to do the evaluation of method because it has the best average accuracy Evaluation of Method In order to evaluate method final prediction accuracy we randomly select data from the dataset input to the model after preprocess and vectorizing The result of the data prediction were shown below  Define people s MBTI personality through Machine learning Table Method prediction result Correct Prediction Wrong Prediction D Wrong D Wrong D Wrong Accuracy of correct prediction of one dimension prediction wrong of two dimension prediction wrong of three dimension prediction wrong From the result we found that the accuracy of method is lower than method We think there may be other factors affecting the accuracy instead of model building issues Discussion Though the administration and interpretation of the MBTI is a huge business and force in shaping the general public s perceptions of psychology Stein and Swan p arguments and criticisms of the MBTI never stop First some studies suggested that the MBTI test has poor reliability and poor validity To be more specific poor reliability means that the test results can vary a lot when the same individual retaking the test Sambursky just like Pittenger p pointed out Across a week re test period of the participants received a different classification on one or more of the MBTI scales As for MBTI s poor validity Boyle p argued that since MBTI types are not source traits verified factor analytically predictions based on these surface traits are inevitably less powerful and remain somewhat speculative Also some studies believed that MBTI is not comprehensive because its categories do not capture the full extent of personality Sambursky p Most importantly MBTI overlooked the fact that personality traits are not static Sambursky p For instance MBTI assumes a person is either an Extrovert or an Introvert however the distribution of personality traits may be a bimodal distribution rather than a normal one suggesting that personality dimensions are continuous with persons being more or less extraverted or introverted Riggo p And that can partially explain why our model failed to meet expectations because there isn t any clear boundary between each personality type for instance even if a person claimed himself to be an INTJ he can also have characteristics of an Extrovert or a Feeler Interpretability Regardless of the high accuracy of our model it is currently still a black box and hard to interpret We need to add a model interpretation for people to comprehend and trust it SHAP is used to interpret how our model makes predictions on a global and local scale Taking the SVM classifier Extraversion E v s Introversion I as an example we randomly take posts from training data and explain how each word impacts the output of the model On the global scale the impact of each word is stacked to create the importance plot as shown in figure The model has lables lable stands for Introvert and lable is Extrovert The top most important words as calculated by SHAP are ne or intuition and extraversion awsome fun love ect These are all the words which we naturally relate to Extraversion suggesting that our model has managed to learn the important features from posts and does have high interpretability Fig SHAP summary plot for Extrovert Introvert SHAP is also able to interpret our model locally The following figure shows a Force Plot of an individual post and explains how each word contributed to that post s prediction Let us take this clean posts as an example cal definitely without doubt nt search answer way scream rational think observant think action acting n awesome article one question could totally wrong  Define people s MBTI personality through Machine learning pictured roo ne piglet ni owl ti mostly roo spontaneous look proud trying think know irl u Here the predicted label is or Extrovert whereas the true label is also The base value is the average of the model output over the training dataset In this case the training dataset is the posts which we randomly sampled and the base value The numbers on the plot are the value of the feature for this post Red arrows represent features that pushed the model score higher or more Extroverted and blue represent features that pushed the score lower or more Introverted The bigger the arrow the bigger the impact Fig SHAP force plot for a specific post For the most part the red arrows are related to Extraversion like the words awesome and action But there are few counter intuitive ones The word fun is interpreted as a contribution to Introversion score To conclude SHAP can help us understand how each word collectively contributes to model prediction although it does have some errors Nevertheless it provides a useful tool to open the black box and interpret our model Comparison between Method and Method Analysis of accuracy method has better performance than method For some models such as XGBoost and CNN they can reach which is a very good accuracy However for method the accuracy of correct prediction of dimensions is only which seems to indicate a weak overall ability of our model to correctly classify all four MBTI dimensions Even though method may achieve higher accuracy of perfect classification they do have a risk of getting their prediction completely wrong Method treats all classes as independent of each other so fails to capture the in built relatedness of some types to other types For example INFP is much more similar to INTJ than it is to ESTJ For method achieve lower rates of perfect classification in exchange for higher rates of approximately correct classification Business Potential Area Our model is expected to accurately predict MBTI and will bring business value in almost anything which involves people Peter We will expand on of the potential model applications Increase successful date matching rates for Dating apps websites Our work will improve pairing success rates for dating applications and websites Through our work the pairing will become much more effective It will greatly increase the pairing success rates and bring a good reputation for the company Product recommendations and personalized marketing Customers are increasingly expecting companies to treat them as individuals rather than mass marketing Lindecrantz et al Personalization can also improve customer retention and brand affinity Understanding how different personality types behave can help companies to understand consumer preferences how to reach them and their acceptance to a marketing campaign Evans Madeline Knowing the audience and personalizing products can boost engagement brand reputation and loyalty The need for segmentation presents a promising application to our model Utilize the power of diversity in the workplace Another potential application builds on top of the buzzword diversity In McKinsey reports that those in the top quartile for ethnic and racial diversity in management were more likely to have financial returns above their industry mean Hunt et al In addition knowing people s personalities and leveraging it could help to build stronger more effective teams in the workplace Like design various team building exercises for teams with different backgrounds or design individualized training programs Our model is expected to accurately identify an individual s personality type based on one s posts which is less subjective compared to the self reported values Therefore the model provides one more facade to consider when making crucial executive decisions or executive talent management reducing the risk of homogeneity in the workplace Recommendation for job matching websites Our model is useful for organizations that provide platforms to link talents and job positions It adds one more dimension to consider when pushing relevant jobs to people with different MBTI Organizations like JobsDB and Linkedin aim to tap into potential customers and push suitable jobs to them Our work will contribute some value to that As we all know different personalities have different expectations of work and personal development for example people with INTJ type would be interested to lead or find management level positions in the job market  Define people s MBTI personality through Machine learning They may have a high interest in positions that require leadership skills Knowing people s personality could help websites like linkedin to push accurate recommendations Future Work Although the model is able to achieve reasonable accuracy we have identified the following two areas of improvement The current datasource could be further improved on two aspects First is to avoid dataset homogeneity as the current dataset is only collected from a single source the Personality Cafe Forum and is therefore prone to the risk of overfitting to that datasource We could reduce bias by including more posts comments from various other data sources and increase the model s ability to generalize Another aspect is to have a more balanced class distribution As we mentioned in the data preprocessing section the personality distribution is highly imbalanced We could get additional data for all personalities and enable the model to learn equally from each of them In addition the model should be robust enough to handle uncertainties in different situations For example the obtained information or the data available for analysis in real life could look quite different from the posts we used in this project The model needs to be generalized to learn from various kinds of data not only posts while achieving similar performance at the same time Conclusion In this project we used methods to identify a person s MBTI type Method is to classify an instance into one of the categories and is able to achieve a reasonable F score of However Method is not without its limitations as it assumes a person can only fall into one of the personality types and that the MBTI types are independent from each other To address these limitations we have designed Method which build binary classification models with one classifier for each dimension Introversion I Extroversion E Intuition N Sensing S Thinking T Feeling F Judging J Perceiving P and combine these results to get a final prediction Method has a lower combined accuracy in terms of correct prediction across all dimensions but it has a different prediction objective Method focuses more on binary classification in each dimension not how well it can label an individual into one of the categories Since the accuracy in each individual dimension is quite high average accuracy is Method could be more useful in situations where knowing which of the MBTI types a person has is less important and just knowing one personality dimension is enough to satisfy the application s requirement For example in companies management recruitment they may care more to know whether a candidate is of type Judging J or Perceiving P not which of the MBTI categories does he she falls into It is worth pointing out that MBTI is not designed to serve a clinical purpose It is influenced by both nature and nurture and may change throughout an individual s life course A person s scores on the MBTI should be used as means to guide us making better decisions and should not be used as labels Reference Alexander B November Does Bert Need Clean Data part classification Alexander Bricken Retrieved April from https bricken co nlp disaster tweets Shperber G November A gentle introduction to doc vec Medium Retrieved April from https medium com wisio a gentle introduction to doc v ec db e c cce e Stein R Swan A B Evaluating the validity of Myers Briggs Type Indicator theory A teaching tool and window into intuitive psychology Social and Personality Psychology Compass e Sambursky V February Myers Briggs test Limitations and need for a better diagnostic tool Endominance Retrieved April from https www endominance com myers briggs test limitati ons and need for a better diagnostic tool Riggio R E February The truth about Myers Briggs types Psychology Today Retrieved April from https www psychologytoday com us blog cutting edge l eadership the truth about myers briggs types Pittenger D J Cautionary comments regarding the Myers Briggs type indicator Consulting Psychology Journal Practice and Research  Define people s MBTI personality through Machine learning Boyle G J Myers Briggs type indicator MBTI some psychometric limitations Australian Psychologist Geyer P Understanding the MBTI and personality type Retrieved February Lindecrantz E Gi M T P Zerbi S Personalizing the Customer Experience Driving Differentiation in Retail McKinsey Insights March Eri im Adresi https www mckinsey com industries retail our insights personalizing the custo merexperience driving differentiation in retail Evans M May The A s of marketing to different personality types Setup Retrieved April from https setup us blog as to marketing to personality typ es Hunt V Layton D Prince S March Why diversity matters McKinsey Company Retrieved April from https www mckinsey com business functions people and organizational performance our insights why diversity m atters
BT Group Project Report E Commerce Product Matching Group Ji Lin Cheng A W Li Qiaojun A X Liu Binghui A R Ye Li A U Zhao Heng A L Github Link https github com Lesterhuihuihui BT Group Shopee Matching Guarantee Abstract In e commerce scenarios recommendation systems based on hundreds of thousands of product information are common and necessary And product matching is an important feature of a recommendation system In this report we study the product matching algorithms with the product data alone which is common in cold start The data is product information from a popular e commerce platform Shopee in In our study we explore the textual and graphic data of products and build models based on the matching performance of different products Our matching methods are evaluated to be applicable to real e commerce platforms Keywords e commerce product matching image transformation text vectorization Introduction E commerce the activity of buying and selling products over the internet has grown rapidly in the st century supported by the global digital revolution After decades of increases and evolving a study published by the IMF International Monetary Fund indicated that there s still a high growth potential of e commerce in Asia In this growing e commerce sector there have been many online platforms that have appeared in the market such as Amazon Ebay Taobao Shopee and Lazada These platforms have since employed different methods to improve their e service quality to acquire and retain customers One of the popular features on the platforms is the recommender system that suggests products to the customers To construct a good recommender system it is important to identify and group similar products from the large pool of product data on the platforms and feed them into the recommendation filtering algorithms However on the Kinda Tidiane E commerce as a Potential New Engine for Growth in Asia IMF Working Papers July online platforms sellers are free to add the product name and upload any image for product display where different images of similar goods may represent the same product or completely different products To avoid the misrepresentation issues where different items are recommended to the customers as similar ones machine learning models could be used to analyze the text and image of the products and correctly match and group them Thus we have identified a Kaggle dataset of consumer products from the ecommerce platform Shopee for our study and development of the machine learning models to compare product similarity and predict the product groupings Project Objective We aim to discover the problem of product similarity on online platforms In e commerce the information of products such as product names and illustrations often varies among different retailers to attract customers Thus in recommendation system design recognizing the similarity between products by their basic data can enhance the overall efficiency by providing more suitable options for users We aim to build predictive models based on Shopee product data to recognize identical or similar products with different names and illustrations In the following sections of the report we first explore our dataset followed by the methodologies we have adopted and finally our model results and the business applications that can be applied Dataset Data Overview We obtained our dataset from a Kaggle competition that is Shopee Price Match Guarantee held in April The dataset contains two parts one is a csv file with records the other is the set of images corresponding  E Commerce Product Matching for User Uploaded Image and Description to the csv file records The data fields available in the csv file are summarized in Table Column Name Data Type Description posting id String Product posting s unique id image String Product image s name image phash String Perceptual hash acts as a fingerprint title String Product s title label group Integer Product s grouping id Table Summary of csv file data For each posting it has an unique posting id a title for describing the product an image name that links to the actual image file in the image dataset and a label group id that indicates the product s group belonging For identical or similar product postings they will be grouped together with the same label group id and this will then be used in the models for matching the similar products Some examples of the posting are shown in Figure Figure First postings For the image dataset all images are saved as jpg files and the file size ranges from KB to MB Some images only show the product itself while some show the product on human models and some also show with text printed on the image Examples of the images are shown in Figure Figure Random images display Data Exploration Within the csv dataset we have noticed that all posting id are unique with a total of postings However the product images posted are not all unique Of the postings only unique images are found indicating that there are postings with duplicated images When grouping the postings with duplicated images we have noted that unique images have been used for more than one posting In Figure the top most used images are displayed with the number of usage Figure Top most used image The most used image Figure has been appearing in postings with different product titles All product titles contain the words bubble wrap which is the product shown in the image although some misspelled it as buble wrap or bubble warp And each of the product titles has put in additional words to describe the product Of the postings we have also observed that they re not all put in the same label group of them are grouped under and are under as shown in Table  E Commerce Product Matching for User Uploaded Image and Description Figure Most used image posting id title lebel group train Tambahan Bubble wrap Plastik Bubble Pelindun train Bubble Warp Pengaman Pengiriman train Tambahan Bubble Wrap train Bubble Wrap untuk ekstra packaging train BUBBLE WRAP EXTRA PACKING UNTUK BARANG ANDA train BUBBLE PACK UNTUK PACKING TAMBAHAN BUBBLE UNT train Tambahan Extra Bubble Wrap Pengaman Packingan train PACKING TAMBAHAN BUBBLE WRAP train Extra Bubble Wrap train bubble wrap BUBLE WRAP train Extra Bubble Wrap Pengaman Packingan train BUBBLE WARP train Bubble Wrap train EXTRA BUBBLE WRAP UNTUK PACKING train Buble Wrap Table Postings using the most used image Within the dataset the total of postings are grouped into distinct labels with the largest label group comprising postings and smallest label group comprising only postings Figure shows that most of the label groups contain less than postings and only a small portion of the label groups contains a double digit number of postings Figure Label group on postings count A closer look at the most postings label groups we can see that the top groups all have more than postings within the group Figure And the top groups all contain postings They re lip tint serum hair band moisturizer face tonic faucet filter and soap respectively Figure which could also indicate that most sales on the platform are in the beauty and household categories Figure Top label groups  E Commerce Product Matching for User Uploaded Image and Description Figure Images of the top label groups To perform text analysis on the product titles we have cleaned up the title text by removing the English stopwords and the punctuations to only keep the text that is meaningful for product description Then a word cloud is used to get us an initial view of the frequently used words for product description and identification As shown in Figure word cloud there are similar high frequency words in the product titles Words like original premium are used by the sellers to describe the products as authentic and Malay words like murah cheap and termurah cheapest are used by the sellers to position the product as budget bargains Figure Word cloud of title words We also counted the words used in the product titles and observed that the title length ranged from to words And most of the postings have the title length between to words distribution is shown Figure with the mode at length of words Figure Product title length distribution Data Preprocessing As explored in the previous parts Our dataset contains numerous image data and text data To make our model more robust we need to conduct critical data processing for both parts For image data all our images have corresponding hash value helping me perform matching score computation However for loads of text data we must apply some cleaning techniques to it As shown in the figure our data contains many numbers punctuations and consecutive characters which will disturb our data training Therefore we dropped all the alphanumeric tokens fixed consecutive characters and applied regex to filter non alphabetic content from tokens Figure Text Frequency st Baseline Clustering Solution To compare with our model we build several simpler baseline models based on clustering Model Introduction DBSCAN is the abbreviation of Density Based Spatial Clustering of Applications with Noise The model finds core data points with high density and expands clusters from them Therefore the model is suitable for clustering similar products which form clusters of similar density  E Commerce Product Matching for User Uploaded Image and Description Compared with a more popular clustering model K nearest Neighbor DBSCAN has better performance when there are outliers and when the samples are sparse in space Also DBSCAN does not require setting of cluster numbers which requires much tuning in K nearest Neighbor Data Preparation To build the model we only use the image data and apply embedding through EfficientNet to vectorize the images into high dimensional data EfficientNet is a convolutional neural network model which transforms graphic or textual data into lower dimensions without losing most of the information The model is also a transfer learning model which enables us to apply pretrained model weights to the embedding model The model we use is the EfficientNet B which is a popular image embedding pretrained model In image embedding to make the model run in the main memory we load data in chunks and process images in batches of to generate images as matrices Then we embed the image matrix with EfficientNet to generate data for modeling DBSCAN Clustering We build a DBSCAN model to cluster similar products and evaluate the matching performance with F score The metrics are suitable for machining We make clustering on each of the samples and for every sample in a cluster we add the others to its matching groups In real circumstances of recommendations based on product matching there should not be a large number of similar products to be displayed to users In the model we should control the numbers of similar samples In the baseline model we simply remove redundant samples to meet the number requirements instead of detailed comparison of product similarity For the DBSCAN model we control the general sizes of clusters by setting the maximum distance between two samples for one to be considered as the neighborhood of the other In our baseline model we tune the parameter from to and compare the corresponding F scores and set the ultimate distance as Also we combine the clustering result of the DBSCAN model with the result from grouping the perceptual hashing value of products to generate an extra result for the baseline model The best F score of perceptual hashing DBSCAN and their combination reaches Figure Tuning result of DBSCAN model nd Deep Learning Baseline Solution Given clustering methods show the low performance in price matching for Shopee we transferred our focus to Computer Vision NLP and phash matching phase Therefore we combined three methods including Perceptual hashing TF IDF and ResNet For perceptual hash we match them by map functions For text analysis we use TFIDF to transform text data into vectors and text embedding and then analyze text chunk by chunk When it comes to image data we load pretrain Pytorch models and set ResNet as our main framework Our modeling performance is demonstrated in the following table which is better than first machine learning models namely clustering method Type F Score Combined Score Image hash score TFIDF ResNet for images Table Deep Learning NLP baseline Modeling Performance Enhancement Data Argumentation There exist loads of duplicated images and concentrated titles in our dataset As shown in the figure some images are overused while other images are only used once This is similar in product titles where the same tiles or expression words are utilized mostly  E Commerce Product Matching for User Uploaded Image and Description Figure Counts of Top duplicated images Figure Counts of Top overused titles For image data Under the circumstance we need to perform data augmentation for existing imbalanced dataset We use OpenCV to argument our image data including Horizontal Flip Shift Scale Rotate Optical Distortion and self defined function to transform original images and create more data The samples of original images and augmented images are shown below Figure Original Image and Augmented Images For text data we use methods to transform existing data which are Swap character Delete character Word Augmenter BERT Augmenter TFIDF Augmenter and Sentence Augmenter respectively Model Adjustment For image hash each image owns their own hash value in our dataset Therefore how to better use hash to match products will be significant for our modeling We applied four hashing methods including average hashing Perceptual hashing pHash difference hashing dHash wavelet hashing wHash as our main pipeline and calculate corresponding F score After consulting Kaggle Gold winners we realize that the pHash will provide us with higher F score For Image color we visualize the main color steam in our dataset in the following figure As shown most sample groups have similar color demanding us to pay more attention to the differences of colors Figure Color hist of our image dataset For local features Feature matching refers to the act of recognizing features of the same object across images with slightly different viewpoints Some key points will be crucial in identifying specific images There are a few different ways to find key points in an image In this part we use OpenCV KeyPoint to seize most local instances and features Its affiliated working principle is shown below Figure KeyPoints to identify products rd Final ShopeeNet Solution Based on previous models and data argumentation we proposed our final model for final matching We decide to further adopt NLP and deep learning methods which are NFNet Swin Transformer ArcFace and TF IDF The aim of our target is to improve our modeling performance compared with our baseline models The pipelines we worked are illustrated in the figures represents preprocess data represents CNN construction represents CNN configuration represents model running and performance  E Commerce Product Matching for User Uploaded Image and Description Figure Image model workflow pipeline Figure Price matching workflow pipeline For image matching NFNet Normalizer Free Networks In convention ResNets without normalization are often unstable for large learning rates or strong data augmentation NFNet proposes an adaptive gradient clipping technique which overcomes these instabilities for ResNets without normalization Figure Working mechanism of NFNet ArcFace Additive Angular Margin Loss ArcFace or Additive Angular Margin Loss is a loss function used in face recognition tasks The softmax is traditionally used in these tasks However the softmax loss function does not explicitly optimize the feature embedding to enforce higher similarity for intraclass samples and diversity for inter class samples which results in a performance gap for deep face recognition under large intra class appearance variations Swin Transformer The Swin Transformer is a type of Vision Transformer It builds hierarchical feature maps by merging image patches shown in gray in deeper layers and has linear computation complexity to input image size due to computation of self attention only within each local window shown in red It can thus serve as a general purpose backbone for both image classification and dense recognition tasks In contrast previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self attention globally Figure Working mechanism of NFNet Image Embedding Models Based on previous methods we design two image matching models helping us extract useful features Their working pipelines are as follows which will supplement each other by extracting image features together At the end of layers we design a normalizer and cross entropy loss function to control modeling performance Figure Final image model working pipeline For title matching TF IDF TF IDF term frequency inverse document frequency is a statistical measure that evaluates how relevant a word is to a document in a collection of documents This is done by multiplying two metrics how many times a word appears in a document and the inverse document frequency of the word across a set of documents  E Commerce Product Matching for User Uploaded Image and Description For Phash matching Phash Perceptual hashes must be robust enough to consider transformations or attacks on a given input and yet be flexible enough to distinguish between dissimilar files Such attacks can include rotation skew contrast adjustment and different compression formats All these challenges make perceptual hashing an interesting field of study and at the forefront of computer science research Validation Data We also split our data into training data validation data and test data into modeling A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model s hyperparameters According to previous matching methods we just combine their matching results using the intersection method and then acquire the final matching method Training Performance Evaluation In this part we use the same performance evaluation method F score as previous parts we did We set epoch equals to and compute the best threshold and corresponding scores When the epoch is greater than then the best score will decrease The best threshold is which appears from the th epoch Epoch Threshold Best F Score Table Best score and threshold of each epoch in training data Figure Accuracy and Threshold of Epoch in training data Test Performance Enhancement Process In retrospect to our modeling performance enhancement process of this part we primarily perform the following techniques in the table Action Final F Score Combine image text and hash Add Normalization With Best Threshold With Validation Final intersection Table Our modeling performance enhancement process Business Applications Matching a seller listed item to an appropriate product has become a fundamental and one of the most significant steps for e commerce platforms for product based experience It has a huge impact for providing better user experiences by making search effective supporting product reviews and product price estimation as well as many other advantages With the machine learning model we built we can now easily identify and group the similar products from the large dataset of the e commerce platform based on just the product image and product title description Then the categorized product listing can be used by the platform to gather the product price insights understand if the product is overpriced or underpriced among the peers and provide market analysis to the suppliers on the platform This allows the platform to encourage the suppliers to stay active and competitive for attracting more customers The grouped product listing can also be used as the source information for the product search engine on the platform to improve the search results From the traditional exact match of terms search our processed dataset would be able to add the similar products into the search results This not only allows the customers to get a more complete listing for their search but also aids the customers to compare and find the best deals With the product groupings assigned it allows the platform to rank the products within the group based on attributes like sales reviews or even the promotion tactics Then on the specific product pages the higher ranked product from the same grouping can be selected to be displayed for customer s comparison and consideration which enriches the customer s user experience and in the event of promotional season attracts the customer s attention to shop on the promoted items in the same product groupings  E Commerce Product Matching for User Uploaded Image and Description Lastly the product grouping can be further analyzed for understanding the relationships between groups Then it could be used to construct recommender systems for recommending similar products to the customers based on user profiles and browsing histories and recommending products in complementary groups based on purchases Limitations and Further Studies Our product matching is only based on the product image and product title which is the limited information we get from the dataset For further studies other attributes of the products can be considered like the product brand weight material etc For further studies our solution can be extended to identify similarity among entities with richer information such as video or audio and support different types of product embedding such as triplet model or graph based models to fit more production use cases The inputs to our models are structured texts that it is not very easy to scale up to millions of classes And computing pairwise similarities among billions of products are resource demanding which makes it challenging in large scale data processing To provide a flexible and consolidated solution at scale distributed computing technology may be considered in the future studies or for actual business applications Lastly the definition of similarity varies across different applications in different eyes e g substitute products could be viewed as similar products with common design patterns could be viewed as similar and products with near colors could be viewed as similar Hence the grouping of the products could be further improved by incorporating feedbacks from the downstrain models or even directly from the users or domain experts References E Shieh S Simhon G Aluri G Papachristoudis D Yakut and D Raghu Attribute Similarity and Relevance Based Product Schema Matching for Targeted Catalog Enrichment IEEE International Conference on Big Knowledge ICBK pp Gingold D August Face Recognition and ArcFace Additive Angular Margin Loss for Deep Face Recognition https medium com analytics vidhya face recognition and arcface additive angular margin loss for deep face recognition abc c Huang B Juaneda C S n cal S L ger P Now you see me The attention grabbing effect of product similarity and proximity in online shopping Journal of Interactive Marketing doi j intmar Jbene M Tigani S Saadane R Chehri A Deep neural network and boosting based hybrid quality ranking for e commerce product search Big Data and Cognitive Computing doi bdcc Khan S March NFNet High Performance Large Scale Image Recognition Without Normalization paper explained https samreenkhan medium com nfnet high performance large scale image recognition without normalization paper explained d b c Payne M November Best Use Cases For Product Matching in Ecommerce How You Can Implement Each One https www width ai post product matching in ecommerce Shieh E Simhon S Aluri G Papachristoudis G Yakut D Raghu D Attribute similarity and relevance based product schema matching for targeted catalog enrichment Paper presented at the doi ICKG Shah Kashif Selcuk Kopru and Jean David Ruvini Neural network based extreme classification and similarity models for product matching Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies Volume Industry Papers Z Liu Y Lin Y Cao H Hu Y Wei Z Zhang S Lin B Guo Swin Transformer Hierarchical Vision Transformer using Shifted Windows doi arXiv Zuo Zhen et al A flexible large scale similar product identification system in e commerce KDD Workshop on Industrial Recommendation Systems
Does Sentiment on WallStreetBets Help to Predict Stock Price Social Sentiment Analysis Using Ensemble Approach Abstract Sentiment on the WallStreetBets a Reddit online forum has become a must track even for mainstream investors due to its role in an unprecedented rally of GameStop stock However limited research exists on evaluating whether WallStreetBets social sentiment meaningfully helps to predict stock prices In this paper we predict sentiment over the entire for a basket of stocks of varying market capitalization using an ensemble learning approach Sentiment features are then used as input to a Long short term memory model to predict stock prices along with other financial indicators Our findings indicate that sentiment does improves the prediction of the some stocks close price particularly those of lower market capitalization Problem Motivation WallStreetBets WSB shot to fame in early for its role in an eye popping short squeeze on GameStop GME stock that sent the stock from a mere at the start of up to in end January Since then social sentiment on stock prices has been closely tracked as seen in the proliferation of various sentiment tracking websites like Swaggystocks and Apewisdom These websites provide high level numbers on overall WSB sentiment and comment volume allowing those who do not have time to pour through the subreddit to get a sense of the topics WSB are actively discussing However further study is required to understand if social sentiment is correlated with the stock price and whether sentiment can act as a leading indicator enhancing related predictive models Should a correlation be shown to exist various financial instruments and trading strategies can be deployed to profitably benefit traders Conversely if there is no relation at all it would imply that the GME rally was a one off event and sentiment tracking is not at all useful Approach and Methodology Ensuring that sentiment is correctly captured is critical to enable downstream correlation and prediction analysis The posts on the WSB forum have a unique syntax and vocabulary which makes interpretation of the call to action and intent of the comments challenging For this reason we hypothesize the following Hypothesis Automatic sentiment analysis packages will not work well on WSB sentiment This is supported by literature which finds that Valence Aware Dictionary and sEntiment Reasoner VADER performance comes up to accuracy even after adding WSB specific terms AlZaabi Another study found that agreement between VADER sentiment and triple human annotated sentiment was as low as Wang Luo Hence it is expected that using just VADER will be limited in predicting the sentiment of the comments and posts and we should explore alternative approaches We thus adopt a semi supervised learning approach by labelling a random subset of the data to verify the accuracy of sentiment analysis before applying our chosen model to the entire WSB corpus Besides model choice Wang Luo find that that combining the output of various models BERT VADER can improve performance on the end task predicting direction of change in stock price We hence test all these approaches in isolation before combining features to achieve the best accuracy outcome Hypothesis Positive WSB sentiment helps to better predict stock price performance and volatility than overall comment volume WSB has an abundance of so called trolls who post meaningless spam Hence we hypothesize that filtering out neutral or noise comments and focusing on positive sentiment signals in terms of positive comment volume or ratio of positive comments to total comments may be more useful in prediction compared to total comment volume To do this we build a LSTM and establish a baseline accuracy for stock price prediction which includes total comment volume Sentiment features are then added and incremental accuracy improvement is measured Hypothesis The predictive power of sentiment on WSB is stronger for stocks of lower market capitalization The narrative of WSB has been one of retail investors taking on traditional institutional investors In line with this forums frequently encourage users to band together to influence the price of a single stock We thus hypothesize that it will be easier for users to influence prices of a stock with lower market capitalization compared to large names  Does Sentiment on R WallStreetBets Help to Predict Stock Price rendering WSB sentiment useful only if the company is relatively small in terms of market capitalization In line with Hypothesis we have chosen tickers with relatively active WSB discussions and categorized them into categories of mega large medium and small market capitalizations for the purpose of hypothesis testing Table Chosen stock tickers for analysis TICKER CAP SIZE MARKET CAP FEB COMMENTS AMZN Mega T TSLA Mega B META FB Mega B NOK Large B PLTR Large B AMC Mid B BB Mid B WISH Small B CLOV Small B Adjustments to Initial Proposal Three key adjustments are made to our initial proposal Firstly instead of multiclass sentiment positive negative neutral the classification task was changed into a binary one as it was empirically found that negative comments were relatively sparse in the comments estimated to be of the dataset WSB by nature is a community where short sellers or negative sentiment is generally frowned upon unlike other investment communities where balanced advice is preferred We hence aim to ensure the classifier can distinguish positive sentiment from noise consisting of both neutral and negative labelled comments Secondly while we had hypothesized that WSB sentiment s predictive power was stronger during meme periods we have found these periods to be too sparse of trading days per stock i e days for meaningful conclusions and remove this hypothesis Finally we have limited our dependent variable to the daily closing stock price of the chosen tickers Initially it was proposed to predict standard deviation and direction of change of the stock However our chosen approach LSTM is inherently not meant to predict these metrics Furthermore direction of change is redundant if we can reasonably predict stock price itself Overall we find that purely focusing on stock price prediction is sufficient for us to measure usefulness and incrementality of sentiment Note that capitalization size is defined according to generally accepted investment community thresholds i e BN for small cap USD BN Data Collection WallStreetBets Dataset Reddit comment and post data is accessed through Pushshift an open source API for searching through historical Reddit data It should be noted that Pushshift is different from the official Reddit API which only provides real time data Pushshift data is scraped from Reddit at a point in time and contains noise like deleted comments and posts that are filtered out before analysis As we are only interested in sentiment relating to the identified tickers we first filter for mentions of the tickers in the Reddit post header over the entire From this we identify posts in where said tickers are quoted and discussed We then pull all comments associated to these posts consisting of comments retrieved Deleted and duplicated comments are then filtered out resulting in unique comments that are used for sentiment analysis Yahoo Finance Stock Price Dataset Stock price metrics were derived through the yfinance library which scrapes data from Yahoo Finance through publicly available APIs Given the previously defined scope we have trading days worth of data for the stocks and S P Sentiment Analysis Pre processing DATA LABELLING We first sample comments from posts from the WSB dataset This is split into batches of comments per human labeler and each comment is manually labelled with positive Class or neutral negative Class sentiment To control for subjectivity each comment is double labelled disputes are discussed and resolved with input from the team of Overall this provides us with a robust labelled dataset of comments positive neutral negative that are used for training and performance evaluation It should be noted that this sample size is significantly more than comparable studies on WSB AlZaabi samples Wang Luo samples Alverez et al DATA CLEANING FOR STATISTICAL BASED MODELS Prior to passing data into classification models that require text encoding the following preprocessing steps are taken for mid cap USD BN for large cap and mega cap when capitalization exceeds USD BN  Does Sentiment on R WallStreetBets Help to Predict Stock Price Lowercase of all text Remove any hyperlinks numbers and punctuation Replace emojis with text description from the demoji package Remove default stop words and several non informative Reddit specific words DATA CLEANING FOR NLP BASED MODELS For pre trained natural language processing models like Valence Aware Dictionary and sEntiment Reasoner VADER and Bidirectional Encoder Representations from Transformers BERT minimal pre processing was done as we found the accuracy performance on our labelled dataset to be maximized when pre processing was minimized These models are trained on a large corpus and research Alzahrani Johnson Fernandez et al shows that standard pre processing applied for statistical based models could be unnecessary and may even negatively impact the performance of the models Furthermore these models often have a built in method for handling emojis Hence the only pre processing steps taken are to remove any hyperlinks and numbers Exploratory Data Analysis From an inspection of the WordCloud for positive comments Figure versus neutral comments Figure Examples include reddit post comment which occur with comparable frequency between positive and neutral sentiment visible differences for the top terms emerge For example emoji related terms like the rocket gorilla moon feature highly Terms relating to potential actions that users may take for example squeeze buying and hold are also noticeable for positive comments For neutral comments Figure a large proportion of the terms are related to the tickers of the stocks themselves This may be because many of these comments are speculative factual or spam in nature with no clear price support intent in terms of buying or holding Examples of neutral comments include asking for others opinion on certain stocks speculative users sharing that said stocks hit various price thresholds factual or users declaring that they will take certain nonsensical actions if share price for said stock hits a certain threshold spam Overall this suggests that focusing on a certain number of important terms will be effective in distinguishing positive versus neutral sentiment Statistical based Machine Learning Models Post pre processing the text corpus is fed to TF IDF vectorizer The number of terms with highest TF IDF and n gram range are hyperparameters which were tuned It was found that top terms was optimal as balanced accuracy dropped when the number was further reduced In contrast balanced accuracy did not drop significantly as the number of terms were decreased from approximately terms On the other hand changing the n gram range did not impact balanced accuracy With inspection of the top terms it was found that changing the n gram range from between gram and gram to between gram and gram was not having a significant impact on the top terms selected since the top terms are more likely to be grams and grams The top terms consisting of gram and grams were filtered to form the TF IDF matrix for model training Table Model The embedding so obtained contained many features Dimensionality reduction was performed by both Principal Component Analysis PCA and Uniformed Manifold Approximation Projection UMAP to compress the information in the embedding into a smaller more manageable number of vector components Nevertheless both PCA Table Model and UMAP Table Model performed significantly worse than the baseline TF IDF model with top terms For PCA reducing to dimensions covered only of total model variation which meant that PCA was not able to effectively reduce the number of dimensions As seen in Figure WordCloud of Positive Comments Figure WordCloud of Neutral Comments Figure WordCloud of Positive Comments Figure WordCloud of Positive Comments  Does Sentiment on R WallStreetBets Help to Predict Stock Price Appendix UMAP also yielded no interesting segregation between the classes Pre trained Natural Language Processing Models VADER VADER Hutto Gilbert is rule based model designed specifically for sentiment analysis and trained on social media corpus like Twitter One of the key outputs of VADER is a dictionary that matches words to a specific sentiment ranging from to In prediction VADER then uses this dictionary to identify the sentiment of the sentence VADER also takes into account punctuations letter case adverb modifiers like very and inflection words like but that may indicate differing levels of sentiment over and above the sentiment of the word itself Given that Twitter and Reddit are both social media platforms it can be expected that the overall grammar and syntax of both platforms may be similar which rather than standardized English may include broken grammar spelling mistakes and wide range of symbols translated as emoticons Finally VADER outputs a positive neutral negative and compounded final score to indicate the sentiment of the text Table Model Still enhancements can be made when WSB specific terminology and their sentiment is taken into consideration Table Model Words like apes rocket moon which in real world context usually have neutral sentiment indicate a very strong positive sentiment in a WSB context Even generally negative words like retard have a positive sentiment given the high degree of self depreciation noted by Bolyston et al on the WSB community and is often used as term of familiarity within the community Therefore these differences in sentiment were considered and the sentiment scores are updated into the dictionary for sentiment prediction Nevertheless VADER this still has relatively low performance not exceeding more than precision overall It is hypothesized that there is still a wide range of nuances not captured in the enhanced sentiment dictionary Hence as shown in Table further tuning was done on the cut off for the point at which the compound final score from VADER was indicating a positive sentiment A cut off of was chosen it balanced both Precision F score and Balanced Accuracy Table Model In particular F was used as a balancing factor as precision continually improves as the cut off point increases able to catch all true positives BERT BERT Devlin et al is a transformer based deep learning pre trained model with almost million parameters trained mainly on Wikipedia corpus by Google While the grammar may be more formal the key strength of BERT lies in the ability to deal with a wide ranging of tasks with more sophisticated word embeddings Table Model Nevertheless a limitation of the dataset is the presence of Out of Vocabulary OOV words like stock tickers While BERT breaks down OOV words into smaller tokens for training given the limited amount of training data k rows this could impact the results Hence the removal of stock tickers was tested and Model in Table shows improved performance Evaluation of Classification models For evaluating model performance Precision and Balanced Accuracy were used as the evaluation metrics Precision was chosen as the cost of false negative is lower than false positives False positives are expensive because they may directly lead to overly optimistic assessments of sentiment and ill advised decisions to buy into the stock Balanced Accuracy was reported as well for completeness Overall the classification model which performed the best is the Extra Trees Classifier model which utilized a combination of the TF IDF vectorizer BERT and VADER outputs The precision score was which was a slight improvement over the next best performance which was the BERT model with ticker removed Table Model Table Classification accuracy on test set comments of predicted sentiment vs labelled sentiment DESCRIPTION PRECISION BALANCED ACCURACY TF IDF LGBM TF IDF LGBM PCA TF IDF KNN UMAP VADER BASELINE VADER WSB VOCAB VADER SENTIMENT PRECISION TUNED BERT BASELINE BERT TICKER REMOVED TF IDF BERT VADER OUTPUT EXTRA TREES These results match up well to benchmarks in existing literature that have used similar human labelled data For VADAR performance maximum plain accuracy is around AlZaabi versus the balanced accuracy we have achieved For BERT performance precision for the majority class is found to be Alvarex et al remarkably similar to the precision for the positive class which we have achieved here This gives us confidence that each approach we have tried is generally performing to the maximum it can be expected to achieve Hence we choose to proceed with Model based on precision as the cost of false negative is much lower than false positives as discussed above The best performing model was then utilized to label the rest of the dataset which had not been labelled by a human labeller to  Does Sentiment on R WallStreetBets Help to Predict Stock Price generate the sentiment related features discussed later in the report Stock Price Prediction Regression Data Setup Features used for regression are set up on a daily basis for the entire trading days The dependent variable is the Daily Close Price Close of the stock Features used for prediction are discussed below SENTIMENT FEATURES We engineer the daily count of positive and neutral comments as well as the percentage of positive comments in the following way With sentiment predicted for all comments in our dataset we assign each comment to a stock ticker using the following attribution logic Count mentions of our specific tickers within a certain comment e g bb or blackberry The comment would be assigned to the ticker with the highest count of mentions in the comment Count mentions of our specific tickers within the post the comment is reacting to In the situation the comment does not mention a stock count for all tickers is in the first step it will be assigned to most mentioned ticker in the post the comment is reacting to This approach to search for short form tickers to attribute comments is similar to that taken by Buz de Melo and found to be more comprehensive than other methods of filtering such as only looking for abbreviations that only contain a preceding sign Following this positive and neutral comments are aggregated by day This gives us sentiment features positive comment count neutral comment count and percentage positive comments computed as positive comments divided by total comments Moving averages moving sums and percentage change of these three variables are then created from up to days to provide a smoothened out time series This gives us a total of sentiment features per stock While these are highly correlated our forecasting approach LSTM can handle multicollinearity BASELINE FEATURES Baseline features were created in the following categories Basic Stock Price Information features open high low prices volume traded for that day and intra day Each feature adds new variables from taking the feature itself variable adding on moving computations for time ranges day to day new variables standard deviation SD log of standard deviation SD log and the lagged variable of close price itself Change Information features In line with the literature Asness et al two momentum indicators were generated for the stock price dataset Percentage change Change and direction of change Dir binary variable representing whether the stock increased or decreased is engineered Both are calculated over preceding days Comment volume features As we want to measure the incremental impact of adding sentiment total comment count and respective moving sums averages and percentage changes are added as this information is available irrespective of sentiment analysis An additional binary variable is meme is also engineered to capture accelerating interest in the stock This is calculated as comment volume exceeding more than two times the day moving average and in the th percentile of daily comments so far Others features Day of week is added as a categorical variable Note that while there are only trading days social features such as sentiment and comment volume discussed below is available days a year To address this when applying time lags section social data is shifted according to calendar date while financial data is shifted according to trading day For example if time shift is day Sunday s sentiment will be used to predict Monday s close price In contrast as there is no financial data for Sunday Friday s financial data will be used to predict Monday s close price Exploratory Data Analysis Figure PLTR close price vs positive comments over  Does Sentiment on R WallStreetBets Help to Predict Stock Price Visual inspection Figure shows that for certain stocks e g PLTR there is some evidence that spikes in positive comments are closely followed by increases in price However the relationship is not straightforward across all stocks and will be demonstrated more concretely through improvement in prediction performance Regression Approach MODEL CHOICE LONG SHORT TERM MEMORY The Long Short Term Memory LSTM recurrent neural network model is commonly used to predict future stock market values in academia Adil Moghar as financial forecasting is regarded as a time series Other advantages of the LSTM model include insensitivity to gap lengths as the gates can retain relevant information over a longer period of time insensitivity to multicollinearity and partial effectiveness against the vanishing gradient problem with direct access to the forget gate activations Since we are less interested in the prediction actual stock prices and more focused on testing the effect of sentiment on stock price prediction performance particularly if the stock have lower market capitalization we have run separate prediction models one with sentiment data and the other without ceteris paribus Any difference in prediction performance can then be attributed to the presence or absence of sentiments Like most neural networks LSTM works best with data between hence min max scaling was applied It is believed normalizing will improve the efficiency and accuracy of training since LSTMs use small weight initialization and unscaled data may result in the model learning large weight values causing instability and higher generalization error Hence we compared models with only min max scaling applied as well as models with an additional standard scaler applied to evaluate if normalization improves predictive performance Subsequently we observed that normalization did not improve performance across the board contrary to common practice Stottner It is likely that normalizing after applying min max diminished its intended effectiveness since all features are kept within a smaller and only positive range making it harder for the nodes in the layers to learn Furthermore for the retail investor to be able to make beneficial decisions there needs to be larger variances detected in the trend to profit from which the normalization would inadvertently have smoothened Hence our final model did not include any normalization of data Our network consists of four layers the first is an LSTM layer which takes each mini batch as input Each mini batch has steps and variables creating neurons The second LSTM layer s input is the sequence from the previous layer and returns values the third layer is dense with neurons and the final dense layer outputs the predicted dependent variable Different activation functions were attempted tanh sigmoid ReLu and softmax with the lowest validation loss observed with tanh and sigmoid Since the sigmoid function is slightly more prone to the vanishing gradient problem we went with tanh which is less prone as values are centered at zero In modelling we have employed early stopping to prevent overfitting on the validation set to allow for optimal generalization performance Patience is set as since it is observed most models stop training when epochs A lower patience would mean earlier stopping resulting in an insufficiently trained model a higher patience may result in overfitting as the training and validation loss plots occasionally show spikes from one epoch to next as seen in Figure PREDICTION HORIZON LAG PERIODS As we are investigating the effect of sentiment of stock performance we expect that sentiment changes quickly in a short period of time Hence each mini batch will consist of sequences days or week where the sequence is the test set A rolling window approach is used where if the sequence length is then the input of days inclusive will be used to generate a prediction for day Figure shows the data structure just described Figure dimensional data structure Furthermore to test robustness we train the LSTM at lags of and days t t t to observe if sentiment is accretive to prediction outcomes t across various advance time horizons Figure Training and validation loss values for CLOV at time shift t to predict t Data not normalized and excluding sentiments  Does Sentiment on R WallStreetBets Help to Predict Stock Price Results Interpretation EVALUATION METRICS Mean Absolute Percentage Error MAPE and Median Absolute Percentage Error MDAPE are suitable metrics for evaluation of the regression results Being expressed as a percentage these two metrics can be used for comparison of the regression results between different stocks as is necessary for testing Hypothesis As MAPE relies on mean instead of median in the calculation MAPE would be more affected by outliers compared to MDAPE Considering that the prediction is potentially used to make stock trading decisions we consider MAPE to be the more appropriate metric because prediction outliers can potentially cause huge losses and should be considered as part of the evaluation metric GENERAL INTERPRETATION Table below summarizes the full results for models with different lag periods tested and days by stock Table Full MAPE Results for different lag periods TICKER SCENARIO DAY DAYS DAYS AMZN Baseline Sentiment Improvement TSLA Baseline Sentiment Improvement FB Baseline Sentiment Improvement NOK Baseline Sentiment Improvement PLTR Baseline Sentiment Improvement AMC Baseline Sentiment Improvement BB Baseline Sentiment Improvement WISH Baseline Sentiment Improvement CLOV Baseline Sentiment Improvement Firstly this shows that the LSTM MAPE is generally low and robust for mega cap large cap and mid cap stocks all of which have baseline MAPE below across all time periods MAPE is generally higher for small cap stocks WISH CLOV because small cap stock price changes tend to be exceedingly volatile Secondly it shows that baseline LSTM MAPE generally increases as prediction horizon increases with the Day time shift generally having the lowest MAPE This is to be expected as recent price should generally have the least fluctuation from current price to be predicted This applies to all stocks with the exception of WISH and BB for which day baseline MAPE is slightly lower than day IMPACT OF SENTIMENT Table further summarizes the results of the regression performance in Table MAPE for out of stocks showed an improvement after addition of sentiment features AMZN and NOK did not show an improvement in MAPE with the addition of sentiment features Furthermore sentiment features definitively improved the prediction results for stocks namely PLTR WISH and CLOV as results improved over all lag periods However sentiment is not always informative as out of stocks AMZN and NOK showed no improvement on MAPE across all time periods Table Summary of models with best MAPE improvement TICKER BEST LAG BASELINE WITH SENTIMENT CHANGE IN MAPE AMZN day TSLA day FB day NOK day PLTR days AMC day BB days WISH days CLOV days As for the impact of market capitalization in line with Hypothesis the two stocks categorized as small capitalization stocks WISH and CLOV showed the greatest and most consistent improvement overall with MAPE reducing by and respectively This improvement in the forecasting of CLOV s stock price is displayed in Appendix Furthermore for mega cap stocks AMZN TSLA FB sentiment was usually not informative and improvements if any were small which makes sense as rallying sufficient retail investors to influence the price of such large companies is a difficult task The relationship with capitalization is not straightforward for large and mid cap stocks We would generally expect to find low to no impact for these categories but sentiment  Does Sentiment on R WallStreetBets Help to Predict Stock Price has still proven useful especially for PLTR BN capitalisation AMC BN and BB BN However price of other large cap stocks namely NOK BN did not show similar effects This could be due to NOK s larger market capitalisation or a lower overall comment volume K for NOK vs K for PLTR K for AMC and K for BB With the limited sample size of stocks we have here it is not possible to isolate the specific driver Finally we find that lag period is important in that MAPE improvement for sentiment does not always appear strongly in all time periods While a day time shift was the best for the majority of stocks the best improvement appeared at a lag period as late as days for stocks PLTR and CLOV If lag period other than the one stated in Table was chosen sentiment may have a negative impact on the prediction performance Overall this suggests that sentiment information may take some time to affect stock price and that looking at longer time lags to days may be beneficial in some cases Conclusion Summary of Results by Hypothesis Through the study we have made significant contribution towards the key hypotheses in Section Hypothesis Efficacy of sentiment analysis models on WSB data Our results for using a pure sentiment model was VADER relatively poorly even with fine tuning to include WSB specific jargon This is similar and to be expected from existing literature However when combining TF IDF features VADER output and BERT output and applying a final classifier we can still improve the accuracy of the prediction of double blind labelled sentiments significantly to achieve an excellent precision result of Hypothesis Impact of using sentiment to predict stock prices Our LSTM model shows that sentiment in general can help in the prediction of short term stock prices versus a baseline model that considers mainly financial indicators using information available for the last days For PLTR WISH and CLOV in particular WSB sentiment provided useful information that can be used to predict the stock s close price in the following period This could be because these stocks investors were influenced by WSB comments or the investors were making positive comments on WSB prior to their purchase decisions Alternatively it could also be due to WSB comments being representative of general investors thoughts For the other stocks comments on WSB were not definitively useful This may be due to various reasons such as commenters not taking investment actions based on their comments insufficient total support from the WSB community and retail investors or insufficient attention in terms of limited comment volume from the WSB community Hypothesis Impact of using sentiment based on market capitalization Sentiment is particularly useful when applied to smaller market capitalization stocks like WISH and CLOV Mega cap stocks show limited improvement with inclusion of sentiment while results are mixed for large to mid cap stocks However results are only for two small stocks and future testing should focus on verifying this conclusion over a wider pool of stocks Overall this study suggests that investors should not ignore the impact of sentiment and that tracking it can be useful in some cases Application to real world problems Sentiment information is becoming increasingly ubiquitous and is a rich and often free data source Hence it is important to understand the extent to which such information can be leveraged to better predict future outcomes While the causal mechanisms are not always clear this does not detract from the potential usefulness of sentiment as an input to various types of price and demand forecasting models Our project can be applied to any context where the opinions of a large group of participants is an important input For example besides stocks it can apply to speculative assets like cryptocurrencies or even stable assets like property prices as price fluctuations in these assets are largely influenced by the broader market rather than institutions While the financially literate advise against emotional investing the converse is true where it is possible to profit off the poor investment decisions driven by emotions of the other parties in the market Top performing funds outperform because they can exploit sentiment changes and profit from arbitrage Yong Chen Hence it is important to understand common sentiment not necessarily to join the rally but rather to stay ahead of it One of the difficulties of extracting sentiment is not just the sheer volume of a large corpus but also when said corpus is highly specialized as is the case for WSB The approach that had been adopted in our study for sentiment analysis could be adopted where sentiments are exchanged on a platform which has its unique syntax e g EDMW HardwareZone Chan Hive The combination of pre trained models such as VADER and BERT with statistical methods such as TF IDF vectorizer resulted in a good performance which allowed the other prediction tasks to be executed Sentiment analysis on a time series can also be used in non financial applications such as political elections or in forecasting demand for a product to then conduct the necessary supply planning For instance understanding sentiment towards Covid as the pandemic evolved to a  Does Sentiment on R WallStreetBets Help to Predict Stock Price pandemic over time can help healthcare systems and pharmaceuticals forecast the demand for vaccines and make the appropriate logistical arrangements to ensure supplies meet demand In the case of other consumer goods this forecast can also be used to make pricing decisions and drive margin growth Limitations and Improvements There are key limitations and potential for future improvements to be highlighted Although the amount of manually labelled data is higher compared to studies sampled in academia it is still a small portion over the entire size of data Given the wide range of emotions conveyed the distribution in the sample data may still be different from the entire population In particular only of the larger data set was noted to have a positive sentiment compared to in the labelled data set One way to address this could be to use the labelled data use clustering methodologies to identify neighbours to expand the labelled data set before training the sentiment prediction models on this new labelled dataset A more reliable method would be to simply have more labelled data which would take more time As noted in Section only K comments were used About K comments were deleted before they could be captured by PushShift API for archiving returning only a removed description upon retrieval These removed comments may have been removed because they were more controversial or violated community guidelines However they could also have contributed to the sentiment of the community at the point in time and therefore limit the study from capturing the true sentiment at the point in time While this cannot be addressed retrospectively any further study to examine the correlation between WSB community sentiment should actively scrape live data from the official Reddit API over a period time to ensure completeness of information Finally it should be noted that was a bull market Rosenbaum Forsyth with exceptional returns This could have also better supported any positive sentiment and speculative trading The correlations observed in the study may be weaker in bear markets Studying the trend over a longer time period is needed to established a stronger relationship between WSB sentiment and stock prices and may well help to establish if there is a meme effect for stock prices in specific periods where there is significant hype over the stock Supporting Code Supporting code for this project may be found at https github com rachelsng WallStreetBets Sentiment Appendix Appendix UMAP Visualisation Appendix CLOV Price Prediction Figure UMAP Mapping Components Figure UMAP Mapping Min Distance Figure UMAP Mapping N Neighbours Figure CLOV close price prediction WITH sentiment features  Does Sentiment on R WallStreetBets Help to Predict Stock Price References Adil Moghar M H Stock Market Prediction Using LSTM Recurrent Neural Network Procedia Computer Science Volume Retrieved from https doi org j procs Alvarez R Bhatt P Zhao X Rios A Turning Stocks into Memes A Dataset for Understanding How Social Communities Can Drive Wall Street arXiv preprint arXiv AlZaabi S A Correlating Sentiment in Reddit s Wallstreetbets with the Stock Market Using Machine Learning Techniques Alzahrani E Jololian L How Different Text preprocessing Techniques Using The BERT Model Affect The Gender Profiling of Authors arXiv preprint arXiv Boylston C Palacios B Tassev P Bruckman A WallStreetBets Positions or Ban arXiv preprint arXiv Buz T de Melo G Should You Take Investment Advice From WallStreetBets A Data Driven Approach arXiv preprint arXiv Devlin J Chang M W Lee K Toutanova K Bert Pre training of deep bidirectional transformers for language understanding arXiv preprint arXiv Fern ndez Mart nez F Luna Jim nez C Kleinlein R Griol D Callejas Z Montero J M Fine Tuning BERT Models for Intent Recognition Using a Frequency Cut Off Strategy for Domain Specific Vocabulary Extension Applied Sciences Forsyth R W December What will it take to kill this bull market we ll find out soon What Will It Take to Kill This Bull Market We ll Find Out Next Year Barron s Retrieved April from https www barrons com articles bull stock market rally GitHub pushshift api Pushshift API GitHub Retrieved April from https github com pushshift api Hutto C Gilbert E May Vader A parsimonious rule based model for sentiment analysis of social media text In Proceedings of the international AAAI conference on web and social media Vol No pp Long C Lucey B M Yarovaya L I Just Like the Stock versus Fear and Loathing on Main Street The Role of Reddit Sentiment in the GameStop Short Squeeze SSRN Electronic Journal Rosenbaum E December The Bull Market s biggest hopes for are in the portfolios of wealthy young investors CNBC Retrieved April from https www cnbc com bull markets biggest hopes for rest with millennial millionaires html Stottner T May Why data should be normalized before training a neural network Medium Retrieved April from https towardsdatascience com why data should be normalized before training a neural network c b f c d Wang C Luo B Predicting GME Stock Price Movement Using Sentiment from Reddit r wallstreetbets In Proceedings of the Third Workshop on Financial Technology and Natural Language Processing pp Yong Chen B H August Sentiment Trading and Hedge Fund Returns The Journal of Finance doi https doi org jofi Figure CLOV close price prediction WITHOUT sentiment features
Detection of Real Disasters from Tweets Group Ankit Malhotra A X Cristian Bojaca A L Liu Yishun A X Ma Yuankai A W Yang Yuchen A N GitHub link https github com RobinNeverBow BT Group Project Abstract Twitter has become an important communication channel in times of emergency The ubiquitousness of smartphones enables people to announce an emergency they are observing in real time Because of this more agencies are interested in programmatically monitoring Twitter However it is not always clear whether a person s words are actually announcing a disaster In this project we aimed to implement various natural language processing NLP techniques to classify the tweets from the users on disasters into real or non real Utilizing the balanced data set after the data preprocessing procedure we experimented with BOW models BERT models Text CNN and GNN models and evaluated their model performance based on accuracy F score and ROC AUC scores Finally we suggested the potential application of the models for emergency responses Introduction Background Nowadays social media offers a realm of information on varied topics ranging from news politics entertainment healthcare to recent trends emotions and opinions of people worldwide With the omnipresence of smartphones and their easy accessibility by people of nearly all age groups it becomes considerably easy to disseminate ideas viewpoints sentiments and different schools of thought with just a mere few clicks This level of convenience in reaching out to numerous people through online channels becomes particularly essential in crises such as natural disasters In such scenarios social media platforms can play a pivotal role in providing critical information about the mishappening including the type of disaster its intensity and precise location of occurrence the problems people face and people s emotions and reactions to name a few While there are different types of social networking platforms available at our disposal Twitter has turned out to be probably one of the best mediums to find out real time information on what s happening around Different stakeholders involving national disaster relief organizations governments media volunteers public etc can use Twitter to collaborate swiftly and effectively However the quality of information being posted by the users often lacks authenticity which can lead to miscommunication and unbefitting actions by the relevant parties concerned This motivated us to address this issue by filtering out the tweets which pertains to a real disaster based on natural language processing NLP techniques and machine learning ML algorithms In the forthcoming sections we will define the problem statement dataset and the models that we implement to solve this problem Problem Statement Our primary objective in this project is to classify the tweets from the users on disasters into real or non real To clarify for instance a user tweets some information relating to a scenic view and the tweet contains the keyword apocalypse Here the word does not correspond to a real disaster but is just used metaphorically Thus it will not be classified as a disaster and will accordingly be assigned a value of Therefore the objective of the study is to build a classification model to accurately identify tweets related to real disasters Deployment of such a model would enable governments or other relevant agencies to monitor information on Twitter more efficiently in order to rapidly respond to the emergencies Dataset Description The main source of information is Kaggle s dataset on Natural Language Processing with Disaster Tweets Kaggle This dataset was originally created by the company Figure Eight now known as Appen Appen It provides an adequate amount of data to deploy NLP models tweets which are ML techniques trained from unstructured specifically text data The dataset obtained from Kaggle contains the following columns a text Contains the raw text extracted from the tweet to be further analyzed b location Establishes the location at which the user was while sending the tweet   c keyword Provides the tweet s most relevant word however does not establish the criteria used to obtain this information d target Whether a given tweet is about a real disaster or not if yes and if no For our study we will mainly focus on the text feature in the dataset as the keyword information are contained in the text and the locations are not the emphasis of this NLP study Exploratory Data Analysis To verify the class imbalance of the target variable we examine the label distribution of the target column and the results are shown in Figure Figure Class distribution of the dataset From the distribution we can see that the split between the two labels was around to thus we would consider it as a balanced data set and no data sampling techniques are required before the modelling The distribution of the word counts per tweet is shown in Figure It can be observed that the word counts of real and non real disaster tweets are of a similar distribution The median number of words is around and the maximum number of words are around The real disaster tweets have less occurrences of length shorter than This observation could indicate that tweets of very short length thus having limited information are likely to be non real disaster tweets Figure Distribution of word counts in tweets We conduct a Part of Speech POS tagging to examine the grammatical composition of the tweets Syntactic components are tagged using the spaCy pipline spaCy including the verbs adjectives nouns and proper nouns The counts of the various components are shown in Figure It can be observed that real and non real disaster tweets have similar number of verbs and adjectives However interestingly the real disaster tweets have on average slightly more nouns and proper nouns It is another indication that real disaster tweets tend to have more information Figure Distribution of the counts of grammatical components To have a rough understanding on the differences between real and non real tweets in terms of the contents we take a look at the word clouds as shown in Figure and Figure We find that some frequent words in real disaster tweets such as fire storm and Hiroshima are able to pinpoint the type or location of the disasters and indicate that its content might be relevant to real disasters Meanwhile the frequent words in non real disaster tweets such as new and time are not referring to disasters Therefore the two types of tweets differ from each other in the lexicons used We would expect a reasonable classification accuracy even with simple bag of word models   Figure Word cloud of real disaster tweets Figure Word count of non real disaster tweets Data Pre processing Data Cleaning This part is the preliminary data cleaning for Tweets content and the operations to be carried out included Remove URLs special characters digits underline and white spaces Make text lowercase Remove stopwords Correct the typos Remove the single letters Word Lemmatization Lemmatization here refers to with the use of a vocabulary and morphological analysis of words remove inflectional endings only and to return the base and dictionary form of a word The WordNetLemmatizer in nltk library is utilized Modeling Bag of Word BOW Models MOTIVATION Given the problem statement it is not always clear whether the individual announcing the disaster through a mere tweet is real or not The tweets contain a mix of spam and non spam content the automated filtering of which makes it an important application The kind of application mentioned above urged us to conduct a binary text classification problem i e there would be two outcomes of an event starting with simple bag of word models as the baseline METHODOLOGY The data pre processing step generated the cleaned version of the text taking into consideration removal of stopwords digits special characters urls etc Post this we proceeded to convert the cleaned text into vector form through bag of word BoW representations more specifically by the term frequency inverse document frequency TF IDF Ultraviolet Analytics which represents the score of the words in each tweet The vector form of the text thus renders it suitable for further analysis and machine learning modeling The TF IDF sparse vectors are used as the independent variables in the classification models The candidate models chosen to be implemented were as follows Logistic Regression LR Multinomial Naive Bayes NB Random Forest RF Support Vector Classifier SVC Light Gradient Boosting Machine LGBM We selected three evaluation methods for each of the models used The metrics were accuracy ROC AUC score and F score ROC AUC score provides the tradeoff between true positive rate and false positive rate which would be relevant to the given problem as we are interested in finding how many real disaster tweets were actually classified as disasters by the prediction models Additionally F score which is based on the combination of two metrics namely precision and recall is particularly well suited for the binary classification The detailed rationale on the choice of metrics will be discussed in Section We used these metrics to compare the optimal performance of the machine learning classification models with one another RESULTS A comparative analysis of the accuracy AUC score and F score across five models used for text classification has been shown in Table below   Table Test Accuracy AUC F Score of ML classification models MODEL ROC AUC ACCURA CY F BEST LR x NB x RF x SVC LGBM x For the given binary classification problem all the models have a fair performance however Logistic Regression Naive Bayes and SVC models perform relatively better in terms of accuracy AUC score and F score Ensemble models such as Random Forest and LightGBM have a slightly lower accuracy after tuning than the LR NB and SVC models This could be due to that the feature space is approximately linearly separable and thus the non linear models do not offer a performance edge Overall the SVC performs the best and its confusion matrix is shown in Table Table Confusion matrix of SVC model PREDICT PREDICT TRUE TRUE To understand which words in the corpus contribute the most to the classification we plot the feature importance of the top words from the RF results as shown in Figure We observe that words indicating the type of disasters are important such as fire bombing and flood It is consistent with our observations in the EDA that the real disaster tweets can provide concrete information on the type of a disaster We also note that the two location words Hiroshima and California are important features That may be due to the two large disasters that are frequently mentioned in the tweets the Hiroshima earthquake and the California wildfire Figure Feature importance of RF model STRENGTHS AND LIMITATIONS We obtained a baseline accuracy of over for three classification models namely LR SVC and NB laying a solid foundation for this problem While these models give a fairly decent accuracy they are based on the BoW assumption that words are independent and no sequence information is capture Therefore we could strive to improve the prediction performance by using some state of the art text classification techniques namely BERT TextCNN and GNN models which will be discussed in the forthcoming sections BERT Ensemble models MODEL DESCRIPTION The Bidirectional Encoder Representations from Transformers BERT model generates contextually based embeddings using bidirectional encoders from the transformer s NN architecture Khalid BERT models are particularly suitable for our disaster tweets classification as it is able to capture the multiple sense of a word For example the word fire has different semantic meanings in a real disaster tweet Fire in Jurong East and a non disaster tweet I couldn t fire up my car A static embedding will not differentiate the two interpretations Thus we would like to experiment and evaluate the performance of BERT embedding Another critical feature of why BERT outperforms other NLP techniques such as Bag of Words BoW is that it is pre trained using a large corpus As a second step it can be fine tuned to a specific dataset and a specific task such as classification or text creation MODEL PRETRAINING   The model performs masking language model MLM and next sentence prediction NSP to understand the context of the text Masking is a technique that trains the model to guess the right word in the blank and the next sentence prediction technique tries to teach the model to recognize the context of the text and think about what sentence makes sense next The masking technique trains the model to use a bidirectional context based approach as opposed to other neural networks architectures that are unidirectional or traditional NLP techniques BoW that are context free MODEL DEPLOYMENT The models input is the text tokenized after adding particular token embeddings such as CLS a unique embedding for classification tasks and SEP that helps understand the model at the end of each sentence Our team explores the BERT model as a preprocessing step to combine it with some ensemble models such as Light GBM Gradient boosting and Random forest We use just CLS embeddings as an input for the models but we set the max sentence length to words due to computational limitations RANDOMIZED SEARCH CROSS VALIDATION For the hyper parameter tuning we used Randomized Search CV to find the best possible model in contrast to grid search this approach improves the efficiency by training just a sample of the possible combinations of the hyper parameters Table Best hyper parameters by model MODEL L RATE MAX DEPTH ESTIMATO RS BEST RF NA LGBM x GB x BERT combined with ensemble models improved some of the metrics such as AUC but the results were similar to the traditional models for some others such as accuracy and F score Table Test Accuracy AUC F Score of BERT classification models MODEL ROC AUC ACCURA CY F BEST RF LGBM x GB x Among the models explored the random forest has the highest performance in all the metrics The confusion matrix of the RF model is shown in Table where we can observe other metrics such as sensitivity and recall in more details Table Confusion matrix of BERT RF model PREDICT PREDICT TRUE TRUE BERT Text CNN TEXT CONVOLUTIONAL NEURAL NETWORK CNN Text can be seen as a one dimensional image so that we can use one dimensional convolutional neural networks to capture associations between adjacent words One of the important parts of Text CNN is the one dimensional convolutional layer Like a two dimensional convolutional layer a one dimensional convolutional layer uses a one dimensional cross correlation operation In this operation the convolution window starts from the leftmost side of the input array and slides on the input array from left to right successively When the convolution window slides to a certain position the input subarray in the window and kernel array are multiplied and summed by element to get the element at the corresponding location in the output array as illustrated in Figure Figure Convolution of Text CNN Similarly we have a one dimensional pooling layer The max over time pooling layer used in Text CNN actually corresponds to a one dimensional global maximum   pooling layer Assuming that the input contains multiple channels and each channel consists of values on different time steps the output of each channel will be the largest value of all time steps in the channel Therefore the input of the max over time pooling layer can have different time steps on each channel TEXT CNN MODELING We implement the previously discussed Text CNN on our disaster tweet classification problem Before we feed the data to the text CNN model we use a BERT transformer to convert our data to a D tensor Different from Section here we will use all the BERT last layer hidden outputs of each word Due to the computational power limitation we have to limit the max sentence length in this case we choose the average sentence length which is We do padding for each text and feed it to the BERT transformer to get the output Still because of the memory size issue we cannot save all dimensions of each word So we choose the last dimensions and each word is converted to a dimension embedding vector Now we construct a text CNN model First connect the input to four D convolutional layers with filter size of with kernel size of and respectively Then connect all the output to max pooling layers and concatenate them together The output are then connected to a six layer fully connected neural network with drop out and batch normalization The structure of this model is shown in the Appendix TEXT CNN RESULTS At about epochs the train and test accuracies are stable The train test accuracy at every epoch is plotted in Figure Figure Train test accuracy of BERT Text CNN model We use the best Text CNN model to get the test performance which achieves an accuracy of ROC AUC score of and a f score of The confusion matrix is shown in Table Table Confusion matrix of BERT Text CNN model PREDICT PREDICT TRUE TRUE Graph Convolution Network GCN Graph neural network GNN has attracted increasing attention as a method of graph analysis in many domains such as social network knowledge graph etc GNN is able to capture the dependencies between graph nodes and also preserve the global structure information of a graph in the embeddings As the state of art GNN has been applied to text classification problems where the training corpus is used to build graph representations of vocabulary and documents In this study we explore this novel method for the binary disaster tweet classification problem Graph For our study we implemented the graph convolution network GCN text classification method proposed by Yao et al The graph is built using the entire corpus where the nodes are the unique words and the documents with the training documents labeled and test documents unlabelled as illustrated in Figure The document word edges represent the word occurrence in the documents and its weight is calculated using the term frequency inverse document frequency TF IDF of a word in a document These edges capture the semantics of the documents i e the tweets The word word edges represent the co occurrence of words where the weights are calculated by the pointwise mutual information PMI with a sliding window size Words that are highly correlated in semantics will have a higher weight on the edge between them The PMI value of a word pair i j is computed as   where W i j is the number of windows containing word i and word j W i is the number of windows containing word i W is the total number of windows in the entire corpus Figure Graph built on documents and vocabulary with document word edges red and word word edges black CONVOLUTION NEURAL NETWORK Unlike the pixels in image data graph nodes do not have a structured spatial relationship To perform convolutions on the graph we implement the two layer GCN proposed by Kipf and Welling which generates directly from the graph embedding vectors for the nodes based on their neighborhoods The embeddings are then fed into a softmax classifier for the text classification The embeddings output from the second layer of the GCN is where A is the adjacency matrix of the graph D is the degree matrix of A X is the diagonal square matrix of ones of the dimension of the number of nodes W and W are the weight matrix of the first and second layer respectively Yao et al suggest that two hidden layers GCN allows the information to be passed to nodes that are two steps away which enable the information exchange between documents even though there are no document document edges in the graph It is also reported that more layers beyond two does not improve the classification performance Therefore we set a two convolution layer structure with the layer size of and respectively GCN RESULTS The previously mentioned GCN is implemented for the disaster tweet classification in this study One important observation is that the graph size is huge if built on the entire dataset The number of edges is roughly of the order of doc vocab vocab Due to the memory constraint we use a random sample of data points from the dataset And of documents are used as the training nodes as the testing nodes We train the model for epochs and the train test accuracy is plotted below in Figure The best accuracy ROC AUC and F score of the model are and respectively The accuracy of the model is not as good as the other models we examined previously It could be due to the small fraction of the data we used for training The graph needs to be generated every time new test data are fed in for classification This process takes long and would result in high latency if deployed to real applications Together with the large memory requirement for the graph we may conclude that the GCN method is suitable for the text classification tasks where the available training data are limited If resources allow further experiments with the whole dataset shall be conducted to gauge its performance against other models Figure Cross entropy loss vs Epoch above Train Test accuracy vs Epoch below Model evaluation For this study we evaluate the model performance based on the accuracy ROC AUC score and F score Accuracy provides a simple and classic measurement of the binary classifiers overall performance However it is biased towards the majority class ROC AUC offers an evaluation of the model independent of the class distribution and is thus chosen as another metric   Normally the choice of emphasis on the model precision or recall will be based on the application and the tradeoff between Type I and Type II error cost For example in our case if the purpose of the application is to identify as many potential disasters as possible and the manual screening of false positive tweets cost little the model evaluation shall focus on the recall On the contrary if the model is deployed with minimum human screening it would be beneficial to have a high precision model to reduce the cost of the false alarms In this study as the model could be potentially applied to both scenarios we use F score the harmonic mean of precision and recall as the metric to achieve a balance The performance of the models examined are summarized in Table Table Summary of model performance MODEL ROC AUC ACCURACY F TF IDF LR TF IDF NB TF IDF RF TF IDF SVC TF IDF LGBM BERT RF BERT LGBM BERT GB BERT TEXT CNN GCN Built on an undersampled dataset Overall when we look at the prior independent metric ROC AUC the BERT Text CNN model performs best as we expected due to its contextual embedding However in terms of accuracy and F score the BOW models outperform BERT marginally That could potentially be due to the slight class imbalance in the dataset With adequate resources the BERT models could be further improved by fine tuning the parameters instead of using the fixed pre trained ones Additionally the performance may improve if we use the complete embedding vectors instead of the truncated ones Application The deployment of the model could potentially help emergency response agencies to monitor social media to identify disasters and collect anecdotal reports which could contain crucial insights for planning the response strategies The detected tweets are likely to offer temporal and spatial details about a tragedy and may be used to guide aids rescues and restorations Additionally the model might create business insights as well For example news companies could potentially utilize it to obtain first hand information about disasters to stay ahead of competitors Also the classification models that we explored can be combined with text generator models such as GPT to inform the people on social media without any human supervision increasing the efficiency of the communication industry Lastly since the police firefighters and government agencies have limited staff we can combine in a first step to identify real disaster information and then automatically perform sentiment analysis to allocate the human resources as efficiently as possible Conclusion We started defining a baseline performance using traditional machine learning models and BoW embeddings which achieves a decent performance In the second stage we explored the BERT model with contextual information that improved some of the metrics such as ROC AUC but the results are similar to the traditional models for some others such as accuracy Another novel technique we explored is Graph Convolution Network GNC This model also underperforms compared to the baseline Still it is worth noting that although the GNC and BERT models can capture more sophisticated relationships in the text we face computational limitations that explain the underperformance of the models The GNC use a small portion of the dataset for training and with the BERT models we have to limit the size of the embeddings considerably and cannot fine tune the model due to memory constraints The explored models provide feasible solutions for disaster tweets identification on social networks which see promising applications for emergency responses References Datasets resource center Appen March Retrieved April from https appen com datasets resource center   English spacy models documentation spaCy Retrieved April from https spacy io models en Khalid S November Bert explained A Complete Guide with Theory and tutorial Medium Retrieved April from https medium com samia khalid bert explained a complete guide with theory and tutorial ac ebc fa c Kipf T N Welling M Semi supervised classification with graph convolutional networks arXiv preprint arXiv Natural language processing with disaster tweets Kaggle Retrieved April from https www kaggle com c nlp getting started overview TF IDF Basics with pandas and Scikit Learn Ultraviolet Analytics Retrieved April from http www ultravioletanalytics com blog tf idf basics with pandas scikit learn Yao L Mao C Luo Y Graph convolutional networks for text classification Proceedings of the AAAI Conference on Artificial Intelligence https doi org aaai v i   Appendix BERT Text CNN Structure
Deal Probability Prediction for a Classified Advertisement Website Group Wang Guangyu A N Chen Keyi A U Liu Huilin A H Liu Ziyin A E Shi Chenxi A L GitHub Link https github com GY BT Group Objective Advertising is always living with us even though people may not be aware of it With new technology in today s world advertising uses all possible media to get its message to their targeted audience For example advertisements are delivered through television newspaper radio audio internet phone apps etc Advertising helps brands to reach more audiences and improve their brand awareness and also helps to drive a low funnel of key performance indicators such as sales Despite covid situation the advertising budget has been significantly increased among top advertisers and most of the social media platform s revenue are coming from advertising This interests our team to study on a Kaggle project which is about the demand prediction of online classified advertisement Avito is the largest and most popular Russian classified advertisement website In this project we are trying to predict demand for online advertisements that run on a website Avito that sells various kinds of goods Leveraging the available data set for various kinds of advertisements we are going to model and predict the probability of making a deal for those advertisements based on its advertisement title description image context location etc With such a prediction Avito could inform sellers on the optimization of their listing advertisements and share with sellers their potential rate to make a deal successfully Data Set Data source The data is from Kaggle From Avito we get Two databases One is about ad attribute data which is stored on a single ad basis including structured data like user id and ad id text data likead title and ad description our target deal probability and so on Also we obtain the images from the ads which is stored in jpg format in the image dataset Combining the above we can totally get a sample of ads from Avito with their advertising features time data and image data From the perspective of analysis we will divide the data into two parts structured data and unstructured data Data Description Structured Data Structured data is mainly from ad attribute dataset and period dataset which are mainly categorical and numerical variables Table Structured data description Variable Type Descriptions item id object Ad id user id object User id region object Ad region city object Ad city parent category object Ad category category name object Ad category param object Optional parameter param object Optional parameter param object Optional parameter price float Ad price item seq int Ad sequential number activation date object Date ad was placed  Deal Probability Prediction for a Classified Advertisement Website user type object User type image object Id code of image image top float classification code for the image deal probability float The target variable Note that param param and param are features about keyword or classification conclusion of the ads defined by website such as the brand of the product size etc Studying a lot of Kaggle contestants ideas many of them treated these features as categorical variables We have reservations on this point It is possible that these variables can be treated as text data In the following analysis we will try and combine both of the two ways of dealing with them Besides it s not possible to verify every transaction with certainty so the Avito supposes that the value of our target variable deal probability can be any float from zero to one Unstructured Data Unstructured data contains text data and image data Table Unstructured data description Variable Type Descriptions title object Ad title description object Ad description param object Optional parameter param object Optional parameter param object Optional parameter image jpg Images from the ads Since Avito is a Russian platform we face the challenge to deal with Russian text features We plan to use some open source tools for dealing with Russian such as nltk russian Russian texts tagging with Natural Language Toolkit red hen lab for Russian NLP word vec skip gram model for Russian language etc As the tools for analyzing Russian may not perform as well as those for English we have a backup plan to use text blob or transliterate to translate the text to English for further analysis For image data we can extract image features such as width and height color and saturation of colors histogram for contrast composition of objects from the jpd documents Since images are the most intuitive representation of ads to users we assume that image related features play an important role in prediction Exploratory Data Analysis Team performed exploratory data analysis on the data set Deal probability is our target variable in this study It represents the rate of successful deal made between the buyer and selling for a particular item Deal probability distribution plot can be seen in figure Most of the items have low deal probability in this data set Around percent of items listed have extremely low deal probability and percent of items have a deal probability of It clearly showed that deal on this advertising platform was not performed well Figure Deal probability distribution for the training data set All numerical variables are not corrected as shown in figure below Surprisingly deal probability and price are not corrected in this study which is kind of inline with our expectation that price may not be the key factor that affects the deal probability Figure Correlation matrix for the training data set Advertisement distribution throughout the region seemed to be fairly proportional shown in figure below The region with the most advertisements was Krasnodar Krai which took percent of the total advertisements On top  Deal Probability Prediction for a Classified Advertisement Website of that Sverdlovsk Oblast had percent of total advertisements Figure Ads Distribution by Region Most of the region had a deal probability between to shown in figure below Orenburg Oblast Bashkortostan and Stavropo Krai seemed to have slightly higher deal probability compared to other regions High deal probability marked to be outlier clearly reflected the lower volume of high deal rate Figure Deal Probability by Region Figure shows most of the users on the website are private which occupied percent of the user group percent are from the company and percent actually from the shop This shows the website mainly for private users for online transactions which is typically the case for classified advertisement websites worldwide Figure User Type Distribution Without doubt private users indeed had the highest deal probability among all user types in this study as reflected in figure Use type distribution pie chart was clearly inline with the deal probability plot in this case Figure Deal Probability by User Type Products that belong to personal belongings had percent of the total advertisements which is the top category followed by Home Garden category and Consumer electronic category shown in figure below These are categories that tend to have more advertisements for a classified website Figure Ads Distribution by Category Service category though had better and consistent deal probability which reflected on figure below Most of the categories deal probability fall into the lower quartile range Figure Deal Probability by Category  Deal Probability Prediction for a Classified Advertisement Website Another interesting fact is that advertisements without price had higher average deal probability as compared to advertisements with price labeled shown in figure Private buyers and sellers could negotiate better offers offline in order to make the deal successfully which is similar to other platforms such as Carousell in this scenario Avito website here may not be able to capture the deal information made in this way which could be a potential improvement needed in the future Figure Deal Probability by Price Advertisements that come with proper description definitely will have higher deal probability as reflected on figure below Detail and appropriate description do help to make transactions better in this case Buyers would like to know as detail as possible for every product or service listed on the website Figure Deal Probability by Description Lastly another interesting fact is that advertisements that come without an image have a slightly higher average deal probability Figure Deal Probability by Image Data Preprocessing Due to computing capacity we only selected image samples and its corresponding structured and text data for the modeling Also since some of the ads did not have images we included all none image ads in the dataset We get a total samples Read and merge Image Data We use opencv to read images Due to the large amount of images images are randomly selected Team noted that images in different advertisements have different width and height After converting the image into an array the shape of the array will be different which will cause challenges in the modeling part Thus we decide to unify the size of images and resize all images shapes The weight of images is while the height of images is After resizing the shape of one image becomes Finally we get a dataframe of image data with the same size and merge it with structured data by image name which is image in the ad attribute dataset We get a dataset with all structure data text data and image data now Handle missing values For structured data only price has missing values For most ads platforms price is the compulsory information required to provide when posting ads Here we keep such assumptions of compulsory price and focus on those with price information As price is related to so many factors like product types and new or old types we do not handle it but drop all samples without price by list wise method For text data since information of advertisements is filled by users and some information is not mandatory for posting advertisements we find there are null values in param param param and description We fill these features with empty strings and these rows remain in the dataset For image related data some advertisements don t have pictures so their image related data are null image top is the classification code for the image defined by Avito We fill missing values by to represent the ads without images For image data we fill nan with a zero matrix that has the same size with other image data Encode categorical features In this part we would like to transform categorical variables into numerical ones Of all the variables there are categorical variables which are listed in Table in Appendix We will first visualize the relationship between such variables and then try several encoding methods combined with domain knowledge Before encoding we first split train and test data to avoid data leakage  Deal Probability Prediction for a Classified Advertisement Website Since city names are duplicated across regions we combined city and region to identify specific cities There are regions and cities in total We could not just do the one hot encoding which will lead to data sparsity From figure arranging the mean deal probability for each region in ascending order we could find the deal probability is significantly different between certain regions while some regions are with similar values The red line is the mean deal probability of all training data We consider using the mean of deal probability in each region to represent them which refers to the transaction level per area Also we plot the mean price distribution by region and we find that consumption level in Orenburg Oblast is much higher than others Compared with the mean price level of all training samples only two regions are higher than the mean level It is possible that the wealth gap is large and the most wealth is concentrated in a few wealthy areas Consumption level in areas is correlated to salary economic development citizen types and so on which will also influence their deal probability in Avito Thus we also encode the region by mean price level of each town to represent the consumption level of each town These two encoding methods are also used in the city variable For the encoding in test data we assume that the data distribution of the training set can be representative of the general cases Therefore we transfer the mean of train data to test data corresponding to each type of the variables Figure Deal Probability V S Region Figure Price V S Region Variable parent category name and category name is the product type in the ads As parent category name only has categories I use one hot encoding to transfer it Also the demand of different product categories on classified advertised platforms is different Besides price is highly correlated with product categories For example for some daily use types of goods although their price is low it is seldom to make a deal on the classified advertising platform That is an iPhone must be more expensive than an apple However few people buy apples on such platforms We could also get such insights in figure and Therefore mean deal probability and mean price of each product type In addition to one hot encoding I also use the above two methods mean deal probability and mean price to encode parent category name As category name has different types we only use the latter two methods to encode it to control data sparsity Figure Deal Probability V S Parent category name Figure Price V S Parent category name From figure above different user types have different deal possibility distributions That is private is the most likely to make a transaction followed by the company and the shop last Therefore we would conduct one hot encoding for these three types of users The same as before we use the mean of deal probability and price for each class to encode them due to the significantly different distribution between user types For the mean price we could gain information about pricing levels for different users Figure Price V S user type As mentioned before param with types param with types and param with types which are the optional description information of products could also be treated as categorical variables These descriptions can be  Deal Probability Prediction for a Classified Advertisement Website used as detailed classification of products Also variable image top is about types of classification of ads image We also use the above two methods mean deal probability and mean price to encode all of them Variable activation date is the date the ad was placed In the dataset the activation period is from March to April Therefore we would like to extract the weekday the day of week like Monday or Sunday from the activation data Since the number of weekday is not ordinary for our prediction problem we still need to encode it From figure Wednesday and Saturday have high deal probability while the deal price is much higher than others on Wednesday To capture such different patterns of deal probability and price we also use the above two methods mean deal probability and mean price to encode weekday Figure Deal probability V S weekday Figure Price V S weekday The summary of the encoding methods is shown in Table I in Appendix After encoding we get numerical variables now Feature Engineering One of the objectives is to understand how the different quality of image and text are affecting the final deal probability Feature engineering for them is necessary because it is the way that can help interpret what kind of images and text are more attractive or have higher quality Text Features Textual data is composed of three parts param title and description Text description that we mainly focus on provides the most comprehensive information The features we chose to depict description contain three parts Basic features like the number of characters and words etc Part of Speech tagging features like numerals adjectives etc Sentiment Analysis features like polarity scores neutral scores etc The reason for choosing these three dimensions is that we assume the length of a sentence can reflect how much information has been provided the rendering how the users describe their products and the emotions conveyed by description may make a difference when people read it and will then reflect on the final deal probability Below is the table of complete text features Table Text features description Type Feature Name Feature Description Basic Features char count Number of characters word count Number of words word density char count word count punctuation count Number of punctuations title word count Number of words that begin with a capital letter in title POS Features NUM Number of numerals A Number of adjectives ADV Number of adverbs S Number of nouns V Number of verbs Sentiment Analysis Features polarity score The combination of the next scores neutral score Measures how neutral the sentence is negative score The negativity of a sentence positive score How positive the sentence is Image Features As the image vectors are both memory consuming and hard to explain we derived some features to access image quality in hope to provide more information to the model  Deal Probability Prediction for a Classified Advertisement Website Table Image features description Type Feature Name Feature Description Size height Height of the image width Width of the image Color color mean color std Mean and standard deviation of all pixels r mean r std b mean b std g mean g std Mean and std of Red Blue Green pixels compare color variations between images for RBG h mean h std s mean s std v mean v std Mean and std of Hue Saturation Value compare different color intensities on top of RBG Bright ness exposure measure the exposure level the perceived brightness contrast image contrast in grayscale Clarity edge score the number of edges used to measure pixel variation blurriness Number of adjectives These features are included into the training and testing data as numerical data Deep Learning Model When running deep learning models we encounter computing capacity issues Our computers are not able to deal with samples As a result we randomly sample data from our train set and test set respectively to complete model building What s more we detect that there are null values in image related features after feature engineering because some advertisements don t have images We fill these null values with before training our model Recurrent Neural Network RNN is an ideal model to do Natural Language Processing NLP since it can recognize the sequential characteristics of text and therefore it can understand text better Meanwhile RNN can handle not only short text but also long text which is an advantage compared with Convolutional Neural Network CNN in NLP Researchers study different deep learning models in NLP and find that RNN performs well and robustly in a broad range of tasks except when the task is essentially a keyphrase recognition task as in some sentiment detection and question answer matching settings Our target is to predict deal probability through understanding the descriptions of products in advertisements RNN will perform well in this scenario CNN is the mainstream method to deal with images Images have high dimensionality For traditional neural networks images are very difficult to deal with due to the computing capacity issues However CNN not only can automatically detect the important features without any human supervision but also is computationally efficient CNN is very effective in reducing the number of parameters without losing on the quality of models As such CNN is very suitable for image processing Generally our techniques are that we use RNN to handle text data MLP to handle numerical data and CNN to handle image data Then we will combine outputs from three models and build MLP on combined outputs to do final prediction Our work flow can be illustrated by figure Figure Work Flow of Deep Learning Model Word Embedding Word embedding is a fundamental part of NLP Through word embedding words that have similar meaning will be grouped together and have close vectors in vector space We combine text in param param param title and description and rename it as full text Then we implement text cleaning by removing http links html tags special characters stopwords and punctuations Also we implement lemmatization Typically pre trained word embedding matrices will have better performance than self trained word embedding matrix in word representation Here since the text is Russian we use pre trained word embedding from fasttext which is trained for Russian on Common Crawl and Wikipedia This model was trained  Deal Probability Prediction for a Classified Advertisement Website using CBOW with position weights in dimension with character n grams of length a window of size and negatives First we keep all words in our dataset to do word embedding Then we convert the text to index After that we unify the length of the text through padding The unified length is Words not found in the embedding matrix will be all zeros Model In this model we have an embedding layer and LSTM architecture in RNN Then we input numerical data directly without any processing For image data we build D CNN with convolutional layers and use max pooling right after each convolutional layer The activation function in three convolutional layers is relu We concate outputs from these models and input them into MLP We first normalize these inputs and then build a hidden layer with activation function to be relu and with the he uniform method to set the initial random weights of layers Also we set dropout layers to control risk of overfitting The dropout rate of this hidden layer is The second hidden layer has the same activation function and initializer with nodes only half of the first hidden layer We use sigmoid as the activation function in the output layer since we need a scaled regression target from to to represent probability The optimizer is rmsprop loss is mse and metrics is rmse Figure Appendix A shows the architecture of model The train RMSE of model is and the test RMSE is The number of trainable parameters in this model is That s a quite big number Too many parameters result in longer training time What s more we do not process numerical data before inputting them to MLP for prediction We want to improve these two points in a later model Model In model the architecture is quite similar to model We add MLP after the input layer of numerical data Also we change the filter size of CNN to With smaller filter size the dimension of CNN after flattening is greatly reduced and thus trainable parameters of the whole model are greatly reduced Beside these we change the optimizer from rmsprop to adam Figure Appendix A shows the architecture of model The train RMSE of model is and the test RMSE is The number of trainable parameters in this model is We greatly reduce the model size and improve the performance by We find that the dimension of output of RNN is different from other output s dimensions We want to unify the output dimension of different type data in a later model Model In model we add additional hidden layers to make the number of nodes of the output layer which is the same as MLPs and CNNs After concatenating the dimension becomes Given the small dimension we reduce the number of layers in MLP for prediction Figure Appendix A shows the architecture of model The train RMSE of model is and the test RMSE is The number of trainable parameters in this model is We slightly improved the performance by Why Deep Learning Model Did Not Perform Well The RMSE of deep learning models is around Since our target variable is from zero to one the performance is not very good Next we will analyze why deep learning models do not perform well in this project First the pre trained word embedding matrix is not suitable for this project Since the text data is Russian we can only choose fasttext as embedding However fasttext is trained from Common Crawl and Wikipedia Text in these two websites may have different scenarios from descriptions in Avito advertisements Some words not found in the embedding matrix also remain zeros That will bring negative effects on later modeling training especially in RNN training Second due to computing capacity we compress images a lot That may result in misleading information in images and our models may fail to capture appropriate information in image data Also unifying the pictures into one size will cause some pictures to stretch and deform resulting in confused information of image data As a result images data may not be very helpful in our models even may mislead the model training Third our target variable is derived from Avito s prediction model and cannot be verified with certainty It is possible that when getting target variables companies only use linear models Therefore deep learning models are too complex and twist the relationships between features and target variables Fourth due to our encoding method we use target variables to encode categorical variables and transform them into numerical variables As a result this encoding method will enhance the linear relationship between features and target variables making nonlinear systems of neural networks less effective Traditional Machine Learning Model Regression Results  Deal Probability Prediction for a Classified Advertisement Website Traditional regression models have faster training speed and are easier to interpret therefore we tested different regression models on top of the deep learning models To make the model more explainable original text and image data are excluded only the numerical data are inputted into the model which includes the encoded data and the features newly generated from texts and images Table Model Results Model Train RMSE Test RMSE Linear Regression Elastic Net XGBoost gblinear Random Forest XGBoost gbtree LGBM linear models non linear models The result shows that traditional models have comparable performance to the deep learning modes Light GBM being the best performing model has better performance than the later The simple linear models also have relatively good performance which add weight to our guess that the relationship between the main features and deal probability is a linear one We also performed hyperparameter tuning on the best performing model namely Light GBM but it only increased the test RMSE by a small margin This further implies that a simple linear model may be a good choice for its fast training speed and interpretability Interpretation of Model Outputs To understand the effects of different features on the deal probability we plotted the feature importance Figure in Appendix A using coefficients for linear models and SHAP for LGBM As the categorical features from the original data are not subjected to change for the sellers in Avito the image and text features are more useful in providing actionable insights However we realized that none of the text features are important across all the models whereas some image features rank quite high in terms of importance Table Important image features Types Features color h mean h std s std r mean b mean g mean colour mean size width height clarity blurriness Evaluation and Conclusion What make up good product images The highly ranked image features provided us with the information on the correlation between some image attributes and the deal probability For the colors mean for red blue green all have negative coefficients while overall pixel mean has positive coefficients suggesting that images that are mono color or strong in one color tone may not be preferred The positive coefficients for h std and s std suggests that images with larger color variation and contrasts are better For size height and width suggests that larger images are preferred which aligns with the common intuition For clarity less blurriness is preferred but it is less important than use of color This may be due to the fact that users are not able to detect small differences in blurriness given the small display area on the website On top of the features we generated Avito s classification of images image top is also an important factor in the xgboost regression Therefore we compared the images from the image top class of highest deal probability and the class with the lowest deal probability On top of the effect of colors size and blurriness as mentioned above the two groups also differ largely in edge score which suggests that images that have higher pixel variation may be preferred This is also consistent with our intuition as such images allow us to better differentiate the different objects and also different the subject from the background Why are text features not important As the plot of feature importance shows text features do not play a crucial role in the prediction of deal probability There are two possible reasons First of all the design of the Avito website diminishes the role of text Textual data is placed in an unobtrusive position which is not easy to be seen by users The  Deal Probability Prediction for a Classified Advertisement Website homepage of the website only presents the picture name price user address and release date There is no description of the product or any textual data When we click on a product and go to its details page a giant image appears on the first screen and the user has to scroll down to the bottom of the screen to see the description Second based on the feature importance results we found that mean dp param which represents the average deal probability for each param is important As mentioned above param is extracted automatically by Avito from the description It provides labeled characteristics of a product for example for girls for boys etc Then we divided the description into two groups based on mean dp param High dp param means this group of param has a higher deal probability and vice versa Figure Text Feature Statistics Based on mean dp param Figure shows that there is no significant difference in the text features between the two groups indicating that it doesn t matter how well the description itself is written as long as it covers the needed characteristics Business Insights With our findings in the previous sections we would provide the following suggestions to the sellers Firstly having a good product image is more important than having a good product description A good product image is defined in section Secondly external features such as city product category and user types can also greatly affect deal probability Section shows relationships of the different external features with the deal probabilities Therefore sellers need to form more realistic expectations given these external conditions Lastly Avito s internal classification of product image and description the features including image top para para para are also important in predicting deal probability That suggests that Avito has rich internal data and is able to accurately classify high selling items from the low ones Given the classification codes the sellers can mimic the characteristics of images and descriptions of the products in the high deal probability class On the other hand the sellers can also work with the website to gain more information on improving sales Future improvements The areas of improvements for our project are listed as follow Our current model shows a strong linear relationship between the features and the deal probability This is likely due to the fact that most of the important features such as image top para and user type are encoded using mean of deal probability With this the model makes the assumption that products of the same image class or same user type will have similar deal probability Therefore the model will not be effective for new products with new image class para etc The assumption will also be challenged if there is a frequent change in user buying pattern and preferences Therefore we can work on generating more features relating to the image and product itself for more robust prediction We were not able to utilize all the data provided by Avito due to memory constraint using more power devices and parallel processing will allow us to include more information into the model We can explore more encoding methods to compare the changes brought to the model and select the best of all  Deal Probability Prediction for a Classified Advertisement Website Appendix A Table I Summary of encoding methods Variable name Encoding method New variable region mean of deal probability in each region mean dp region mean of price in each region mean p region city mean of deal probability in each city mean dp city mean of price in each city mean p city parent ca tegory na me One hot encoding all parent category na me mean of deal probability in each parent category na me mean dp par categ ory name mean of price in each parent category na me mean p par catego ry name category name mean of deal probability in each category name mean dp category name mean of price in each category name mean p category n ame param param param mean of deal probability in each type mean dp param mean dp param mean dp param mean of price in each type mean p param m ean p param me an p param user type One hot encoding all user type mean of deal probability in each user type mean dp user type mean of price in each user type mean p user type image to p mean of deal probability in each image top mean dp image to p mean of price in each image top mean p image top activation date mean of deal probability in each weekday mean dp weekday mean of price in each weekday mean p weekday Figure The First Version of Deep Learning Model  Deal Probability Prediction for a Classified Advertisement Website Figure The Second Version of Deep Learning Model Figure The Third Version of Deep Learning Model Figure Coefficient Plot for Linear Regression Figure Coefficient Plot for XGBoost gblinear Figure Feature Importance for XGBoost gblinear  Deal Probability Prediction for a Classified Advertisement Website References Wenpeng Yin Katharina Kann Mo Yu Hinrich Sch tze Comparative Study of CNN and RNN for Natural Language Processing Anon Word vectors for languages fasttext Retrieved April from https fasttext cc docs en crawl vectors html Rich et al Keras Multiple inputs and mixed data July Retrieved April from https pyimagesearch com keras multiple inputs and mixed data pyis cta modal MridulMazumdar MridulMazumdar avito demand pre diction Prediction of deal probability using advertisement data Retrieved April from https github com MridulMazumdar avito demand predi ction Shivamb Ideas for image features and image quality May Retrieved April from https www kaggle com code shivamb ideas for image features and image quality Marian Stefanescu Measuring Enhancing Image Quality attributes January Retrieved April from https towardsdatascience com measuring enhancing image quality attributes b f e somang Tuning hyperparameters under minutes LGBM March Retrieved April from https www kaggle com code somang tuning hyperparameters under minutes lgbm notebook Anon Retrieved April from https www nltk org api nltk html
What Makes a High rating Movie Reviews Mining and Rating Prediction https github com chuhan BT Group Abstract In this project we trying to use Machine Learning Deep Learning and NLP models to understand and predict the movie rating in IMDB The final selected model is Light GBM model with inputs of both basic features and text features And the results shows that review sentiment score revenue budget and runtime are the most important features that impact movie rating Problem Definition As a commodity of the art form the film is an important part of modern cultural life The global film and video market reached a value of nearly billion in having increased at a compound annual growth rate And the market size is expected to reach billion by with the post pandemic economic recovery according to a new report by Grand View Research Inc The study of the success of movies mainly goes through three stages The first stage was in s mainly through analyzing the audience feedback collected by audience research institute In the second stage represented by Barry Litman a large number of impact factors were added to conduct multiple linear regression models to predict the variables including movie box office The third stage mainly used the massive content generated by netizens online as the main source of prediction Nowadays the development of information platforms such as Twitter and YouTube allow audiences to express their views and opinions freely And with the development of network films public praise has become the main criterion for advertisers to judge a movie which influences their investment Information transparency has led to a strong correlation between film reputations and revenues The audience s approbation degree with the film will be reflected through its rating which is an important standard to measure the success of the film IMDB Dataset https www kaggle com ashirwadsangwan imdb dataset The movies Dataset This project aims to predict the rating of films to quantify their popularity We will use IMDb movie data to conduct machine learning models including classification models ensemble methods like Decision Tree Random Forest Gradient Boost XGBoost and neural network a deep learning algorithm that helps mimic the non linear and complex patterns in calculations And we will also use natural language processing methods to do reviews text mining and posters image processing to contribute to our prediction Through conducting these methods we can put forward suggestions on film production marketing and distribution EDA Data Preprocessing All data used in this project are from Kaggle com we combine IMDb Dataset and The Movies Dataset through IMDb Ids and combine Review Dataset through movie titles The final dataset contains more than movies from countries The objective of our project is to find the reasons behind a high rating and vote of movies in IMDb using all these available information as well as online movie reviews to predict ratings of new movies in IMDb Thus there are five candidate variables that indicate the popularity degree of a movie they are averageRating vote average numVotes popularity and revenue We finally choose averageRating as model s target variable due to its normal distribution and easier interpretability Besides there are other basic information of movies including type genres release date directors actors and so on Text and image data also help to bring insights of a hit movie in this project we use NLP techniques to do the sentiment analysis of audience reviews and use Neural Network to process movie posters The table below shows information of variables used in this project https www kaggle com rounakbanik the movies dataset IMDB Review Dataset https www kaggle com ebiswas imdb review dataset  Submission and Formatting Instructions for ICML Table Table of variables from Movie Dataset VARIABLES TYPE TARGET VARIABLE AVERAGE RATING FLOAT MOVIE TITLE PRIMARY TITLE STRING ORIGINAL TITLE STRING MOVIE RELATED INFORMATION TITLE TYPE STRING START YEAR INT RELEASE DATE DATETIME IS ADULT BOOLEAN GENRES STRING RUNTIME FLOAT BUDGET INT ORIGINAL LANGUAGE STRING PRODUCTION COMPANIES STRING PRODUCTION COUNTRIES STRING STATUS STRING DIRECTOR STRING ACTOR STRING TEXT DATA OVERVIEW STRING KEYWORDS STRING REVIEWS STRING IMAGE DATA POSTER PATH STRING Exploratory Data Analysis After simple processing the whole dataset we gain numeric variables and categorical variables excluding text and image data NUMERIC DATA ANALYSIS The descriptive statistics table of numeric variables shows in appendix We also find that a large part of movies doesn t have the data of budget and revenue which means further data collection or fill in techniques may be needed if we want to use these two variables We also draw the histograms of all numeric variables to check bias of data distribution Winsorize processing is used before drawing in order to remove outliers Finally we draw a correlation matrix for the numerical variables seeking some correlation between variables We can see relatively strong correlation between numercial and averageRating CATEGORICAL DATA ANALYSIS We visualize the categorical features using wordcloud and histgrams appendix We can see some distribution across different categories in the appendix Data Pre processing Among the whole dataset data movies are used for training and movies for testing For all numerical features we normalize both training and test dataset using mean and standard deviation of training dataset For categorical features there exits so many classes for several features like directors and actors so we decide to simplify classes as below before applying one hot encoding Table Categorical features remained for one hot encoding FEATURES CLASSES TITLE TYPE movie short tv Episode tv Mini Series tv Movie tv Series tv Short tv Special video START YEAR pre post GENRES Action Adult Adventure Animation Biography Comedy Crime Documentary Drama Family Fantasy Film Noir Foreign History Horror Music Musical Mystery News Reality TV Romance Sci Fi Short Sport TV Movie Talk Show Thriller War Western ORIGINAL LANGUAGE en fr it ja de es ru hi others PRODUCTION COMPANIES Paramount Pictures Metro Goldwyn Mayer MGM Twentieth Century Fox Film Corporation Warner Bros Universal Pictures others PRODUCTION COUNTRIES US GB FR CA JP IT others STATUS Canceled In Production Planned Post Production Released Rumored DIRECTOR John Ford Michael Curtiz Werner Herzog Alfred Hitchcock Woody Allen Georges M li s Sidney Lumet Jean Luc Godard Charlie Chaplin Raoul Walsh others ACTOR John Wayne Jackie Chan Nicolas Cage Robert De Niro G rard Depardieu Michael Caine Burt Lancaster Paul Newman Bruce Willis Barbara Stanwyck others After preprocessing there are totally features excluding text features used in the following models Movie Overviews and Reviews Analysis Overviews and Keywords Analysis In the text mining part we have two types of data One is the movie overview and another is the movie keywords For overviews we first use the Texthero package to lowercase and remove digits URLs punctuation stop words and HTML tags lemmatization and stemming are also used to clean texts In the model part we conduct the pre trained Distil BETR model without fine tunes to conduct feature extraction BERT is short for Bidirectional Encoder Representations from Transformers which  Submission and Formatting Instructions for ICML indicates a transformer based machine learning technique for natural language processing pre training After inputting sentences into BERT the Bert Tokenizer will first break sentences into tokens add CLS and SEP tokens and then use their indexes in vocab to replace them The Bert deep learning layers will finally extract features with a dimension of We first rated movies above as high scores and those below as low scores By conducting Logistic Regression models using features extracted by BERT we can get out of sample accuracy of For keywords mining we first use the same text cleaning methods as the movie overview After that we conduct three methods including the Bag of Word model the One hot encoding method and the TF IDF model Because there are too many words for movie overviews we do not conduct these three models for it The bag of Word model is a straightforward way to turn a sentence into a vector representation and it can convert sentences in the text to a word frequency matrix by counting the number of occurrences of each word We also drop infrequent words that appear less than times in total and the dimension of features is After setting movies above as label and those below as label we can get out of sample accuracy of According to the coefficients of LR the top words having a positive influence include anime gojira bell innocence and jazz And the top words having a negative influence include carry bigfoot karate bikini and duringcreditssting We also use One Hot encoding to get a matrix of dimensions of whether or not the frequent word appears in the sentence which leads to the accuracy of And the top words having a positive influence include cinematic samurai phenomenon anime and classic And the top words having a negative influence include escort bat bikini pok mon zomby which are quite different from BOW TF IDF is a commonly used weighting technique for text mining which is used to assess the importance of a word to one of the documents in a corpus And it is calculated by term frequency times inverse document frequency After getting features with dimensions we can get an accuracy of The top words having a positive influence on the LR model include anime immigrant history samurai and noir And the top words having a negative influence include slasher bikini sex zomby and mutant From the influence of the word in the three models we can see that anime and samurai make a great contribution to high rates and bikini and zomby make a great contribution to low rates But because the matrix is too sparse the influential words are not similar in different models To reduce feature dimensions we also conduct principal component analysis on the TF IDF matrix The cumulative explained variance is shown in Figure To make sure that the cumulative explained variance is larger than the first PCs are chosen Figure The cumulative explained variance of PCA The out of sample performance accuracy using the LR model is The accuracy is slightly lower than LR using TF IDF matrix and higher than LR using BOW and One hot encoding matrix The total models performance is shown in Table Table LR models performance using text features LR ACCURACY FEATURE DIMENSION OVERVIEW BERT KEYWORDS BOW ONEHOTENCODER TF IDF TF IDF PCA Therefore in classification model construction we will mainly use features extracted from BERT Reviews Analysis SENTIMENT ANALYSIS We conduct sentiment analysis for movie reviews to extract emotional information from people s reviews on movies We apply Valence Aware Dictionary VADER a module in NLTK to complete the analysis Meanwhile the sentiment score generated by VADER can then be input as a feature to predict movies ratings Although the standard criteria in VADER is we choose a different threshold to classify reviews sentiment type which is and a compound score bigger than indicates positive and a compound score smaller than indicates a negative paragraph to extract more representative phrases and control the sample size According to the distribution of sentiment score we can see the mean is around We believe most of the audience have a herd mentality and try to praise so the extreme positive remarks can better reflect the advantages of the film  Submission and Formatting Instructions for ICML Figure Sentiment Score and Sentiment Type LDA We utilize Latent Dirichlet Allocation LDA to summarize the topics in positive and negative reviews respectively LDA builds a topic per document model and words per topic model modeled as Dirichlet distributions After modeling a similarity based optimal method for LDA is used to determine the number of topics and perform topic analysis The specific steps are as follows Take the initial number of topics k values get the initial model and calculate the similarity average cosine distance between topics Increase or decrease the value of k retrain the model and calculate the similarity between topics again Repeat step until the optimal k value is obtained From Figure it can be seen that for positive and negative comment data the average cosine similarity between topics reaches the lowest when the number of topics is Therefore the number of topics can be chosen as Figure Cosine Similarity of Topics Based on the result of LDA we can see the negative comments description of the movie mainly deals with the subject words like war space etc and little about the filming techniques of the movie However the positive comments mainly contain the actors acting storyline theme love society etc These factors mainly influence people s positive comments on the movie Table LDA Topics Sample NEGATIVE TOPICS POSITIVE TOPICS AMERICAN GE RMAN JAPANE SE BRITISH WH ITE ARMY HIST ORY YEARS BL ACK BATTLE ACTION LIK E FIGHT SE RIES EFFEC TS TIME WO RLD PLOT S PACE ALIEN CHARACTER PERFORMANC E ROLE CAST MICHAEL CHA RACTERS ACT ION SCREEN LIFE PEOPLE WORLD LOV E STORY RE AL HUMAN B EAUTIFUL TR UE SOCIETY Rating Prediction In this part we are trying to use machine learning and deep learning models to predict the average rating of each movie using both basic metadata and text features generated from NLP models We try both Regression and Classification models to find more insights For Regression models the target variable is average rating ranged from For Classification models we set target equals to when average rating greater than or equal to and otherwise Also for the input features we try both basic features and basic features plus text features Linear Regression Logistic Regression For Linear Regression the model performance is shown below Table Linear Regression Performance BASIC MODEL BASIC OVERVIEW REVIEW MODEL TRAIN MSE TEST MSE TRAIN MSE TEST MSE LINEAR REGRESSION Both training and test MSE drop around after adding text features For Logistic Regression we can draw the confusion matrix to compare the model performance Figure Test confusion matrix of Basic Model left and Basic Overview Review Model right using Logistic Regression The numbers of True Positive and True negative increase a lot after adding overview and review features And both two models have a balanced TP FP and Recall rate Tree Models In this case we adopt different tree models including the Decision Tree the Light GBM and the XGBoost Here are the performances of the tree regressors  Submission and Formatting Instructions for ICML Table Train Test MSE for regression tree models BASIC MODEL BASIC OVERVIEW REVIEW MODEL TRAIN MSE TEST MSE TRAIN MSE TEST MSE DECISION TREE LIGHT GBM XGB It can be found out that by adding more features the regression performance is significantly enhanced With features including basic overview and review features LightGBM model performs the best with a minimum MSE of on the test set To be more detailed for this LightGBM regression model the hyperparameters selected after tuning is bagging fraction feature fraction num leaves For classification we used the following tree models for prediction random forest XGBoost and Light GBM since they can capture more complex relationship between features and variable and usually provide high performance We first used default hyperparameter and then tuned the best performing basic model Light GBM The final hyperparameter used are listed as follows colsample bytree learning rate max depth min child samples min child weight n estimators reg alpha reg lambda subsample The results of the model performance are summarized in the section Multilayer Perceptron MLP Here are the hyperparameters model structure loss function and optimizer of regression and classification model Table MLP hyperparameters REGRESSION MODEL CLASSIFICATION MODEL MODEL STRUCTURE Input Linear Output Input Linear Output BATCH SIZE NUMBER OF EPOCHS LOSS FUNCTION MSE Binary Cross Entropy OPTIMIZER Adam Adam For MLP regression model we draw the loss curve during training process Figure Training and Test MSE during training process of Basic Model left and Basic Overview Review Model right using MLP The overall MSE for both training and test achieve a lower level when adding overview and review features to the basic model Test MSE of Basic Model only slightly drop during training process while test MSE of Basic Overview Review Model shows an obvious decrease trend which means MLP learns better after adding overview and review features The results of MLP classification model show that although the accuracy results are similar in these two models Basic Overview Review Model can gain higher True Positive rate and more balanced TP and FP rate in out of sample data Figure Test confusion matrix of Basic Model left and Basic Overview Review Model right using MLP Regression Models Comparison Table Train Test MSE for regression models BASIC MODEL BASIC OVERVIEW REVIEW MODEL TRAIN MSE TEST MSE TRAIN MSE TEST MSE LINEAR REGRESSION DECISION TREE LIGHT GBM XGB MLP If we see the target variable as continuous predicting the average rating of a movie will be a regression problem  Submission and Formatting Instructions for ICML Compared to see it as a classification problem it takes the order and the real meaning of the target number into consideration The regression model based on Light GBM performs the best with an MSE of on the test set It uses basic data features extracted from the overview and the review to train Classification Models Comparison The distribution of high rating movie between the train and test dataset are shown in the figures below We can see the distribution are balanced We chose accuracy as evaluation metrics since we care about the classification of both classes Figure Distribution of High Rating Movie in Train Test Dataset Table Train Test accuracy for classification models BASIC MODEL BASIC OVERVIEW REVIEW MODEL TRAIN MSE TEST MSE TRAIN MSE TEST MSE LOGISTIC REGRESSION RANDOM FOREST LIGHT GBM XGB MLP We can see that the Light GBM provide the highest test accuracy for both models with and without text feature after hyperparameter tuning The model accuracy is also improved overall after adding the text features Explainable AI Insights In this part the model based on Light GBM with features is further explore We try to find out the key to high rating movies by global interpretation as well as local interpretation In global interpretation feature importance scores and permutation feature importance are calculated to find out the most impactful features on a movie s average rating In local interpretation by conducing LIME and SHAP ratio we try to find out why the movie with the highest average rating and the one with the lowest rating get their results respectively Global Interpretation The feature importance figure shown in Figure is plotted based on the feature importance score of the Light GBM model Permutation feature importance is shown in Figure It can be discovered that the top features are very alike especially the runtime the sentiment score based on the review and the revenue of the movie which indicates that regardless of the methods chosen to calculate the feature importance the result is robust and thus the movie makers surely should pay more attention to these top features Figure Feature importance Figure Permutation feature importance For classification model we used Light GBM which provides the best prediction accuracy for feature importance interpretation We also used Permutation Feature Importance model which shuffles the feature value within the dataset calculate the performance loss happened due to shuffling based on Light GBM for feature importance The results of the two models are slightly different because of the different algorithm behind As we can see from the figures in appendix for classification model without text features runtime revenue budget and startYear pre are important features for the high rating movie prediction After adding text features we can see that the sentiment score start to play a prominent role in prediction while the other featues remain as important features but the importance level falls behind the sentiment score This is reasonable since people leave positive comment of the movie will give a high rating to the movie  Submission and Formatting Instructions for ICML Figure Feature importance for classification model with text feature using Light GBM Figure Permutation feature importance for classification model with text feature using Light GBM Figure Feature importance for classification model without text feature using lightgbm Figure Permutation feature importance for classification model without text feature using lightgbm Local Interpretation In local interpretation we further look into the record with the highest ratings and the lowest ratings There are two movies get the highest average rating of and we pick one between them Firstly the SHAP value is calculated SHAP value is used to interpret the impact of having a certain value for a given feature in comparison to the prediction we d make if that feature took some baseline value The results are shown in Figure It can be found out that the runtime and a genre of documentary contribute a lot and positively to its high average rating Secondly the LIME is adopted to try to explain the result generated by a black box Before putting the sample into the LIME explainer we should first make some adjustments to our dataset and to be more detailed we should have the dataset before we conduct one hot encoding on the categorical features since one hot encoding is not allowed in the lime However the feature of genre is an exception because a movie can be categorized into multiple genres at the same time and this will not cause meaningless input The result of LIME is shown in Figure Still it shows that the runtime and a genre of documentary is very important in helping the movie achieve such a high rating Similarly we also look into the movie with the lowest average rating of The SHAP value is shown in Figure and the LIME result is shown in Figure This time the results are not that similar But they both suggest that movie makers should pay more attention to people s reviews especially the sentiment implied in them  Submission and Formatting Instructions for ICML Figure The SHAP value of the movie with the highest rating Figure The LIME result of the movie with the highest rating Figure The SHAP value of the movie with the lowest rating Figure The LIME result of the movie with the lowest rating Image Classification In the IMDB dataset there is a feature called which contains the URLs for poster image of each film We download poster images from https image tmdb org and gather total poster images from valid URLs Exploratory Data Analysis Among total posters of posters are from movies that have average rating greater than and of posters are from movies with average rating less than Figure Class balance of poster dataset From Word Cloud we can found that most of posters belong to Drama and Comedy and these movies are most produced in US Figure Word Cloud of movie production countries and movie genres in poster dataset Data Pre processing In order to balance two classes and save ram we extract a sub dataset with images images with positive targets and with negative targets For the target variable we also treat them as previous classification models that target equals to when average rating greater than or equal to and otherwise For the image matrix we also normalize pixel number to between and by dividing After that we resize all images to for the convenience of deep learning model pipeline and finally gain a D Numpy array of shape Then we use of images as training set and as test set  Submission and Formatting Instructions for ICML Model Training and Selection Insights The models that we use to classify images are two CNNs with different structures which are shown in Appendix and Both models share the same hyperparameters that and First we try CNN with one Conv D layer and two Full connected layers and the model turns out to be a random guess model on test set even though the training accuracy is close to It seems that this model encounters an overfitting problem at beginning stage Then we try second CNN model with three Conv D layers and two Full connected layers The model also experiences a high level of overfitting but the test accuracy is a little bit better than the first CNN at As can be seen from the accuracy curve during training process as well as the Class Activation Map from Grad Cam the model actually doesn t learn the correct rules for prediction Figure Training Test accuracy during training process Figure Class Activation Map for negative class Table Train Test MSE for classification models TRAINING ACCURACY TEST ACCURACY CNN CNN Since the results are dissatisfied we make final conclusion that movie posters may have little impact on people s rating in IMDB Conclusion In this project we trying to use Machine Learning Deep Learning and NLP models to understand and predict the movie rating in IMDB Here are the conclusions that we gain through this project For overview feature extraction features generated from BERT can achieve most accurate results In review analysis part the scores of the sentiment analysis are concentrated at so most of the viewers in the sample have a positive evaluation of the movie The most important concern in positive evaluation is actors acting performance and storyline In rating prediction part among both regression and classification models Light GBM can achieve smallest MSE and highest accuracy Our final prediction model is Light GBM model with inputs of both basic features and text features In interpretation part review sentiment score revenue budget and runtime are the most important features that impact average rating In the trial of image classification the results shows that movie poster may has little impact on viewers rating According to the conclusions we have some strategy suggestions for movie companies that may help to gain a higher rating among viewers Before movie comes out improve movie budget and runtime can help to get higher rating After movie comes out higher review sentiment score and higher revenue can lead to higher rating Besides review sentiment score is highly related to actors acting performance and storyline References Sharda R Delen D Predicting box office success of motion pictures with neural networks Expert Systems with Applications Shangbo ZHENG Jian ZHOU Modeling on box office revenue prediction of movie based on neural network Journal of Computer Applications Xuejun WANG The Brief Introduction of The History of Box Office Research Chongqing University  Submission and Formatting Instructions for ICML Appendix Descriptive statistics table of numeric variables startYear runtime popularity Average Rating count mean std min max Appendix Figure histograms of all numeric variables Appendix Figure correlation matrix among numeric variables Appendix Model Structure of CNN Appendix Model Structure of CNN
